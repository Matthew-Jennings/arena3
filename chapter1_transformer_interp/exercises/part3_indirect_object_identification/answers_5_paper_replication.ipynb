{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paper Replication\n",
    "- Replicate most of the other results from the [IOI paper](https://arxiv.org/abs/2211.00593)\n",
    "\n",
    "- Practice more open-ended, less guided coding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"ACCELERATE_DISABLE_RICH\"] = \"1\"\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import torch as t\n",
    "from torch import Tensor\n",
    "import numpy as np\n",
    "import einops\n",
    "from tqdm.notebook import tqdm\n",
    "import plotly.express as px\n",
    "import webbrowser\n",
    "import re\n",
    "import itertools\n",
    "from jaxtyping import Float, Int, Bool\n",
    "from typing import List, Optional, Callable, Tuple, Dict, Literal, Set, Union\n",
    "from functools import partial\n",
    "from IPython.display import display, HTML\n",
    "from rich.table import Table, Column\n",
    "from rich import print as rprint\n",
    "import circuitsvis as cv\n",
    "from pathlib import Path\n",
    "from transformer_lens.hook_points import HookPoint\n",
    "from transformer_lens import utils, HookedTransformer, ActivationCache\n",
    "from transformer_lens.components import Embed, Unembed, LayerNorm, MLP\n",
    "from ioi_dataset import NAMES, IOIDataset\n",
    "from solutions import format_prompt, make_table\n",
    "\n",
    "t.set_grad_enabled(False)\n",
    "\n",
    "from arena3.chapter1_transformer_interp.exercises.plotly_utils import (\n",
    "    imshow,\n",
    "    line,\n",
    "    scatter,\n",
    "    bar,\n",
    ")\n",
    "import tests\n",
    "\n",
    "device = t.device(\"cuda\") if t.cuda.is_available() else t.device(\"cpu\")\n",
    "\n",
    "MAIN = __name__ == \"__main__\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model, set up dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HookedTransformer.from_pretrained(\n",
    "    \"gpt2-small\",\n",
    "    center_unembed=True,\n",
    "    center_writing_weights=True,\n",
    "    fold_ln=True,\n",
    "    refactor_factored_attn_matrices=True,\n",
    ")\n",
    "\n",
    "N = 25\n",
    "ioi_dataset = IOIDataset(\n",
    "    prompt_type=\"mixed\",\n",
    "    N=N,\n",
    "    tokenizer=model.tokenizer,\n",
    "    prepend_bos=False,\n",
    "    seed=1,\n",
    "    device=str(device),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logits_to_ave_logit_diff_2(\n",
    "    logits: Float[Tensor, \"batch seq d_vocab\"],\n",
    "    ioi_dataset: IOIDataset = ioi_dataset,\n",
    "    per_prompt=False,\n",
    ") -> Union[Float[Tensor, \"\"], Float[Tensor, \"batch\"]]:\n",
    "    \"\"\"\n",
    "    Returns logit difference between the correct and incorrect answer.\n",
    "\n",
    "    If per_prompt=True, return the array of differences rather than the average.\n",
    "    \"\"\"\n",
    "\n",
    "    # Only the final logits are relevant for the answer\n",
    "    # Get the logits corresponding to the indirect object / subject tokens respectively\n",
    "    io_logits = logits[\n",
    "        range(logits.size(0)), ioi_dataset.word_idx[\"end\"], ioi_dataset.io_tokenIDs\n",
    "    ]  # [batch]\n",
    "    s_logits = logits[\n",
    "        range(logits.size(0)), ioi_dataset.word_idx[\"end\"], ioi_dataset.s_tokenIDs\n",
    "    ]  # [batch]\n",
    "    # Find logit difference\n",
    "    answer_logit_diff = io_logits - s_logits\n",
    "    return answer_logit_diff if per_prompt else answer_logit_diff.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abc_dataset = ioi_dataset.gen_flipped_prompts(\"ABB->XYZ, BAB->XYZ\")\n",
    "\n",
    "model.reset_hooks(including_permanent=True)\n",
    "\n",
    "ioi_logits_original, ioi_cache = model.run_with_cache(ioi_dataset.toks)\n",
    "abc_logits_original, abc_cache = model.run_with_cache(abc_dataset.toks)\n",
    "\n",
    "ioi_per_prompt_diff = logits_to_ave_logit_diff_2(ioi_logits_original, per_prompt=True)\n",
    "abc_per_prompt_diff = logits_to_ave_logit_diff_2(abc_logits_original, per_prompt=True)\n",
    "\n",
    "ioi_average_logit_diff = logits_to_ave_logit_diff_2(ioi_logits_original).item()\n",
    "abc_average_logit_diff = logits_to_ave_logit_diff_2(abc_logits_original).item()\n",
    "\n",
    "print(f\"Average logit diff (IOI dataset): {ioi_average_logit_diff:.4f}\")\n",
    "print(f\"Average logit diff (ABC dataset): {abc_average_logit_diff:.4f}\")\n",
    "\n",
    "make_table(\n",
    "    colnames=[\"IOI prompt\", \"IOI logit diff\", \"ABC prompt\", \"ABC logit diff\"],\n",
    "    cols=[\n",
    "        map(format_prompt, ioi_dataset.sentences),\n",
    "        ioi_per_prompt_diff,\n",
    "        map(format_prompt, abc_dataset.sentences),\n",
    "        abc_per_prompt_diff,\n",
    "    ],\n",
    "    title=\"Sentences from IOI vs ABC distribution\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copying & writing direction results\n",
    "- Start by replicating the paper's analysis of the Name Mover Heads and Negative Name Mover Heads.\n",
    "\n",
    "- Our previous analysis should have pretty much convinced us that these heads are copying / negatively copying our indirect object token, but the results here show this with a bit more rigour."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: replicate writing direction results\n",
    "\n",
    "- Figure 3(c) from the paper plots the output of the strongest name mover and negative name mover heads against the attention probabilities for `END` attending to `IO` or `S` (color-coded).\n",
    "\n",
    "- Some clarifications:\n",
    "  - \"Projection\" here is being used synonymously with \"dot product\".\n",
    "\n",
    "  - We're projecting onto the name embedding. I.e., the embedding vector for the token to which attention is being paid.\n",
    "    - This is not the same as the logit diff, which we got by projecting the heads' output onto the difference between the unembedding vectors for `IO` and `S`.\n",
    "\n",
    "  - We're doing this because the question we're trying to answer is: *\"does the attention head copy (or anti-copy) the names to which it pays attention?\"*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scatter_embedding_vs_attn(\n",
    "    attn_from_end_to_io: Float[Tensor, \"batch\"],\n",
    "    attn_from_end_to_s: Float[Tensor, \"batch\"],\n",
    "    projection_in_io_dir: Float[Tensor, \"batch\"],\n",
    "    projection_in_s_dir: Float[Tensor, \"batch\"],\n",
    "    layer: int,\n",
    "    head: int,\n",
    "):\n",
    "    scatter(\n",
    "        x=t.concat([attn_from_end_to_io, attn_from_end_to_s], dim=0),\n",
    "        y=t.concat([projection_in_io_dir, projection_in_s_dir], dim=0),\n",
    "        color=[\"IO\"] * N + [\"S\"] * N,\n",
    "        title=f\"Projection of the output of {layer}.{head} along the name<br>embedding vs attention probability on name\",\n",
    "        title_x=0.5,\n",
    "        labels={\n",
    "            \"x\": \"Attn prob on name\",\n",
    "            \"y\": \"Dot w Name Embed\",\n",
    "            \"color\": \"Name type\",\n",
    "        },\n",
    "        color_discrete_sequence=[\"#72FF64\", \"#C9A5F7\"],\n",
    "        width=650,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_and_show_scatter_embedding_vs_attn(\n",
    "    layer: int,\n",
    "    head: int,\n",
    "    cache: ActivationCache = ioi_cache,\n",
    "    dataset: IOIDataset = ioi_dataset,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Creates and plots a figure equivalent to 3(c) in the paper.\n",
    "\n",
    "    This should involve computing the four 1D tensors:\n",
    "        attn_from_end_to_io\n",
    "        attn_from_end_to_s\n",
    "        projection_in_io_dir\n",
    "        projection_in_s_dir\n",
    "    and then calling the scatter_embedding_vs_attn function.\n",
    "    \"\"\"\n",
    "    # Get the value written to the residual stream at the end token by this head\n",
    "    z = cache[utils.get_act_name(\"z\", layer)][:, :, head]  # [batch seq d_head]\n",
    "    N = z.size(0)\n",
    "    output = z @ model.W_O[layer, head]  # [batch seq d_model]\n",
    "    output_on_end_token = output[\n",
    "        t.arange(N), dataset.word_idx[\"end\"]\n",
    "    ]  # [batch d_model]\n",
    "\n",
    "    # Get the directions we'll be projecting onto\n",
    "    io_unembedding = model.W_U.T[dataset.io_tokenIDs]  # [batch d_model]\n",
    "    s_unembedding = model.W_U.T[dataset.s_tokenIDs]  # [batch d_model]\n",
    "\n",
    "    # Get the value of projections, by multiplying and summing over the d_model dimension\n",
    "    projection_in_io_dir = (output_on_end_token * io_unembedding).sum(-1)  # [batch]\n",
    "    projection_in_s_dir = (output_on_end_token * s_unembedding).sum(-1)  # [batch]\n",
    "\n",
    "    # Get attention probs, and index to get the probabilities from END -> IO / S\n",
    "    attn_probs = cache[\"pattern\", layer][:, head]  # [batch seqQ seqK]\n",
    "    attn_from_end_to_io = attn_probs[\n",
    "        t.arange(N), dataset.word_idx[\"end\"], dataset.word_idx[\"IO\"]\n",
    "    ]  # [batch]\n",
    "    attn_from_end_to_s = attn_probs[\n",
    "        t.arange(N), dataset.word_idx[\"end\"], dataset.word_idx[\"S1\"]\n",
    "    ]  # [batch]\n",
    "\n",
    "    # Show scatter plot\n",
    "    scatter_embedding_vs_attn(\n",
    "        attn_from_end_to_io,\n",
    "        attn_from_end_to_s,\n",
    "        projection_in_io_dir,\n",
    "        projection_in_s_dir,\n",
    "        layer,\n",
    "        head,\n",
    "    )\n",
    "\n",
    "\n",
    "nmh = (9, 9)\n",
    "calculate_and_show_scatter_embedding_vs_attn(*nmh)\n",
    "\n",
    "nnmh = (11, 10)\n",
    "calculate_and_show_scatter_embedding_vs_attn(*nnmh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpretation\n",
    "\n",
    "- For head `9.9`, both the `S` and `IO` tokens exhibit a positive correlation between their contribution to logit difference and the attention probability on their name.\n",
    "  - This suggests the head is just copying names it attends to from the name to the `END` token.\n",
    "  - We can see that it is paying more attention to the `IO` token and less on `S`, which is what we expect (thanks to Q-composition with the S-inhibition heads).\n",
    "\n",
    "- The same is true for the negative name mover head `11.10`, only it works in the opposite direction: actively suppressing the logit score for the names it attends to.\n",
    "  - **Note**: it's important that we observe this negative correlation, because this shows us that the head really is anti-copying the IO token (rather than just copying the S token)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: replicate copying score results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_copying_scores(\n",
    "    model: HookedTransformer, k: int = 5, names: list = NAMES\n",
    ") -> Float[Tensor, \"2 layer-1 head\"]:\n",
    "    \"\"\"\n",
    "    Gets copying scores (both positive and negative) as described in page 6 of the IOI paper, for every (layer, head) pair in the model.\n",
    "\n",
    "    Returns these in a 3D tensor (the first dimension is for positive vs negative).\n",
    "\n",
    "    Omits the 0th layer, because this is before MLP0 (which we're claiming acts as an extended embedding).\n",
    "    \"\"\"\n",
    "    results = t.zeros((2, model.cfg.n_layers, model.cfg.n_heads), device=device)\n",
    "\n",
    "    name_tokens: Int[Tensor, \"batch 1\"] = model.to_tokens(names, prepend_bos=False)\n",
    "    name_embeddings: Int[Tensor, \"batch 1 d_model\"] = model.embed(name_tokens)\n",
    "\n",
    "    resid_after_mlp0 = name_embeddings + model.blocks[0].mlp(\n",
    "        model.blocks[0].ln2(name_embeddings)\n",
    "    )\n",
    "\n",
    "    for layer in tqdm(range(model.cfg.n_layers), desc=\"Layers\"):\n",
    "        for head in range(model.cfg.n_heads):\n",
    "\n",
    "            resid_after_OV_pos = (\n",
    "                resid_after_mlp0 @ model.W_V[layer, head] @ model.W_O[layer, head]\n",
    "            )\n",
    "            resid_after_OV_neg = (\n",
    "                resid_after_mlp0 @ -model.W_V[layer, head] @ model.W_O[layer, head]\n",
    "            )\n",
    "\n",
    "            logits_pos = model.unembed(model.ln_final(resid_after_OV_pos)).squeeze()\n",
    "            logits_neg = model.unembed(model.ln_final(resid_after_OV_neg)).squeeze()\n",
    "\n",
    "            topk_logits = t.topk(logits_pos, dim=-1, k=k).indices\n",
    "            in_topk = (topk_logits == name_tokens).any(-1)\n",
    "\n",
    "            bottomk_logits = t.topk(logits_neg, dim=-1, k=k).indices\n",
    "            in_bottomk = (bottomk_logits == name_tokens).any(-1)\n",
    "\n",
    "            # Fill in results\n",
    "            results[:, layer, head] = t.tensor(\n",
    "                [in_topk.float().mean(), in_bottomk.float().mean()]\n",
    "            )\n",
    "    return results\n",
    "\n",
    "\n",
    "copying_results = get_copying_scores(model)\n",
    "\n",
    "imshow(\n",
    "    copying_results,\n",
    "    facet_col=0,\n",
    "    facet_labels=[\"Positive copying scores\", \"Negative copying scores\"],\n",
    "    title=\"Copying scores of attention heads' OV circuits\",\n",
    "    width=800,\n",
    ")\n",
    "\n",
    "\n",
    "heads = {\n",
    "    \"name mover\": [(9, 9), (10, 0), (9, 6)],\n",
    "    \"negative name mover\": [(10, 7), (11, 10)],\n",
    "}\n",
    "\n",
    "for i, name in enumerate([\"name mover\", \"negative name mover\"]):\n",
    "    make_table(\n",
    "        title=f\"Copying Scores ({name} heads)\",\n",
    "        colnames=[\"Head\", \"Score\"],\n",
    "        cols=[\n",
    "            list(map(str, heads[name])) + [\"[dark_orange bold]Average\"],\n",
    "            [f\"{copying_results[i, layer, head]:.2%}\" for (layer, head) in heads[name]]\n",
    "            + [f\"[dark_orange bold]{copying_results[i].mean():.2%}\"],\n",
    "        ],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation of early heads\n",
    "\n",
    "Three different kinds of heads early in the circuit:\n",
    "\n",
    "1. Previous token heads\n",
    "\n",
    "2. Induction heads\n",
    "\n",
    "3. Duplicate token heads.\n",
    "\n",
    "Can validate all at once using a sequence of random `n` tokens followed by same `n` tokens repeated:\n",
    "\n",
    "1. Prev token heads, by measuring the attention patterns with an offset of one (i.e. one below the diagonal).\n",
    "\n",
    "2. Induction heads, by measuring the attention patterns with an offset of `n-1` (i.e. the second instance of a token paying attention to the token after its first instance).\n",
    "\n",
    "3. Duplicate token heads, by measuring the attention patterns with an offset of `n` (i.e. a token paying attention to its previous instance).\n",
    "\n",
    "In all three cases, if heads score close to 1 on these metrics, it's strong evidence that they are working as this type of head.\n",
    "\n",
    "**Note**: it's a leaky abstraction to say things like \"head X is an induction head\", since we're only observing it on a certain distribution. For instance, it's not clear what the role of induction heads and duplicate token heads is when there are no duplicates (they could in theory do something completely different).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: perform head validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_repeated_tokens(\n",
    "    model: HookedTransformer, seq_len: int, batch: int = 1\n",
    ") -> Float[Tensor, \"batch 2*seq_len\"]:\n",
    "    \"\"\"\n",
    "    Generates a sequence of repeated random tokens (no start token).\n",
    "    \"\"\"\n",
    "    rep_tokens_half = t.randint(0, model.cfg.d_vocab, (batch, seq_len), dtype=t.int64)\n",
    "    rep_tokens = t.cat([rep_tokens_half, rep_tokens_half], dim=-1).to(device)\n",
    "    return rep_tokens\n",
    "\n",
    "\n",
    "def get_attn_scores(\n",
    "    model: HookedTransformer,\n",
    "    seq_len: int,\n",
    "    batch: int,\n",
    "    head_type: Literal[\"duplicate\", \"prev\", \"induction\"],\n",
    ") -> Float[Tensor, \"n_layers n_heads\"]:\n",
    "    \"\"\"\n",
    "    Returns attention scores for sequence of duplicated tokens, for every head.\n",
    "    \"\"\"\n",
    "\n",
    "    tokens = generate_repeated_tokens(model, seq_len, batch)\n",
    "\n",
    "    _, cache = model.run_with_cache(\n",
    "        tokens, return_type=None, names_filter=lambda name: name.endswith(\"pattern\")\n",
    "    )\n",
    "\n",
    "    if head_type == \"duplicate\":\n",
    "        src_indices = range(seq_len)\n",
    "        dest_indices = range(seq_len, 2 * seq_len)\n",
    "\n",
    "    elif head_type == \"prev\":\n",
    "        src_indices = range(seq_len)\n",
    "        dest_indices = range(1, seq_len + 1)\n",
    "\n",
    "    elif head_type == \"induction\":\n",
    "        src_indices = range(1, seq_len + 1)\n",
    "        dest_indices = range(seq_len, 2 * seq_len)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid head type: '{head_type}'\")\n",
    "\n",
    "    result = t.zeros(\n",
    "        (model.cfg.n_layers, model.cfg.n_heads), device=device, dtype=t.float32\n",
    "    )\n",
    "    for layer in tqdm(range(model.cfg.n_layers), desc=\"Layers\"):\n",
    "        for head in range(model.cfg.n_heads):\n",
    "            attn_scores = cache[\"pattern\", layer]  # [batch head dest src]\n",
    "            result[layer, head] = (\n",
    "                attn_scores[:, head, dest_indices, src_indices].mean().item()\n",
    "            )\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def plot_early_head_validation_results(seq_len: int = 50, batch: int = 50):\n",
    "    \"\"\"\n",
    "    Produces a plot that looks like Figure 18 in the paper.\n",
    "    \"\"\"\n",
    "    head_types = [\"duplicate\", \"prev\", \"induction\"]\n",
    "\n",
    "    results = t.stack(\n",
    "        [\n",
    "            get_attn_scores(model, seq_len, batch, head_type=head_type)\n",
    "            for head_type in head_types\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    imshow(\n",
    "        results,\n",
    "        facet_col=0,\n",
    "        facet_labels=[\n",
    "            f\"{head_type.capitalize()} token attention prob.<br>on sequences of random tokens\"\n",
    "            for head_type in head_types\n",
    "        ],\n",
    "        labels={\"x\": \"Head\", \"y\": \"Layer\"},\n",
    "        width=1300,\n",
    "    )\n",
    "\n",
    "\n",
    "model.reset_hooks()\n",
    "plot_early_head_validation_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimal Circuit\n",
    "\n",
    "### Background: faithfulness, completeness, and minimality\n",
    "\n",
    "IOI paper authors developed three criteria for validating their circuit explanations:\n",
    "\n",
    "1. **Faithful**: the circuit can perform as well as the model\n",
    "   - equivalent to $F|(C) - F(M)|$ being small, where $C$ is the circuit, $M$ is the model, and $F$ is the performance metric function\n",
    "\n",
    "2. **Complete**: the circuit contains all nodes used to perform the task\n",
    "   - equivalent to $|F(C \\setminus K) - F(M \\setminus K)|$ being small for any subset $K \\subset C$, including when $K$ is the empty set, showing that completeness implies faithfulness.\n",
    "\n",
    "   - Although completeness implies faithfulness, faithfulness does *not* impliy completeness.\n",
    "\n",
    "   - Backup name mover heads illustrate this point. They are used in the task, and without understanding the role they play you'll have an incorrect model of reality\n",
    "     - E.g., you'll think ablating the name mover heads would destroy performance, which turns out not to be true.\n",
    "\n",
    "     - If you define a circuit that doesn't contain backup name mover heads then it will be faithful (the backup name mover heads won't be used) but not complete.\n",
    "\n",
    "     - I.e., is $K$ is the set of name mover heads, $C \\setminus K$ performs worse than $M \\setminus K$, because the latter contains backup name mover heads, while the former does not\n",
    "\n",
    "3. **Minimal**: the circuit does not contain nodes that are irrelevant to the task\n",
    "   - Non-minimal circuits may not be mechanistically understandable, which defeats the purpose of this kind of circuit analysis.\n",
    "\n",
    "If all three criteria are met, then the circuit is considered a reliable explanation for model behaviour."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Towards minimality\n",
    "\n",
    "- We've analysed most components of the circuit, now try ablating everything except those core components and verify model performance\n",
    "\n",
    "- Very large ablation!\n",
    "  - Everything except for the output of each of our key attention heads (e.g., duplicate token heads or S-inhibition heads) at a single sequence position. \n",
    "    - E.g., for the DTHs, this is the `S2` token; and for SIHs, this is the `end` token.\n",
    "  \n",
    "  - Given that our core circuit has 26 heads in total, and our sequences have length around 20 on average, this means we're ablating all but $(26/144)/20 \\approx 1\\%$ of our attention heads' output\n",
    "  \n",
    "    - Note that the number of possible paths through the model is reduced by ***much*** more than this\n",
    "\n",
    "- How to ablate?\n",
    "  - Zero ablation? Some non-obvious problems:\n",
    "    - Heads might be \"expecting\" non-zero input, and setting the input to zero is essentially an arbitrary choice which takes it off distribution.\n",
    "      - You can think of this as adding a bias term to this head, which might mess up subsequent computation and lead to noisy results.\n",
    "  \n",
    "  - Mean ablation?\n",
    "    - Set a head's output to its average output over `ioi_dataset`.\n",
    "\n",
    "    - Problem: taking the mean over this dataset might contain relevant information for solving the IOI task.\n",
    "      - E.g., the `is_duplicated` flag that gets written to S2 will be present for all sequences, so averaging won't remove this information.\n",
    "    \n",
    "    - Solution (for this task): ablate with the mean of the ABC dataset rather than the IOI dataset.\n",
    "      - Removes the problem of averages still containing relevant information from solving the IOI task.\n",
    "\n",
    "- Complication: the sentences have different templates, and the positions of tokens like `S` and `IO` are not consistent across these templates\n",
    "  - E.g.:\n",
    "    ```python\n",
    "    \"Then, [B] and [A] had a long argument and after that [B] said to [A]\"\n",
    "    \"After the lunch [B] and [A] went to the [PLACE], and [B] gave a [OBJECT] to [A]\"\n",
    "    ```\n",
    "   \n",
    "  - We avoided this problem in previous exercises by choosing a very small set of sentences, where all the important tokens had the same indices.\n",
    "\n",
    "  - Solution: take mean over each *template* and ablate, rather than whole dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: constructing the minimal circuit\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CIRCUIT = {\n",
    "    \"name mover\": [(9, 9), (10, 0), (9, 6)],\n",
    "    \"backup name mover\": [\n",
    "        (10, 10),\n",
    "        (10, 6),\n",
    "        (10, 2),\n",
    "        (10, 1),\n",
    "        (11, 2),\n",
    "        (9, 7),\n",
    "        (9, 0),\n",
    "        (11, 9),\n",
    "    ],\n",
    "    \"negative name mover\": [(10, 7), (11, 10)],\n",
    "    \"s2 inhibition\": [(7, 3), (7, 9), (8, 6), (8, 10)],\n",
    "    \"induction\": [(5, 5), (5, 8), (5, 9), (6, 9)],\n",
    "    \"duplicate token\": [(0, 1), (0, 10), (3, 0)],\n",
    "    \"previous token\": [(2, 2), (4, 11)],\n",
    "}\n",
    "\n",
    "SEQ_POS_TO_KEEP = {\n",
    "    \"name mover\": \"end\",\n",
    "    \"backup name mover\": \"end\",\n",
    "    \"negative name mover\": \"end\",\n",
    "    \"s2 inhibition\": \"end\",\n",
    "    \"induction\": \"S2\",\n",
    "    \"duplicate token\": \"S2\",\n",
    "    \"previous token\": \"S1+1\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_means_by_template(\n",
    "    means_dataset: IOIDataset, model: HookedTransformer\n",
    ") -> Float[Tensor, \"layer batch seq head_idx d_head\"]:\n",
    "    \"\"\"\n",
    "    Returns the mean of each head's output over the means dataset. This mean is\n",
    "    computed separately for each group of prompts with the same template (these\n",
    "    are given by means_dataset.groups).\n",
    "    \"\"\"\n",
    "    # Cache the outputs of every head\n",
    "    _, means_cache = model.run_with_cache(\n",
    "        means_dataset.toks.long(),\n",
    "        return_type=None,\n",
    "        names_filter=lambda name: name.endswith(\"z\"),\n",
    "    )\n",
    "\n",
    "    # Create tensor to store means\n",
    "    batch, seq_len = len(means_dataset), means_dataset.max_len\n",
    "    means = t.zeros(\n",
    "        size=(model.cfg.n_layers, batch, seq_len, model.cfg.n_heads, model.cfg.d_head),\n",
    "        device=model.cfg.device,\n",
    "    )\n",
    "\n",
    "    # Get set of different templates for this data\n",
    "    for layer in range(model.cfg.n_layers):\n",
    "        z_for_this_layer = means_cache[\n",
    "            utils.get_act_name(\"z\", layer)\n",
    "        ]  # [batch seq head d_head]\n",
    "\n",
    "        for template_group in means_dataset.groups:\n",
    "            z_for_this_template = z_for_this_layer[template_group]\n",
    "            z_means_for_this_template = einops.reduce(\n",
    "                z_for_this_template, \"batch seq head d_head -> seq head d_head\", \"mean\"\n",
    "            )\n",
    "            means[layer, template_group] = z_means_for_this_template\n",
    "\n",
    "    return means\n",
    "\n",
    "\n",
    "def get_heads_and_posns_to_keep(\n",
    "    means_dataset: IOIDataset,\n",
    "    model: HookedTransformer,\n",
    "    circuit: Dict[str, List[Tuple[int, int]]],\n",
    "    seq_pos_to_keep: Dict[str, str],\n",
    ") -> Dict[int, Bool[Tensor, \"batch seq head\"]]:\n",
    "    \"\"\"\n",
    "    Returns a dictionary mapping layers to a boolean mask giving the indices of the\n",
    "    z output which *shouldn't* be mean-ablated.\n",
    "\n",
    "    The output of this function will be used for the hook function that does ablation.\n",
    "    \"\"\"\n",
    "    heads_and_posns_to_keep = {}\n",
    "    batch, seq, n_heads = len(means_dataset), means_dataset.max_len, model.cfg.n_heads\n",
    "\n",
    "    for layer in range(model.cfg.n_layers):\n",
    "\n",
    "        mask = t.zeros(size=(batch, seq, n_heads))\n",
    "\n",
    "        for head_type, head_list in circuit.items():\n",
    "            seq_pos = seq_pos_to_keep[head_type]\n",
    "            indices = means_dataset.word_idx[seq_pos]\n",
    "\n",
    "            for layer_idx, head_idx in head_list:\n",
    "                if layer_idx == layer:\n",
    "                    mask[:, indices, head_idx] = 1\n",
    "\n",
    "        heads_and_posns_to_keep[layer] = mask.bool()\n",
    "\n",
    "    return heads_and_posns_to_keep\n",
    "\n",
    "\n",
    "def hook_fn_mask_z(\n",
    "    z: Float[Tensor, \"batch seq head d_head\"],\n",
    "    hook: HookPoint,\n",
    "    heads_and_posns_to_keep: Dict[int, Bool[Tensor, \"batch seq head\"]],\n",
    "    means: Float[Tensor, \"layer batch seq head d_head\"],\n",
    ") -> Float[Tensor, \"batch seq head d_head\"]:\n",
    "    \"\"\"\n",
    "    Hook function which masks the z output of a transformer head.\n",
    "\n",
    "    heads_and_posns_to_keep\n",
    "        Dict created with the get_heads_and_posns_to_keep function. This tells\n",
    "        us where to mask.\n",
    "\n",
    "    means\n",
    "        Tensor of mean z values of the means_dataset over each group of prompts\n",
    "        with the same template. This tells us what values to mask with.\n",
    "    \"\"\"\n",
    "    # Get the mask for this layer, and add d_head=1 dimension so it broadcasts correctly\n",
    "    mask_for_this_layer = (\n",
    "        heads_and_posns_to_keep[hook.layer()].unsqueeze(-1).to(z.device)\n",
    "    )\n",
    "\n",
    "    # Set z values to the mean\n",
    "    z = t.where(mask_for_this_layer, z, means[hook.layer()])\n",
    "\n",
    "    return z\n",
    "\n",
    "\n",
    "def add_mean_ablation_hook(\n",
    "    model: HookedTransformer,\n",
    "    means_dataset: IOIDataset,\n",
    "    circuit: Dict[str, List[Tuple[int, int]]] = CIRCUIT,\n",
    "    seq_pos_to_keep: Dict[str, str] = SEQ_POS_TO_KEEP,\n",
    "    is_permanent: bool = True,\n",
    ") -> HookedTransformer:\n",
    "    \"\"\"\n",
    "    Adds a permanent hook to the model, which ablates according to the circuit and\n",
    "    seq_pos_to_keep dictionaries.\n",
    "\n",
    "    In other words, when the model is run on ioi_dataset, every head's output will\n",
    "    be replaced with the mean over means_dataset for sequences with the same template,\n",
    "    except for a subset of heads and sequence positions as specified by the circuit\n",
    "    and seq_pos_to_keep dicts.\n",
    "    \"\"\"\n",
    "\n",
    "    model.reset_hooks(including_permanent=True)\n",
    "\n",
    "    # Compute the mean of each head's output on the ABC dataset, grouped by template\n",
    "    means = compute_means_by_template(means_dataset, model)\n",
    "\n",
    "    # Convert this into a boolean map\n",
    "    heads_and_posns_to_keep = get_heads_and_posns_to_keep(\n",
    "        means_dataset, model, circuit, seq_pos_to_keep\n",
    "    )\n",
    "\n",
    "    # Get a hook function which will patch in the mean z values for each head, at\n",
    "    # all positions which aren't important for the circuit\n",
    "    hook_fn = partial(\n",
    "        hook_fn_mask_z, heads_and_posns_to_keep=heads_and_posns_to_keep, means=means\n",
    "    )\n",
    "\n",
    "    # Apply hook\n",
    "    model.add_hook(lambda name: name.endswith(\"z\"), hook_fn, is_permanent=is_permanent)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ioi_circuit_extraction\n",
    "\n",
    "\n",
    "model = ioi_circuit_extraction.add_mean_ablation_hook(\n",
    "    model, means_dataset=abc_dataset, circuit=CIRCUIT, seq_pos_to_keep=SEQ_POS_TO_KEEP\n",
    ")\n",
    "\n",
    "ioi_logits_minimal = model(ioi_dataset.toks)\n",
    "\n",
    "print(\n",
    "    f\"Average logit difference (IOI dataset, using entire model): {logits_to_ave_logit_diff_2(ioi_logits_original):.4f}\"\n",
    ")\n",
    "print(\n",
    "    f\"Average logit difference (IOI dataset, only using circuit): {logits_to_ave_logit_diff_2(ioi_logits_minimal):.4f}\"\n",
    ")\n",
    "\n",
    "\n",
    "model = add_mean_ablation_hook(\n",
    "    model, means_dataset=abc_dataset, circuit=CIRCUIT, seq_pos_to_keep=SEQ_POS_TO_KEEP\n",
    ")\n",
    "\n",
    "ioi_logits_minimal = model(ioi_dataset.toks)\n",
    "\n",
    "print(\n",
    "    f\"Average logit difference (IOI dataset, using entire model): {logits_to_ave_logit_diff_2(ioi_logits_original):.4f}\"\n",
    ")\n",
    "print(\n",
    "    f\"Average logit difference (IOI dataset, only using circuit): {logits_to_ave_logit_diff_2(ioi_logits_minimal):.4f}\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
