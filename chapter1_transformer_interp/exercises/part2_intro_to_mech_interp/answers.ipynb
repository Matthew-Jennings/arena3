{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TransformerLens: Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import plotly.express as px\n",
    "import torch as t\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import einops\n",
    "from jaxtyping import Int, Float\n",
    "from typing import List, Optional, Tuple\n",
    "import functools\n",
    "from tqdm import tqdm\n",
    "from IPython.display import display\n",
    "import webbrowser\n",
    "import gdown\n",
    "from transformer_lens.hook_points import HookPoint\n",
    "from transformer_lens import utils, HookedTransformer, HookedTransformerConfig, FactoredMatrix, ActivationCache\n",
    "import circuitsvis as cv\n",
    "\n",
    "# Make sure exercises are in the path\n",
    "chapter = r\"chapter1_transformer_interp\"\n",
    "exercises_dir = Path(f\"{os.getcwd().split(chapter)[0]}/{chapter}/exercises\").resolve()\n",
    "section_dir = exercises_dir / \"part2_intro_to_mech_interp\"\n",
    "if str(exercises_dir) not in sys.path: sys.path.append(str(exercises_dir))\n",
    "\n",
    "from arena3.chapter1_transformer_interp.exercises.plotly_utils import imshow, hist, plot_comp_scores, plot_logit_attribution, plot_loss_difference\n",
    "from part1_transformer_from_scratch.solutions import get_log_probs\n",
    "import part2_intro_to_mech_interp.tests as tests\n",
    "\n",
    "# Saves computation time, since we don't need it for the contents of this notebook\n",
    "t.set_grad_enabled(False)\n",
    "\n",
    "device = t.device(\"cuda\" if t.cuda.is_available() else \"cpu\")\n",
    "\n",
    "MAIN = __name__ == \"__main__\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and Running Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2_small: HookedTransformer = HookedTransformer.from_pretrained(\"gpt2-small\")\n",
    "\n",
    "# Can instead define a model via   HookedTransformer.from_config(cfg)\n",
    "\n",
    "print(gpt2_small.cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_description_text = '''## Loading Models\n",
    "\n",
    "HookedTransformer comes loaded with >40 open source GPT-style models. You can load any of them in with `HookedTransformer.from_pretrained(MODEL_NAME)`. Each model is loaded into the consistent HookedTransformer architecture, designed to be clean, consistent and interpretability-friendly.\n",
    "\n",
    "For this demo notebook we'll look at GPT-2 Small, an 80M parameter model. To try the model the model out, let's find the loss on this paragraph!'''\n",
    "\n",
    "loss = gpt2_small(model_description_text, return_type=\"loss\")\n",
    "print(\"Model loss:\", loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gpt2_small.to_str_tokens(\"gpt2\"))\n",
    "print()\n",
    "print(gpt2_small.to_str_tokens([\"gpt2\", \"gpt2\"]))\n",
    "print()\n",
    "print(gpt2_small.to_tokens(\"gpt2\"))\n",
    "print()\n",
    "print(gpt2_small.to_string([50256, 70, 457, 17]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = gpt2_small(model_description_text, return_type=\"logits\")\n",
    "prediction = logits.argmax(dim=-1).squeeze()[:-1]\n",
    "expected = gpt2_small.to_tokens(model_description_text).squeeze()[1:]\n",
    "num_correct = (prediction == expected).sum()\n",
    "\n",
    "print(f\"Correctly identified {num_correct}/{len(true_tokens)}, or {num_correct/len(true_tokens)*100:.2f}% of tokens. Correct tokens:\")\n",
    "print(f\"{gpt2_small.to_str_tokens(prediction[prediction == expected])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caching all Activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2_text = \"Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspecific datasets.\"\n",
    "gpt2_tokens = gpt2_small.to_tokens(gpt2_text)\n",
    "gpt2_logits, gpt2_cache = gpt2_small.run_with_cache(gpt2_tokens, remove_batch_dim=True)\n",
    "\n",
    "print(gpt2_cache.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_patterns_layer_0 = gpt2_cache[\"pattern\", 0] # shorthand\n",
    "\n",
    "attn_patterns_layer_0_copy = gpt2_cache[\"blocks.0.attn.hook_pattern\"] # direct cache indexing\n",
    "\n",
    "t.testing.assert_close(attn_patterns_layer_0, attn_patterns_layer_0_copy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise:   Verify Activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer0_pattern_from_cache = gpt2_cache[\"pattern\", 0]\n",
    "\n",
    "layer0_attn_scores = einops.einsum(gpt2_cache[\"q\", 0], gpt2_cache[\"k\", 0], \"seqQ n h, seqK n h -> n seqQ seqK\")\n",
    "\n",
    "mask = t.triu(t.ones_like(layer0_attn_scores, dtype=bool), diagonal=1).to(device)\n",
    "\n",
    "layer0_attn_scores[mask] = -1.0e9\n",
    "layer0_pattern_from_q_and_k = (layer0_attn_scores / gpt2_small.cfg.d_head**0.5).softmax(dim=-1)\n",
    "\n",
    "t.testing.assert_close(layer0_pattern_from_cache, layer0_pattern_from_q_and_k)\n",
    "print(\"Tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualising Attention Heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(gpt2_cache))\n",
    "attention_pattern = gpt2_cache[\"pattern\", 0]\n",
    "print(attention_pattern.shape)\n",
    "gpt2_str_tokens = gpt2_small.to_str_tokens(gpt2_text)\n",
    "\n",
    "print(\"Layer 0 Head Attention Patterns:\")\n",
    "display(cv.attention.attention_patterns(\n",
    "    tokens=gpt2_str_tokens, \n",
    "    attention=attention_pattern,\n",
    "    attention_head_names=[f\"L0H{i}\" for i in range(12)],\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualising Neuron Activations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron_activations_for_all_layers = t.stack([\n",
    "    gpt2_cache[\"post\", layer] for layer in range(gpt2_small.cfg.n_layers)\n",
    "], dim=1)\n",
    "# shape = (seq_pos, layers, neurons)\n",
    "\n",
    "cv.activations.text_neuron_activations(\n",
    "    tokens=gpt2_str_tokens,\n",
    "    activations=neuron_activations_for_all_layers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron_activations_for_all_layers_rearranged = utils.to_numpy(einops.rearrange(neuron_activations_for_all_layers, \"seq layers neurons -> 1 layers seq neurons\"))\n",
    "\n",
    "cv.topk_tokens.topk_tokens(\n",
    "    # Some weird indexing required here ¯\\_(ツ)_/¯\n",
    "    tokens=[gpt2_str_tokens], \n",
    "    activations=neuron_activations_for_all_layers_rearranged,\n",
    "    max_k=7, \n",
    "    first_dimension_name=\"Layer\", \n",
    "    third_dimension_name=\"Neuron\",\n",
    "    first_dimension_labels=list(range(12))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding induction heads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toy Attention-Only Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = HookedTransformerConfig(\n",
    "    d_model=768,\n",
    "    d_head=64,\n",
    "    n_heads=12,\n",
    "    n_layers=2,\n",
    "    n_ctx=2048,\n",
    "    d_vocab=50278,\n",
    "    attention_dir=\"causal\",\n",
    "    attn_only=True, # defaults to False\n",
    "    tokenizer_name=\"EleutherAI/gpt-neox-20b\", \n",
    "    seed=398,\n",
    "    use_attn_result=True,\n",
    "    normalization_type=None, # defaults to \"LN\", i.e. layernorm with weights & biases\n",
    "    positional_embedding_type=\"shortformer\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "REPO_ID = \"callummcdougall/attn_only_2L_half\"\n",
    "FILENAME = \"attn_only_2L_half.pth\"\n",
    "\n",
    "weights_path = hf_hub_download(repo_id=REPO_ID, filename=FILENAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HookedTransformer(cfg)\n",
    "pretrained_weights = t.load(weights_path, map_location=device)\n",
    "model.load_state_dict(pretrained_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: visualise attention patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"We think that powerful, significantly superhuman machine intelligence is more likely than not to be created this century. If current machine learning techniques were scaled up to this level, we think they would by default produce systems that are deceptive or manipulative, and that no solid plans are known for how to avoid this.\"\n",
    "logits, cache = model.run_with_cache(text, remove_batch_dim=True)\n",
    "\n",
    "str_tokens = model.to_str_tokens(text)\n",
    "for layer_idx in range(model.cfg.n_layers):\n",
    "    attention_pattern = cache[\"pattern\", layer_idx]\n",
    "    display(cv.attention.attention_heads(tokens=str_tokens, attention=attention_pattern))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: write own detectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "THRESH = 0.4\n",
    "\n",
    "def current_attn_detector(cache: ActivationCache) -> List[str]:\n",
    "    '''\n",
    "    Returns a list e.g. [\"0.2\", \"1.4\", \"1.9\"] of \"layer.head\" which you judge to be current-token heads\n",
    "    '''\n",
    "    attn_heads = []\n",
    "    for layer in range(model.cfg.n_layers):\n",
    "        for head in range(model.cfg.n_heads):\n",
    "            attention_pattern = cache[\"pattern\", layer][head]\n",
    "            score = attention_pattern.diagonal().mean()\n",
    "            if score > THRESH:\n",
    "                attn_heads.append(f\"{layer}.{head} ({score:.4f})\")\n",
    "    return attn_heads\n",
    "\n",
    "def prev_attn_detector(cache: ActivationCache) -> List[str]:\n",
    "    '''\n",
    "    Returns a list e.g. [\"0.2\", \"1.4\", \"1.9\"] of \"layer.head\" which you judge to be prev-token heads\n",
    "    '''\n",
    "    attn_heads = []\n",
    "    for layer in range(model.cfg.n_layers):\n",
    "        for head in range(model.cfg.n_heads):\n",
    "            attention_pattern = cache[\"pattern\", layer][head]\n",
    "            score = attention_pattern.diagonal(-1).mean()\n",
    "            if score > THRESH:\n",
    "                attn_heads.append(f\"{layer}.{head} ({score:.4f})\")\n",
    "    return attn_heads\n",
    "\n",
    "def first_attn_detector(cache: ActivationCache) -> List[str]:\n",
    "    '''\n",
    "    Returns a list e.g. [\"0.2\", \"1.4\", \"1.9\"] of \"layer.head\" which you judge to be first-token heads\n",
    "    '''\n",
    "    attn_heads = []\n",
    "    for layer in range(model.cfg.n_layers):\n",
    "        for head in range(model.cfg.n_heads):\n",
    "            attention_pattern = cache[\"pattern\", layer][head]\n",
    "            score = attention_pattern[:, 0].mean()\n",
    "            if score > THRESH:\n",
    "                attn_heads.append(f\"{layer}.{head} ({score:.4f})\")\n",
    "    return attn_heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Heads attending to current token  = \", \", \".join(current_attn_detector(cache)))\n",
    "print(\"Heads attending to previous token = \", \", \".join(prev_attn_detector(cache)))\n",
    "print(\"Heads attending to first token    = \", \", \".join(first_attn_detector(cache)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Different text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text2 = \"The goal of mechanistic interpretability is to take a trained model and reverse engineer the algorithms the model learned during training from its weights. It is a fact about the world today that we have computer programs that can essentially speak English at a human level (GPT-3, PaLM, etc), yet we have no idea how they work nor how to write one ourselves. This offends me greatly, and I would like to solve this! Mechanistic interpretability is a very young and small field, and there are a lot of open problems - if you would like to help, please try working on one!\"\n",
    "logits2, cache2 = model.run_with_cache(text2, remove_batch_dim=True)\n",
    "\n",
    "# str_tokens2 = model.to_str_tokens(text2)\n",
    "# for layer_idx in range(model.cfg.n_layers):\n",
    "#     attention_pattern = cache[\"pattern\", layer_idx]\n",
    "#     display(cv.attention.attention_heads(tokens=str_tokens2, attention=attention_pattern))\n",
    "\n",
    "print(\"Heads attending to current token  = \", \", \".join(current_attn_detector(cache2)))\n",
    "print(\"Heads attending to previous token = \", \", \".join(prev_attn_detector(cache2)))\n",
    "print(\"Heads attending to first token    = \", \", \".join(first_attn_detector(cache2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**!!! Same heads !!!** Similar scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking for the induction capability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: plot per-token loss on repeated sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_repeated_tokens(\n",
    "    model: HookedTransformer, seq_len: int, batch: int = 1\n",
    ") -> Int[Tensor, \"batch full_seq_len\"]:\n",
    "    '''\n",
    "    Generates a sequence of repeated random tokens\n",
    "\n",
    "    Outputs are:\n",
    "        rep_tokens: [batch, 1+2*seq_len]\n",
    "    '''\n",
    "    prefix = (t.ones(batch, 1) * model.tokenizer.bos_token_id).long().to(device)\n",
    "    tokens = t.randint(0, model.cfg.d_vocab, (batch, seq_len), dtype=t.long, device=device)\n",
    "    return t.cat([prefix, tokens, tokens], dim=-1).to(device)\n",
    "\n",
    "def run_and_cache_model_repeated_tokens(model: HookedTransformer, seq_len: int, batch: int = 1) -> Tuple[t.Tensor, t.Tensor, ActivationCache]:\n",
    "    '''\n",
    "    Generates a sequence of repeated random tokens, and runs the model on it, returning logits, tokens and cache\n",
    "\n",
    "    Should use the `generate_repeated_tokens` function above\n",
    "\n",
    "    Outputs are:\n",
    "        rep_tokens: [batch, 1+2*seq_len]\n",
    "        rep_logits: [batch, 1+2*seq_len, d_vocab]\n",
    "        rep_cache: The cache of the model run on rep_tokens\n",
    "    '''\n",
    "    # SOLUTION\n",
    "    tokens = generate_repeated_tokens(model, seq_len, batch)\n",
    "    logits, cache = model.run_with_cache(tokens)\n",
    "    return tokens, logits, cache\n",
    "\n",
    "\n",
    "seq_len = 50\n",
    "batch = 1\n",
    "(rep_tokens, rep_logits, rep_cache) = run_and_cache_model_repeated_tokens(model, seq_len, batch)\n",
    "rep_cache.remove_batch_dim()\n",
    "rep_str = model.to_str_tokens(rep_tokens)\n",
    "model.reset_hooks()\n",
    "log_probs = get_log_probs(rep_logits, rep_tokens).squeeze()\n",
    "\n",
    "print(f\"Performance on the first half: {log_probs[:seq_len].mean():.3f}\")\n",
    "print(f\"Performance on the second half: {log_probs[seq_len:].mean():.3f}\")\n",
    "\n",
    "plot_loss_difference(log_probs, rep_str, seq_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking for Induction Attention Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in range(model.cfg.n_layers):\n",
    "    attention_pattern = rep_cache[\"pattern\", layer]\n",
    "    display(cv.attention.attention_heads(tokens=rep_str, attention=attention_pattern))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: make an induction detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rep_cache['hook_embed'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def induction_attn_detector(cache: ActivationCache) -> List[str]:\n",
    "    '''\n",
    "    Returns a list e.g. [\"0.2\", \"1.4\", \"1.9\"] of \"layer.head\" which you judge to be induction heads\n",
    "\n",
    "    Remember - the tokens used to generate rep_cache are (bos_token, *rand_tokens, *rand_tokens)\n",
    "    '''\n",
    "    # \"'1 - ' because we're attending to the token *following* previous occurence of the current token\"\n",
    "    offset = 1 - ((cache[\"hook_embed\"].shape[0] - 1) // 2)\n",
    "    attn_heads = []\n",
    "    for layer in range(model.cfg.n_layers):\n",
    "        for head in range(model.cfg.n_heads):\n",
    "            attention_pattern = cache[\"pattern\", layer][head]\n",
    "            score = attention_pattern.diagonal(offset=offset).mean()\n",
    "            if score > 0.4:\n",
    "                attn_heads.append(f\"{layer}.{head}\")\n",
    "    return attn_heads\n",
    "\n",
    "\n",
    "print(\"Induction heads = \", \", \".join(induction_attn_detector(rep_cache)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TransformerLens: Hooks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: calculate induction scores with hooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 50\n",
    "batch = 10\n",
    "rep_tokens_10 = generate_repeated_tokens(model, seq_len, batch)\n",
    "\n",
    "# We make a tensor to store the induction score for each head.\n",
    "# We put it on the model's device to avoid needing to move things between the GPU and CPU, which can be slow.\n",
    "induction_score_store = t.zeros((model.cfg.n_layers, model.cfg.n_heads), device=model.cfg.device)\n",
    "\n",
    "\n",
    "def induction_score_hook(\n",
    "    pattern: Float[Tensor, \"batch head_index dest_pos source_pos\"],\n",
    "    hook: HookPoint,\n",
    "):\n",
    "    '''\n",
    "    Calculates the induction score, and stores it in the [layer, head] position of the `induction_score_store` tensor.\n",
    "    '''\n",
    "    diag_offset = 1 - seq_len\n",
    "\n",
    "    induction_score_store[hook.layer()][:] = pattern.diagonal(offset=diag_offset, dim1=-2, dim2=-1).mean(dim=(0, 2))\n",
    "\n",
    "pattern_hook_names_filter = lambda name: name.endswith(\"pattern\")\n",
    "\n",
    "# Run with hooks (this is where we write to the `induction_score_store` tensor`)\n",
    "model.run_with_hooks(\n",
    "    rep_tokens_10, \n",
    "    return_type=None, # For efficiency, we don't need to calculate the logits\n",
    "    fwd_hooks=[(\n",
    "        pattern_hook_names_filter,\n",
    "        induction_score_hook\n",
    "    )]\n",
    ")\n",
    "\n",
    "# Plot the induction scores for each head in each layer\n",
    "imshow(\n",
    "    induction_score_store, \n",
    "    labels={\"x\": \"Head\", \"y\": \"Layer\"}, \n",
    "    title=\"Induction Score by Head\", \n",
    "    text_auto=\".2f\",\n",
    "    width=900, height=400\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: find induction heads in GPT2-small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_pattern_hook(\n",
    "    pattern: Float[Tensor, \"batch head_index dest_pos source_pos\"],\n",
    "    hook: HookPoint,\n",
    "):\n",
    "    print(\"Layer: \", hook.layer())\n",
    "    display(\n",
    "        cv.attention.attention_patterns(\n",
    "            tokens=gpt2_small.to_str_tokens(rep_tokens[0]), \n",
    "            attention=pattern.mean(0)\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 50\n",
    "batch = 10\n",
    "tokens = generate_repeated_tokens(gpt2_small, seq_len, batch)\n",
    "print(f\"Tokens: {tokens}\")\n",
    "print(f\"Layers: {gpt2_small.cfg.n_layers}\")\n",
    "print(f\"Heads: {gpt2_small.cfg.n_heads}\")\n",
    "\n",
    "induction_score_store = t.zeros((gpt2_small.cfg.n_layers, gpt2_small.cfg.n_heads), device=gpt2_small.cfg.device)\n",
    "\n",
    "gpt2_small.run_with_hooks(\n",
    "    tokens, \n",
    "    return_type=None, # For efficiency, we don't need to calculate the logits\n",
    "    fwd_hooks=[(\n",
    "        pattern_hook_names_filter,\n",
    "        induction_score_hook\n",
    "    )]\n",
    ")\n",
    "\n",
    "imshow(\n",
    "    induction_score_store, \n",
    "    labels={\"x\": \"Head\", \"y\": \"Layer\"},\n",
    "    title=\"Induction Score by Head\", \n",
    "    text_auto=\".1f\",\n",
    "    width=800\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Heads 5.1, 5.5, 6.9... strongly inductiony\n",
    "Also check out 9.0 out of interest. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for induction_head_layer in [5, 9]:\n",
    "    gpt2_small.run_with_hooks(\n",
    "        rep_tokens, \n",
    "        return_type=None, # For efficiency, we don't need to calculate the logits\n",
    "        fwd_hooks=[\n",
    "            (utils.get_act_name(\"pattern\", induction_head_layer), visualize_pattern_hook)\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building interpretability tools "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: build logit attribution tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logit_attribution(\n",
    "    embed: Float[Tensor, \"seq d_model\"],\n",
    "    l1_results: Float[Tensor, \"seq nheads d_model\"],\n",
    "    l2_results: Float[Tensor, \"seq nheads d_model\"],\n",
    "    W_U: Float[Tensor, \"d_model d_vocab\"],\n",
    "    tokens: Int[Tensor, \"seq\"]\n",
    ") -> Float[Tensor, \"seq-1 n_components\"]:\n",
    "    '''\n",
    "    Inputs:\n",
    "        embed: the embeddings of the tokens (i.e. token + position embeddings)\n",
    "        l1_results: the outputs of the attention heads at layer 1 (with head as one of the dimensions)\n",
    "        l2_results: the outputs of the attention heads at layer 2 (with head as one of the dimensions)\n",
    "        W_U: the unembedding matrix\n",
    "        tokens: the token ids of the sequence\n",
    "\n",
    "    Returns:\n",
    "        Tensor of shape (seq_len-1, n_components)\n",
    "        represents the concatenation (along dim=-1) of logit attributions from:\n",
    "            the direct path (seq-1,1)\n",
    "            layer 0 logits (seq-1, n_heads)\n",
    "            layer 1 logits (seq-1, n_heads)\n",
    "        so n_components = 1 + 2*n_heads\n",
    "    '''\n",
    "    seq_len_adj = tokens.shape[0] - 1\n",
    "    W_U_correct_tokens = W_U[:, tokens[1:]]\n",
    "\n",
    "    embed_out = einops.einsum(embed[:-1, :], W_U_correct_tokens, \"seq_len_adj d_model, d_model seq_len_adj -> seq_len_adj\")\n",
    "    l1_out = einops.einsum(l1_results[:-1, :, :], W_U_correct_tokens, \"seq_len_adj nheads d_model, d_model seq_len_adj -> seq_len_adj nheads\")\n",
    "    l2_out = einops.einsum(l2_results[:-1, :, :], W_U_correct_tokens, \"seq_len_adj nheads d_model, d_model seq_len_adj -> seq_len_adj nheads\")\n",
    "\n",
    "    return t.cat([embed_out.unsqueeze(-1), l1_out, l2_out], dim=-1)\n",
    "\n",
    "\n",
    "text = \"We think that powerful, significantly superhuman machine intelligence is more likely than not to be created this century. If current machine learning techniques were scaled up to this level, we think they would by default produce systems that are deceptive or manipulative, and that no solid plans are known for how to avoid this.\"\n",
    "logits, cache = model.run_with_cache(text, remove_batch_dim=True)\n",
    "str_tokens = model.to_str_tokens(text)\n",
    "tokens = model.to_tokens(text)\n",
    "\n",
    "with t.inference_mode():\n",
    "    embed = cache[\"embed\"]\n",
    "    l1_results = cache[\"result\", 0]\n",
    "    l2_results = cache[\"result\", 1]\n",
    "    logit_attr = logit_attribution(embed, l1_results, l2_results, model.W_U, tokens[0])\n",
    "    # Uses fancy indexing to get a len(tokens[0])-1 length tensor, where the kth entry is the predicted logit for the correct k+1th token\n",
    "    correct_token_logits = logits[0, t.arange(len(tokens[0]) - 1), tokens[0, 1:]]\n",
    "    t.testing.assert_close(logit_attr.sum(1), correct_token_logits, atol=1e-3, rtol=0)\n",
    "    print(\"Tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualise logit attributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = cache[\"embed\"]\n",
    "l1_results = cache[\"result\", 0]\n",
    "l2_results = cache[\"result\", 1]\n",
    "logit_attr = logit_attribution(embed, l1_results, l2_results, model.W_U, tokens[0])\n",
    "\n",
    "plot_logit_attribution(model, logit_attr, tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "\n",
    "1. Highest logit attribution comes from direct path (embed). High attributions in direct path are `| super|_7`, `| more|_12` `| machine|_24` and especially `| manip|_46`. Why? They are **offer very probably bigrams**. Examples are words split in two (e.g., \"manipulative\"), or words that are often paired with another. E.g., \"more likely\"\n",
    "\n",
    "2. The second layer seems to contribute more than that first layer. Why? Attributions from a layer cannot pick up on a head's effect in composition within another head. Layer 0 therefore includes no compositional effects. In contrast, attributions for layer 1 heads do not only include the paths through those heads, but also the compositional path through layer 1 combined with heads in layer 0.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: logit attribution for the induction heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 50\n",
    "\n",
    "embed = rep_cache[\"embed\"]\n",
    "l1_results = rep_cache[\"result\", 0]\n",
    "l2_results = rep_cache[\"result\", 1]\n",
    "first_half_tokens = rep_tokens[0, : 1 + seq_len]\n",
    "second_half_tokens = rep_tokens[0, seq_len:]\n",
    "\n",
    "# (each with a single call to the `logit_attribution` function)\n",
    "first_half_logit_attr = logit_attribution(embed[:seq_len+1], l1_results[:seq_len+1], l2_results[:seq_len+1], model.W_U, first_half_tokens)\n",
    "second_half_logit_attr = logit_attribution(embed[seq_len:], l1_results[seq_len:], l2_results[seq_len:], model.W_U, second_half_tokens)\n",
    "\n",
    "assert first_half_logit_attr.shape == (seq_len, 2*model.cfg.n_heads + 1)\n",
    "assert second_half_logit_attr.shape == (seq_len, 2*model.cfg.n_heads + 1)\n",
    "\n",
    "plot_logit_attribution(model, first_half_logit_attr, first_half_tokens, \"Logit attribution (first half of repeated sequence)\")\n",
    "plot_logit_attribution(model, second_half_logit_attr, second_half_tokens, \"Logit attribution (second half of repeated sequence)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpretation:\n",
    "1. Logit attribution for first hald of the sequence (first plot) is pretty meaningless. This sequence is random - no structure.\n",
    "2. Previously, we observed that heads 1.4 and 1.10 seemed to be acting as induction heads. This plot gives further evidence that this is the case, since these two heads have a large logit attribution scores on sequences in which the only(?) way to get accurate predictions is to use the induction mechanism. Also, like our attention scores result, 1.10 is a stronger (induction) head than 1.4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
