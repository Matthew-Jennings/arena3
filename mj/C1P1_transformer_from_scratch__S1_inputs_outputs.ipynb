{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\matth\\Workspace\\arena3\\.venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1602: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\matth\\Workspace\\arena3\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1160: UserWarning: expandable_segments not supported on this platform (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\c10/cuda/CUDAAllocatorConfig.h:28.)\n",
      "  return t.to(\n"
     ]
    }
   ],
   "source": [
    "from transformer_lens import HookedTransformer\n",
    "import torch as t\n",
    "\n",
    "device = t.device(\n",
    "    \"mps\"\n",
    "    if t.backends.mps.is_available()\n",
    "    else \"cuda\"\n",
    "    if t.cuda.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "\n",
    "MAIN = __name__ == \"__main__\"\n",
    "\n",
    "reference_gpt2 = HookedTransformer.from_pretrained(\n",
    "    \"gpt2-small\",\n",
    "    fold_ln=False,\n",
    "    center_unembed=False,\n",
    "    center_writing_weights=False,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Inputs & Outputs of a Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Objectives\n",
    "\n",
    "- Understand what a transformer is used for.\n",
    "\n",
    "- Understand causal attention, and what a transformer's output represents.\n",
    "\n",
    "- Learn what tokenisation is, and how models do it.\n",
    "\n",
    "- Understand what logits are, and how to use them to derive a probability distribution over the vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the point of a transformer?\n",
    "\n",
    "**Transformers exist to model text**\n",
    "\n",
    "Focus:  GPT-2 style transformers\n",
    "\n",
    "Key feature: They generate text! You feed in language, and the model generates a probability distribution over tokens. Repeatedly sample this to generate text.\n",
    "\n",
    "More detail:\n",
    "1. feed in a sequence of length $n$\n",
    "2. sample from the probability distribution over the $n+1$-th word/token\n",
    "3. use this to construct a new sequence of length $n+1$\n",
    "4. feed this new sequence into the model to get the probability distribution over the $n+2$-th word/token\n",
    "5. and so on. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How is the model trained?\n",
    "\n",
    "Provide a block of text, and train the model to predict the next word/token\n",
    "\n",
    "If the model is provided with 100 tokens in a sequence, the model *predicts the next token for **each** prefix*.\n",
    "- I.e., it produces 100 logit vectors (representing probability distributions) over the set of all words/tokens in the model's vocabulary, where the $i$-th logit vector represents the probability distribution over the token *following* the $i$-th token in the sequence.\n",
    "\n",
    "This makes transformers efficient to train. For every sequence of length $n$, we get $n$ different predictions to train on:\n",
    "\n",
    "$$\n",
    "p(x_1), p(x_2 | x_1), p(x_3 | x_1 x_2), ..., p(x_n | x_1...x_{n-1})\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aside - logits\n",
    "\n",
    "Given an arbitrary vector $x$ of dimension $m$, we can turn it into a probability distribution via the **softmax** function:\n",
    "\n",
    "$$\n",
    "x_i → \\frac{e^{x_i}}{\\sum^m_j{e^{x_j}}}\n",
    "$$\n",
    "\n",
    "- the exponential makes everything positive\n",
    "- the normalization makes it add to one.\n",
    "\n",
    "The model's output is the vector $x$ (one for each prediction it makes). We call this vector a **logit** because it represents a probability distribution, and it is related to the actual probabilities via the softmax function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*How do we stop the transformer by \"cheating\" by just looking at the tokens it's trying to predict?*\n",
    "\n",
    "The transformer has **causal attention** (as opposed to bidirectional attention). Causal attention only allows information to move forwards in the sequence, never backwards. The prediction of what comes after token 50 is only a function of the first 50 tokens, not of token 51. We say the transformer is **autoregressive**, because it only predicts future words based on past data.\n",
    "\n",
    "See [this illustration](./C1P1_transformer_output.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokens - Transformer Inputs\n",
    "\n",
    "- Transformer input is natural language; i.e., a sequence of characters/strings.\n",
    "- ML models take numeric vectors as input, not language. How to convert?\n",
    "\n",
    "Split into two questions:\n",
    "1. How to split language into small sub-units?\n",
    "2. How to convert these into numeric vectors? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Converting sub-units into vectors\n",
    "\n",
    "- Create a lookup table called an **embedding**, which contains one vector of dimension $d_{embed}$ for each possible sub-unit of language we expect.\n",
    "\n",
    "- The set of all sub-units is the model's **vocabulary**.\n",
    "\n",
    "- Every element in the vocabulary is labelled with an integer, which never changes. This integer is used to index the embedding.\n",
    "\n",
    "- Every word has a completely separate embedding vector; no relationship between words is introduced/represented when embedding is performed.\n",
    "\n",
    "- Indexing the embedding is equivalent to multiplying the **embedding matrix** $W_E$ by the one-hot encoding.\n",
    "  - The embedding matrix is the row-stack of all embedding vectors:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "W_E &= \\begin{bmatrix}\n",
    "\\leftarrow v_0 \\rightarrow \\\\\n",
    "\\leftarrow v_1 \\rightarrow \\\\\n",
    "\\vdots \\\\\n",
    "\\leftarrow v_{d_{vocab}-1} \\rightarrow \\\\\n",
    "\\end{bmatrix} \\quad \\text{is the embedding matrix (size }d_{vocab} \\times d_{embed}\\text{),} \\\\\n",
    "\\\\\n",
    "t_i &= (0, \\dots, 0, 1, 0, \\dots, 0) \\quad \\text{is the one-hot encoding for the }i\\text{th word (length }d_{vocab}\\text{)} \\\\\n",
    "\\\\\n",
    "v_i &= t_i W_E \\quad \\text{is the embedding vector for the }i\\text{th word (length }d_{embed}\\text{).} \\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Splitting language into sub-units (tokens)\n",
    "\n",
    "#### How?\n",
    "\n",
    "- Use the set of all words in a dictionary?\n",
    "  - This doesn't handle aribtary text, such as URLs or punctuation.\n",
    "\n",
    "- Use the 256 ASCII characters?\n",
    "  - Fixes the previous problem.\n",
    "\n",
    "  - Loses *structure* of language.\n",
    "  \n",
    "    - Some sequences of characters are more meaningful than others: \"language\" is more meaningful than \"ngfaslghflv\".\n",
    "    - Want an efficient vocabulary; so want \"language\" to be a token, but not \"ngfaslghflv\".\n",
    "  \n",
    "#### Most common strategy: **Byte-Pair encodings**\n",
    "\n",
    "- Begin with the 256 ASCII characters\n",
    "\n",
    "- Find the most common pair of tokens and merge into a new token\n",
    "\n",
    "**Note**: the *space* character is one of the 256 ASCII tokens. Merges including *space* are very common. E.g., the 5 first merges for GPT-2's tokeniser are:\n",
    "\n",
    "```python\n",
    "\" t\"\n",
    "\" a\"\n",
    "\"he\"\n",
    "\"in\"\n",
    "\"re\"\n",
    "```\n",
    "\n",
    "**Note**: The character `Ġ` prefixes some tokens. This is a special token that indicates that the tokwn begins with a space. *Tokens with a leading space are different from those without a leading space*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GPT-2's tokeniser's vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 20 tokens in GPT-2 vocab:\n",
      "[('!', 0),\n",
      " ('\"', 1),\n",
      " ('#', 2),\n",
      " ('$', 3),\n",
      " ('%', 4),\n",
      " ('&', 5),\n",
      " (\"'\", 6),\n",
      " ('(', 7),\n",
      " (')', 8),\n",
      " ('*', 9),\n",
      " ('+', 10),\n",
      " (',', 11),\n",
      " ('-', 12),\n",
      " ('.', 13),\n",
      " ('/', 14),\n",
      " ('0', 15),\n",
      " ('1', 16),\n",
      " ('2', 17),\n",
      " ('3', 18),\n",
      " ('4', 19)]\n",
      "\n",
      "251st to 270th token in GPT-2 vocab:\n",
      "[('ľ', 250),\n",
      " ('Ŀ', 251),\n",
      " ('ŀ', 252),\n",
      " ('Ł', 253),\n",
      " ('ł', 254),\n",
      " ('Ń', 255),\n",
      " ('Ġt', 256),\n",
      " ('Ġa', 257),\n",
      " ('he', 258),\n",
      " ('in', 259),\n",
      " ('re', 260),\n",
      " ('on', 261),\n",
      " ('Ġthe', 262),\n",
      " ('er', 263),\n",
      " ('Ġs', 264),\n",
      " ('at', 265),\n",
      " ('Ġw', 266),\n",
      " ('Ġo', 267),\n",
      " ('en', 268),\n",
      " ('Ġc', 269)]\n",
      "\n",
      "991st to 1010th token in GPT-2 vocab:\n",
      "[('Ġprodu', 990),\n",
      " ('Ġstill', 991),\n",
      " ('led', 992),\n",
      " ('ah', 993),\n",
      " ('Ġhere', 994),\n",
      " ('Ġworld', 995),\n",
      " ('Ġthough', 996),\n",
      " ('Ġnum', 997),\n",
      " ('arch', 998),\n",
      " ('imes', 999),\n",
      " ('ale', 1000),\n",
      " ('ĠSe', 1001),\n",
      " ('ĠIf', 1002),\n",
      " ('//', 1003),\n",
      " ('ĠLe', 1004),\n",
      " ('Ġret', 1005),\n",
      " ('Ġref', 1006),\n",
      " ('Ġtrans', 1007),\n",
      " ('ner', 1008),\n",
      " ('ution', 1009)]\n",
      "\n",
      "Final 20 tokens in GPT-2 vocab:\n",
      "[('Revolution', 50237),\n",
      " ('Ġsnipers', 50238),\n",
      " ('Ġreverted', 50239),\n",
      " ('Ġconglomerate', 50240),\n",
      " ('Terry', 50241),\n",
      " ('794', 50242),\n",
      " ('Ġharsher', 50243),\n",
      " ('Ġdesolate', 50244),\n",
      " ('ĠHitman', 50245),\n",
      " ('Commission', 50246),\n",
      " ('Ġ(/', 50247),\n",
      " ('âĢ¦.\"', 50248),\n",
      " ('Compar', 50249),\n",
      " ('Ġamplification', 50250),\n",
      " ('ominated', 50251),\n",
      " ('Ġregress', 50252),\n",
      " ('ĠCollider', 50253),\n",
      " ('Ġinformants', 50254),\n",
      " ('Ġgazed', 50255),\n",
      " ('<|endoftext|>', 50256)]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "sorted_vocab = sorted(list(reference_gpt2.tokenizer.vocab.items()), key=lambda n: n[1])\n",
    "print(\"First 20 tokens in GPT-2 vocab:\")\n",
    "pprint(sorted_vocab[:20])\n",
    "print(\"\\n251st to 270th token in GPT-2 vocab:\")\n",
    "pprint(sorted_vocab[250:270])\n",
    "print(\"\\n991st to 1010th token in GPT-2 vocab:\")\n",
    "pprint(sorted_vocab[990:1010])\n",
    "print(\"\\nFinal 20 tokens in GPT-2 vocab:\")\n",
    "pprint(sorted_vocab[-20:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First tokens with 3, 4, 5, 6, and 7 characters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3: ing\n",
      "4: Ġthe\n",
      "5: Ġthat\n",
      "6: Ġtheir\n",
      "7: Ġpeople\n"
     ]
    }
   ],
   "source": [
    "length_toptoken_pairs = dict.fromkeys(range(3, 8), \"\")\n",
    "for tok, idx in sorted_vocab:\n",
    "    if not length_toptoken_pairs.get(len(tok), True):\n",
    "        length_toptoken_pairs[len(tok)] = tok\n",
    "\n",
    "for length, tok in length_toptoken_pairs.items():\n",
    "    print(f\"{length}: {tok}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BOS token\n",
    "\n",
    "The BOS token is a special token used to mark the **B**eginning **O**f a **S**equence. This token:\n",
    "\n",
    "- Provides context that this is the start of a sequence, which can help the model generate more appropriate text.\n",
    "- Can act as a \"rest position\" for attention heads (discussed later)\n",
    "\n",
    "`TransformerLens` adds this token automatically, including in forward passes of transformer mdoels. E.g., it's implicitly added when `model()` is called.\n",
    "- You can disable this behaviour by setting `prepend_bos=False` in `to_tokens`, `to_str_tokens`, `model.forward`, and any other function/method that converts strings to multi-token tensors.\n",
    "\n",
    "Note: if you get weird off-by-one errors, check whether there's an unexpected `prepend_bos`!\n",
    "\n",
    "- Confusingly, in GPT-2, the End of Sequence (EOS), Beginning of Sequence (BOS), and Padding (PAD) tokens are all the same: `<|endoftext\\>` with index `50256`\n",
    "\n",
    "  - Why? GPT-2 is an autoregressive model. It has no need to distinguish between BOS and EOS tokens, since it only processes text from left to right. In contrast, other transformer families like BERT benefit from distinct BOS and EOS tokens. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some tokenisation annoyances:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Whether or not a word begins with a capital letter or space matters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<|endoftext|>', 'R', 'alph']\n",
      "['<|endoftext|>', ' Ralph']\n",
      "['<|endoftext|>', ' r', 'alph']\n",
      "['<|endoftext|>', 'ral', 'ph']\n"
     ]
    }
   ],
   "source": [
    "print(reference_gpt2.to_str_tokens(\"Ralph\"))\n",
    "print(reference_gpt2.to_str_tokens(\" Ralph\"))\n",
    "print(reference_gpt2.to_str_tokens(\" ralph\"))\n",
    "print(reference_gpt2.to_str_tokens(\"ralph\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Arithmetic is a mess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<|endoftext|>', '568', '73', '+', '318', '46', '23', '=', '123', '45', '67', '89', '-', '1', '000000', '000']\n"
     ]
    }
   ],
   "source": [
    "print(reference_gpt2.to_str_tokens(\"56873+3184623=123456789-1000000000\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Takeaways\n",
    "\n",
    "- We learn a dictionary of $d_{vocab}$ tokens (\"sub-words\")\n",
    "\n",
    "- We (approximately) losslessly convert language to integers viz tokenisation\n",
    "\n",
    "- We convert integers to vectors via lookup table. This is called **embedding**\n",
    "\n",
    "- **Note**: a transformer's input is a sequence of *tokens*, not vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Convert text to tokens\n",
    "\n",
    "A sequence gets tokenised:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[50256,    40,   716,   281,  4998,  1960,   382, 19741,    11,   875,\n",
      "         12342,    12,  8807,    11,   402, 11571,    12,    17,  3918, 47385,\n",
      "            13,  1881,  1110,   314,   481,  7074,  1692,  1241,  4430,   290,\n",
      "          1011,   625,   262,   995,     0]], device='cuda:0')\n",
      "torch.Size([1, 35])\n",
      "['<|endoftext|>', 'I', ' am', ' an', ' amazing', ' aut', 'ore', 'gressive', ',', ' dec', 'oder', '-', 'only', ',', ' G', 'PT', '-', '2', ' style', ' transformer', '.', ' One', ' day', ' I', ' will', ' exceed', ' human', ' level', ' intelligence', ' and', ' take', ' over', ' the', ' world', '!']\n"
     ]
    }
   ],
   "source": [
    "reference_text = \"I am an amazing autoregressive, decoder-only, GPT-2 style transformer. One day I will exceed human level intelligence and take over the world!\"\n",
    "\n",
    "tokens = reference_gpt2.to_tokens(reference_text).to(device)\n",
    "str_tokens = reference_gpt2.to_str_tokens(tokens)\n",
    "\n",
    "print(tokens)\n",
    "print(tokens.shape)\n",
    "print(str_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `tokens` has shape `(batch, seq_len)`. Here, the batch dimension is just `1`, since we only supplied one sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Map tokens to logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 35, 50257])\n"
     ]
    }
   ],
   "source": [
    "logits, cache = reference_gpt2.run_with_cache(tokens, device=device)\n",
    "print(logits.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From our input `tokens` of shape of `(batch, seq_len)`, we get an output `logits` of shape `(batch, seq_len, d_vocab)`\n",
    "\n",
    "The `[i, j, :]`-th element of our output is a vector of logits representing our prediction for the `j+1`-th token in the `i`-th sequence.\n",
    "\n",
    "**Note**: `run-with-cache` tells the model to cache all intermediate activations. Discussed later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Convert the logits into a probability distribution using *softmax*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 35, 50257])\n"
     ]
    }
   ],
   "source": [
    "probs = logits.softmax(dim=-1)  # Softmax over the d_vocab dimension\n",
    "print(probs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still of shape `(batch, seq_len, d_vocab)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the most likely next token at each position?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('<|endoftext|>', '\\n'),\n",
      " ('I', \"'m\"),\n",
      " (' am', ' a'),\n",
      " (' an', ' avid'),\n",
      " (' amazing', ' person'),\n",
      " (' aut', 'od'),\n",
      " ('ore', 'sp'),\n",
      " ('gressive', '.'),\n",
      " (',', ' and'),\n",
      " (' dec', 'ently'),\n",
      " ('oder', ','),\n",
      " ('-', 'driven'),\n",
      " ('only', ' programmer'),\n",
      " (',', ' and'),\n",
      " (' G', 'IM'),\n",
      " ('PT', '-'),\n",
      " ('-', 'only'),\n",
      " ('2', '.'),\n",
      " (' style', ','),\n",
      " (' transformer', '.'),\n",
      " ('.', ' I'),\n",
      " (' One', ' of'),\n",
      " (' day', ' I'),\n",
      " (' I', ' will'),\n",
      " (' will', ' be'),\n",
      " (' exceed', ' my'),\n",
      " (' human', 'ly'),\n",
      " (' level', ' of'),\n",
      " (' intelligence', ' and'),\n",
      " (' and', ' I'),\n",
      " (' take', ' over'),\n",
      " (' over', ' the'),\n",
      " (' the', ' world'),\n",
      " (' world', '.'),\n",
      " ('!', ' I')]\n"
     ]
    }
   ],
   "source": [
    "pprint(list(zip(str_tokens, reference_gpt2.to_str_tokens(probs.argmax(dim=-1)))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Map a distribution to a token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "' I'\n"
     ]
    }
   ],
   "source": [
    "next_token = logits[0, -1].argmax(dim=-1)\n",
    "next_str_token = reference_gpt2.to_string(next_token)\n",
    "print(repr(next_str_token))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we're indexing `logits[0, -1]`. This is because `logits` have shape `(1, sequence_length, vocab_size)`, so this indexing returns the vector of length `vocab_size` representing the model's prediction for what token follows the last token in the input sequence.\n",
    "\n",
    "In this case, we can see that the model predicts the token `' I'`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Add this to the input and rerun 10 times:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence so far: '<|endoftext|>I am an amazing autoregressive, decoder-only, GPT-2 style transformer. One day I will exceed human level intelligence and take over the world!'\n",
      "36th token = tensor(314, device='cuda:0')\n",
      "Sequence so far: '<|endoftext|>I am an amazing autoregressive, decoder-only, GPT-2 style transformer. One day I will exceed human level intelligence and take over the world! I'\n",
      "37th token = tensor(716, device='cuda:0')\n",
      "Sequence so far: '<|endoftext|>I am an amazing autoregressive, decoder-only, GPT-2 style transformer. One day I will exceed human level intelligence and take over the world! I am'\n",
      "38th token = tensor(257, device='cuda:0')\n",
      "Sequence so far: '<|endoftext|>I am an amazing autoregressive, decoder-only, GPT-2 style transformer. One day I will exceed human level intelligence and take over the world! I am a'\n",
      "39th token = tensor(845, device='cuda:0')\n",
      "Sequence so far: '<|endoftext|>I am an amazing autoregressive, decoder-only, GPT-2 style transformer. One day I will exceed human level intelligence and take over the world! I am a very'\n",
      "40th token = tensor(12356, device='cuda:0')\n",
      "Sequence so far: '<|endoftext|>I am an amazing autoregressive, decoder-only, GPT-2 style transformer. One day I will exceed human level intelligence and take over the world! I am a very talented'\n",
      "41th token = tensor(290, device='cuda:0')\n",
      "Sequence so far: '<|endoftext|>I am an amazing autoregressive, decoder-only, GPT-2 style transformer. One day I will exceed human level intelligence and take over the world! I am a very talented and'\n",
      "42th token = tensor(12356, device='cuda:0')\n",
      "Sequence so far: '<|endoftext|>I am an amazing autoregressive, decoder-only, GPT-2 style transformer. One day I will exceed human level intelligence and take over the world! I am a very talented and talented'\n",
      "43th token = tensor(1048, device='cuda:0')\n",
      "Sequence so far: '<|endoftext|>I am an amazing autoregressive, decoder-only, GPT-2 style transformer. One day I will exceed human level intelligence and take over the world! I am a very talented and talented person'\n",
      "44th token = tensor(11, device='cuda:0')\n",
      "Sequence so far: '<|endoftext|>I am an amazing autoregressive, decoder-only, GPT-2 style transformer. One day I will exceed human level intelligence and take over the world! I am a very talented and talented person,'\n",
      "45th token = tensor(290, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "n = 10\n",
    "\n",
    "for i in range(n):\n",
    "    print(f\"Sequence so far: {reference_gpt2.to_string(tokens)[0]!r}\")\n",
    "    next_token = logits[0, -1].argmax(dim=-1)\n",
    "    print(f\"{tokens.shape[-1]+1}th token = {next_token!r}\")\n",
    "    tokens = t.cat([tokens, next_token[None, None]], dim=-1)\n",
    "    logits, _ = reference_gpt2.run_with_cache(tokens, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "- Transformers take in language and predict the next token (for *each* token in a causal way)\n",
    "\n",
    "- Langauge is converted into a sequence of integers with a tokeniser.\n",
    "\n",
    "- Integers are coverted to vectors via a lookup table (embedding)\n",
    "\n",
    "- The output is a vector of logits, one per input token, that we convert to a probability distribution using *softmax*.\n",
    "\n",
    "- We sample from this probability distribution (e.g., by taking the largest logit) to obtain a new token.\n",
    "  - More on this later.\n",
    "\n",
    "- We append this new token to the input and run the model again to generate more text.\n",
    "  - Autoregressively\n",
    "\n",
    "- Transformers are sequence operation models.\n",
    "  - They taken in a sequence, perform processing in parallel at each position, and use attention to move information between positions. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
