{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import math\n",
    "import os\n",
    "from pprint import pprint\n",
    "\n",
    "import datasets\n",
    "from jaxtyping import Float, Int\n",
    "import torch as t\n",
    "from torch import Tensor\n",
    "from torch.utils.data import DataLoader\n",
    "from transformer_lens import HookedTransformer\n",
    "from transformer_lens.utils import tokenize_and_concatenate\n",
    "from tqdm.notebook import tqdm\n",
    "import wandb\n",
    "\n",
    "import C1P1_solutions as solutions\n",
    "from C1P1__mj_implementation import Config, DemoTransformer, get_log_probs\n",
    "\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = t.device(\n",
    "    \"mps\"\n",
    "    if t.backends.mps.is_available()\n",
    "    else \"cuda\"\n",
    "    if t.cuda.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load GPT-2 Small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\matth\\Workspace\\arena3\\.venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1602: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\matth\\Workspace\\arena3\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1160: UserWarning: expandable_segments not supported on this platform (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\c10/cuda/CUDAAllocatorConfig.h:28.)\n",
      "  return t.to(\n"
     ]
    }
   ],
   "source": [
    "reference_gpt2 = HookedTransformer.from_pretrained(\n",
    "    \"gpt2-small\",\n",
    "    fold_ln=False,\n",
    "    center_unembed=False,\n",
    "    center_writing_weights=False,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Objectives\n",
    "\n",
    "- Understand how to train a transformer from scratch.\n",
    "\n",
    "- Write a basic transformer training loop.\n",
    "\n",
    "- Interpret the transformer's failing cross entropy loss with reference to features of the training data\n",
    "  - E.g., bigram frequencies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model\n",
    "\n",
    "- We'll train a 2L, 4 heads per layer model, with context length 256, for 10*200 steps of batch size 16, just to show what it looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cfg = Config(\n",
    "    debug=False,\n",
    "    d_model=256,\n",
    "    n_heads=4,\n",
    "    d_head=64,\n",
    "    d_mlp=1024,\n",
    "    n_layers=2,\n",
    "    n_ctx=256,\n",
    "    d_vocab=reference_gpt2.cfg.d_vocab,\n",
    ")\n",
    "model = DemoTransformer(model_cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Arguments "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TransformerTrainingArgs:\n",
    "    batch_size = 16\n",
    "    epochs = 10\n",
    "    max_steps_per_epoch = 200\n",
    "    lr = 1e-3\n",
    "    weight_decay = 1e-2\n",
    "    wandb_project: str | None = \"day1-demotransformer\"\n",
    "    wandb_name: str | None = None\n",
    "\n",
    "\n",
    "args = TransformerTrainingArgs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load in a tiny dataset made by Neel Nanda, with the first 10K entries in the Pile (inspired by Stas' version for OpenWebText!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 10000\n",
      "})\n",
      "It is done, and submitted. You can play “Survival of the Tastiest” on Android, and on the web. Playi\n"
     ]
    }
   ],
   "source": [
    "dataset = datasets.load_dataset(\"NeelNanda/pile-10k\", split=\"train\").remove_columns(\n",
    "    \"meta\"\n",
    ")\n",
    "print(dataset)\n",
    "print(dataset[0][\"text\"][:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `tokenise_and_concatenate` is a useful function that takes our dataset of strings, and returns a dataset of token IPs ready to feed into the model.\n",
    "  - We then create a data loader from this tokenised dataset.\n",
    "\n",
    "- The useful method `train_test_split` can give us a training and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = tokenize_and_concatenate(\n",
    "    dataset,\n",
    "    reference_gpt2.tokenizer,\n",
    "    streaming=False,\n",
    "    max_length=model.cfg.n_ctx,\n",
    "    column_name=\"text\",\n",
    "    add_bos_token=True,\n",
    "    num_proc=4,\n",
    ")\n",
    "\n",
    "dataset_dict = tokenized_dataset.train_test_split(test_size=1000)\n",
    "train_loader = DataLoader(\n",
    "    dataset_dict[\"train\"],\n",
    "    batch_size=args.batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    dataset_dict[\"test\"],\n",
    "    batch_size=args.batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we iterate through these data loaders, we will find dictionaries with the single key `tokens`, which maps to a tensor of token IDs with shape `(batch, seq_len)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['tokens'])\n",
      "torch.Size([16, 256])\n"
     ]
    }
   ],
   "source": [
    "first_batch = train_loader.dataset[: args.batch_size]\n",
    "\n",
    "pprint(first_batch.keys())\n",
    "pprint(first_batch[\"tokens\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The key parts of the gradient update setup are:\n",
    "  - Calculating the (cross-entropy) loss between a model's output and the true labels,\n",
    "\n",
    "  - `loss.backward()` calculates gradients of the loss with respect to the model parameters,\n",
    "\n",
    "  - `optimizer.step()` updates the model parameters using the gradients,\n",
    "\n",
    "  - `optimizer.zero_grad()` zeros the gradients so they don't accumulate.\n",
    "\n",
    "- The training loops can be packaged up into a class that includes methods for training and validation steps among other things.\n",
    "\n",
    "- We can use dataclasses to store all arguments relevant to training in one place, and pass them to our trainer class.\n",
    "  - Watch out for scope! Want to refer to `self.args` within the trainer class, not the global `args`.\n",
    "\n",
    "- You can use Weights and Biases to track experiments and log relevant variables. The three essential functions are:\n",
    "  - `wandb.init()` initialises a new run, takes arguments `project`, `name`, and `config`, among others.\n",
    "\n",
    "  - `wandb.log()` logs a dictionary of variables. E.g., `{\"loss\": loss}`. Also takes a `step` argument.\n",
    "\n",
    "  - `wandb.finish()` is called at the end of training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: write training loop\n",
    "\n",
    "- Calculate cross entropy loss using `get_log_probs` from the previous section.\n",
    "\n",
    "- Use the optimiser `t.optim.AdamW` (Adam with weight decay), and with hyperparameters `lr` and `weight_decay` taken from your `TransformerTrainingArgs` dataclass instance.\n",
    "\n",
    "- Easy to calculate accuracy by having `validation_step` return a 1D boolean tensor indicating the positions where the model's prediction was correct.\n",
    "  - Can concatenate all of these tensor together and take the mean to get the overall accuracy for the epoch.\n",
    "\n",
    "- `max_steps_per_epoch` is provided as a hack to make sure the training phase in each epoch doesn't last too long.\n",
    "  - You can terminate the training phase after this many steps.\n",
    "\n",
    "- Remember to move tokens to your device via `tokens.to(device)`.\n",
    "\n",
    "- Feel free to refer to the [training loops from Chapter 0](https://arena-ch0-fundamentals.streamlit.app/%5B0.3%5D_ResNets#training-loop)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerTrainer:\n",
    "    def __init__(self, args: TransformerTrainingArgs, model: DemoTransformer):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.args = args\n",
    "\n",
    "        # AdamW is a variant of Adam that combines the advantages of Adam's adaptive learning rates\n",
    "        # with the benefits of direct weight decay (decoupled from the optimization steps).\n",
    "        # This leads to more effective regularisation and better generalisation.\n",
    "        self.optimizer = t.optim.AdamW(\n",
    "            self.model.parameters(), lr=args.lr, weight_decay=args.weight_decay\n",
    "        )\n",
    "\n",
    "        self.step = 0\n",
    "\n",
    "    def training_step(\n",
    "        self, batch: dict[str, Int[Tensor, \"batch seq\"]]\n",
    "    ) -> Float[Tensor, \"\"]:\n",
    "        \"\"\"\n",
    "        Calculates the loss on the tokens in the batch, performs a gradient update step, and logs the loss.\n",
    "\n",
    "        Remember that `batch` is a dictionary with the single key 'tokens'.\n",
    "        \"\"\"\n",
    "\n",
    "        tokens = batch[\"tokens\"].to(device)\n",
    "        logits = self.model(tokens)\n",
    "\n",
    "        # Calculate the loss. Mean to give a single scalar value indicating performance.abs\n",
    "        # Negative to make it a minimisation problem.\n",
    "        # Logarithm to simplify the calculation - sums instead of products.\n",
    "        loss = -get_log_probs(logits, tokens).mean()\n",
    "\n",
    "        # Computes the gradients of the loss with respect to all model parameters that have requires_grad=True.\n",
    "        # Updates the grad attributes of the model's parameters.\n",
    "        loss.backward()\n",
    "\n",
    "        # Adjust model parameters according to new gradients\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # Zero the gradients for the next iteration (PyTorch accumulates gradients by default)\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        self.step += 1\n",
    "\n",
    "        wandb.log({\"train_loss\": loss.item(), \"step\": self.step})\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch: dict[str, Int[Tensor, \"batch seq\"]]):\n",
    "        \"\"\"\n",
    "        Calculates & returns the accuracy on the tokens in the batch (i.e. how often the model's prediction\n",
    "        is correct). Logging should happen in the `train` function (after we've computed the accuracy for\n",
    "        the whole validation set).\n",
    "        \"\"\"\n",
    "        tokens = batch[\"tokens\"].to(device)\n",
    "\n",
    "        logits = self.model(tokens)[:, :-1]\n",
    "        predicted_tokens = logits.argmax(dim=-1)\n",
    "\n",
    "        return (predicted_tokens == tokens[:, 1:]).flatten()\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Trains the model, for `self.args.epochs` epochs. Also handles wandb initialisation, and early stopping\n",
    "        for each epoch at `self.args.max_steps_per_epoch` steps.\n",
    "        \"\"\"\n",
    "        wandb.init(\n",
    "            project=self.args.wandb_project, name=self.args.wandb_name, config=self.args\n",
    "        )\n",
    "\n",
    "        accuracy = float(\"nan\")\n",
    "\n",
    "        progress_bar = tqdm(total=self.args.max_steps_per_epoch * self.args.epochs)\n",
    "\n",
    "        for epoch in range(self.args.epochs):\n",
    "            for i, batch in enumerate(self.train_loader()):\n",
    "                loss = self.training_step(batch)\n",
    "                progress_bar.update()\n",
    "                progress_bar.set_description(\n",
    "                    f\"Epoch {epoch+1}, loss: {loss:.3f}, accuracy: {accuracy:.2f}\"\n",
    "                )\n",
    "                if i >= self.args.max_steps_per_epoch:\n",
    "                    break\n",
    "\n",
    "            correct_predictions = t.concat(\n",
    "                [self.validation_step(batch) for batch in self.test_loader()]\n",
    "            )\n",
    "            accuracy = correct_predictions.float().mean().item()\n",
    "            wandb.log({\"accuracy\": accuracy}, step=self.step)\n",
    "\n",
    "        wandb.finish()\n",
    "\n",
    "    def train_loader(self) -> DataLoader:\n",
    "        \"\"\"Returns train loader (as in code above).\"\"\"\n",
    "        return DataLoader(\n",
    "            dataset_dict[\"train\"],\n",
    "            batch_size=self.args.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=4,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "\n",
    "    def test_loader(self) -> DataLoader:\n",
    "        \"\"\"Returns test loader (as in code above).\"\"\"\n",
    "        return DataLoader(\n",
    "            dataset_dict[\"test\"],\n",
    "            batch_size=self.args.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=4,\n",
    "            pin_memory=True,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DemoTransformer(model_cfg).to(device)\n",
    "args = TransformerTrainingArgs()\n",
    "trainer = TransformerTrainer(args, model)\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations\n",
    "\n",
    "- Comparing our run ([WandB link](https://wandb.ai/matthew-jennings/day1-demotransformer/runs/bqo9iwnz?nw=nwusermatthewjennings)) with one from the solutions ([WandB link](https://wandb.ai/callum-mcdougall/day1-demotransformer/reports/wandb-training-run--Vmlldzo2NTEyMzg4?accessToken=1xmja86i6ugeii40hvlefe1sp90v7cn9p4rrkhxobryjviuxfsydak4fx3gqx18q)) confirms similar performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The loss curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The [loss curve](https://wandb.ai/matthew-jennings/day1-demotransformer/reports/train_loss-24-09-15-11-46-38---Vmlldzo5MzcxMDAx) seems to start at a value around 10-11, decreases quickly and then levels out.\n",
    "\n",
    "- This is related to the kinds of algorithms that the model learns during training.\n",
    "\n",
    "- At the beginning, the model outputs random noise; something like \"predict each token with approximately uniform probability\". I.e., $Q(x) = 1 / d_{vocab}$ for all $x$. This gives us a cross-entropy loss equal to $\\log(d_{vocab})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d_vocab = 50257\n",
      "Cross entropy loss on uniform distribution = 10.825\n"
     ]
    }
   ],
   "source": [
    "d_vocab = model.cfg.d_vocab\n",
    "\n",
    "print(f\"d_vocab = {d_vocab}\")\n",
    "print(f\"Cross entropy loss on uniform distribution = {math.log(d_vocab):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next thing we might expect the model to learn is the frequencies of words in the English language (i.e., **unigram frequencies**). Small common tokens like `\" and\"` or `\" the\"` might appear much more frequently than others. This was give an average cross-entropy loss of:\n",
    "\n",
    "$$\n",
    "-\\sum_x{p_x \\log{p_x}}\n",
    "$$\n",
    "\n",
    "where $p_x$ is the actual frequency of the word/token in our training data.\n",
    "\n",
    "We can evaluate this as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy of the token distribution = 7.349\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenized_dataset[:][\"tokens\"].flatten()\n",
    "\n",
    "freqs = t.bincount(tokens, minlength=model.cfg.d_vocab).float()\n",
    "\n",
    "probs = freqs / freqs.sum()\n",
    "\n",
    "distn = t.distributions.Categorical(probs=probs)\n",
    "entropy = distn.entropy().item()\n",
    "\n",
    "print(f\"Entropy of the token distribution = {entropy:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note the lower value of 7.349**\n",
    "\n",
    "After unigram frequencies, the next thing our model usually learns is **bigram frequencies**; the frequencies of pairs of adjacent tokens in the training data. E.g., `\"I\"` and `\" am\"` are common tokens, but their bigram frequency is much higher than it would be if they occurred independently.\n",
    "\n",
    "Bigram frequencies actually take you pretty far, since they help with:\n",
    "- Some simple grammatical rules\n",
    "  - E.g., a full stop being followed by a capitalised word.\n",
    "\n",
    "- Weird quirks of tokenisation\n",
    "  - E.g., ` \"manip\"` being followed by `\"ulative\"`\n",
    "\n",
    "- Common names\n",
    "  - E.g., `\"Barack\"` being followed by `\" Obama\"`\n",
    "\n",
    "##### After approxmating bigram frequencies, we need smarter techniques, like:\n",
    "  - Trigrams, which require attention heads\n",
    "  \n",
    "  - Induction heads\n",
    "\n",
    "  - Fact memorisation\n",
    "\n",
    "  - Other grammar/syntax rules.\n",
    "\n",
    "Marginal improvement gets much harder, which flattens the loss curve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: log completions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Choose a handle of prompts, and log the model's completions on those sentences.\n",
    "\n",
    "- Log at a lower frequency than loss (e.g., once every 10-100 batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "John and Mary went to the Stewart Chrysar seals alien phenomenon unlockingNOTE intercourseiegemoderateCTographwu hunter75\n"
     ]
    }
   ],
   "source": [
    "def sampling_fn(model: DemoTransformer, prompt: str) -> str:\n",
    "    sampler = solutions.TransformerSampler(model, reference_gpt2.tokenizer)\n",
    "    output = sampler.sample(\n",
    "        prompt, temperature=0.7, top_p=0.95, max_tokens_generated=16\n",
    "    )\n",
    "    return output\n",
    "\n",
    "\n",
    "model = DemoTransformer(model_cfg).to(device)\n",
    "\n",
    "# Should be entirely random, because it uses a newly initialized model\n",
    "print(sampling_fn(model, prompt=\"John and Mary went to the\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <bound method _WandbInit._resume_backend of <wandb.sdk.wandb_init._WandbInit object at 0x00000202D2462F90>> (for pre_run_cell), with arguments args (<ExecutionInfo object at 202decd3c90, raw_cell=\"@dataclass\n",
      "class TransformerTrainingArgsLogText(Tr..\" store_history=True silent=False shell_futures=True cell_id=vscode-notebook-cell:/c%3A/Users/matth/Workspace/arena3/mj/C1P1_transformer_from_scratch__S3_training.ipynb#X50sZmlsZQ%3D%3D>,),kwargs {}:\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "_WandbInit._resume_backend() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;31mTypeError\u001b[0m: _WandbInit._resume_backend() takes 1 positional argument but 2 were given"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <bound method _WandbInit._pause_backend of <wandb.sdk.wandb_init._WandbInit object at 0x00000202D2462F90>> (for post_run_cell), with arguments args (<ExecutionResult object at 202dea1e250, execution_count=22 error_before_exec=None error_in_exec=None info=<ExecutionInfo object at 202decd3c90, raw_cell=\"@dataclass\n",
      "class TransformerTrainingArgsLogText(Tr..\" store_history=True silent=False shell_futures=True cell_id=vscode-notebook-cell:/c%3A/Users/matth/Workspace/arena3/mj/C1P1_transformer_from_scratch__S3_training.ipynb#X50sZmlsZQ%3D%3D> result=None>,),kwargs {}:\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "_WandbInit._pause_backend() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;31mTypeError\u001b[0m: _WandbInit._pause_backend() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class TransformerTrainingArgsLogText(TransformerTrainingArgs):\n",
    "    text_sample_freq: int = 20\n",
    "    table_log_freq: int = 200\n",
    "\n",
    "\n",
    "def train_with_text_logs(self, sampling_fn, prompts):\n",
    "    \"\"\"\n",
    "    Trains the model, for `self.args.epochs` epochs. Also handles wandb initialisation, and early stopping\n",
    "    for each epoch at `self.args.max_steps_per_epoch` steps.\n",
    "\n",
    "    This also takes 2 extra arguments:\n",
    "        sampling_fn: function which takes model & a single prompt (i.e. text string) and returns text string output\n",
    "        prompt_list: list of prompts we'll log output on\n",
    "    \"\"\"\n",
    "    wandb.init(\n",
    "        project=self.args.wandb_project, name=self.args.wandb_name, config=self.args\n",
    "    )\n",
    "\n",
    "    accuracy = float(\"nan\")\n",
    "\n",
    "    progress_bar = tqdm(total=self.args.max_steps_per_epoch * self.args.epochs)\n",
    "\n",
    "    completions = []\n",
    "\n",
    "    for epoch in range(1, self.args.epochs + 1):\n",
    "        for i, batch in enumerate(self.train_loader()):\n",
    "            loss = self.training_step(batch)\n",
    "            progress_bar.update()\n",
    "            progress_bar.set_description(\n",
    "                f\"Epoch {epoch}, loss: {loss:.3f}, accuracy: {accuracy:.2f}\"\n",
    "            )\n",
    "\n",
    "            if self.step % self.args.text_sample_freq == 0:\n",
    "                completions_this_step = [\n",
    "                    sampling_fn(self.model, prompt) for prompt in prompts\n",
    "                ]\n",
    "                completions.append([epoch, self.step, *completions_this_step])\n",
    "\n",
    "            if self.step % self.args.table_log_freq == 0:\n",
    "                wandb.log(\n",
    "                    {\n",
    "                        \"completions_table\": wandb.Table(\n",
    "                            data=completions,\n",
    "                            columns=[\n",
    "                                \"Epoch\",\n",
    "                                \"Step\",\n",
    "                                *[f\"Prompt_{i}\" for i in range(1, len(prompts) + 1)],\n",
    "                            ],\n",
    "                        )\n",
    "                    }\n",
    "                )\n",
    "\n",
    "            if i >= self.args.max_steps_per_epoch:\n",
    "                break\n",
    "\n",
    "        correct_predictions = t.concat(\n",
    "            [self.validation_step(batch) for batch in self.test_loader()]\n",
    "        )\n",
    "        accuracy = correct_predictions.float().mean().item()\n",
    "        wandb.log({\"accuracy\": accuracy}, step=self.step)\n",
    "\n",
    "    wandb.finish()\n",
    "\n",
    "\n",
    "TransformerTrainer.train_with_text_logs = train_with_text_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <bound method _WandbInit._resume_backend of <wandb.sdk.wandb_init._WandbInit object at 0x00000202D2462F90>> (for pre_run_cell), with arguments args (<ExecutionInfo object at 202decd5450, raw_cell=\"prompts = [\n",
      "    \"John and Mary went to the\",\n",
      "    \"..\" store_history=True silent=False shell_futures=True cell_id=vscode-notebook-cell:/c%3A/Users/matth/Workspace/arena3/mj/C1P1_transformer_from_scratch__S3_training.ipynb#X51sZmlsZQ%3D%3D>,),kwargs {}:\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "_WandbInit._resume_backend() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;31mTypeError\u001b[0m: _WandbInit._resume_backend() takes 1 positional argument but 2 were given"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:bww5d6dx) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49eb73c37569416da60a5bf64a2c9b97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.009 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.137212…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train_loss</td><td>█▆▄▄▂▄▄▂▄▃▄▃▃▂▃▂▃▁▂▃▂▃▃▂▃▃▂▃▂▃▂▁▁▁▂▃▂▁▁▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>step</td><td>200</td></tr><tr><td>train_loss</td><td>6.59604</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">super-donkey-10</strong> at: <a href='https://wandb.ai/matthew-jennings/day1-demotransformer/runs/bww5d6dx' target=\"_blank\">https://wandb.ai/matthew-jennings/day1-demotransformer/runs/bww5d6dx</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240915_151115-bww5d6dx\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:bww5d6dx). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fdd4191408143a0a18dc8d5e78f7e14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.18.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\matth\\Workspace\\arena3\\mj\\wandb\\run-20240916_085217-we03ivb1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/matthew-jennings/day1-demotransformer/runs/we03ivb1' target=\"_blank\">fanciful-tree-11</a></strong> to <a href='https://wandb.ai/matthew-jennings/day1-demotransformer' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/matthew-jennings/day1-demotransformer' target=\"_blank\">https://wandb.ai/matthew-jennings/day1-demotransformer</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/matthew-jennings/day1-demotransformer/runs/we03ivb1' target=\"_blank\">https://wandb.ai/matthew-jennings/day1-demotransformer/runs/we03ivb1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "524070237c4c444aaf74a1d0b5ed1196",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de13ec62328c4952b939fe59bcd9bf26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.305 MB of 0.311 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.978864…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train_loss</td><td>█▆▅▆▄▅▄▄▃▅▄▅▂▃▂▄▃▁▂▄▄▃▃▃▃▂▃▂▃▂▂▁▂▃▂▂▃▂▃▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>step</td><td>2010</td></tr><tr><td>train_loss</td><td>5.30513</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">fanciful-tree-11</strong> at: <a href='https://wandb.ai/matthew-jennings/day1-demotransformer/runs/we03ivb1' target=\"_blank\">https://wandb.ai/matthew-jennings/day1-demotransformer/runs/we03ivb1</a><br/>Synced 4 W&B file(s), 10 media file(s), 10 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240916_085217-we03ivb1\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompts = [\n",
    "    \"John and Mary went to the\",\n",
    "    \"The cat sat on the mat\",\n",
    "    \"The quick brown fox jumped over the\",\n",
    "]\n",
    "\n",
    "model = DemoTransformer(model_cfg).to(device)\n",
    "\n",
    "args = TransformerTrainingArgsLogText()\n",
    "trainer = TransformerTrainer(args, model)\n",
    "trainer.train_with_text_logs(sampling_fn, prompts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
