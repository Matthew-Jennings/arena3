{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import os\n",
    "\n",
    "from dataclasses import dataclass\n",
    "import einops\n",
    "from jaxtyping import Float, Int\n",
    "from rich.table import Table\n",
    "from rich import print as rprint\n",
    "import torch as t\n",
    "from torch import Tensor\n",
    "from torch.distributions.categorical import Categorical\n",
    "from transformer_lens import HookedTransformer\n",
    "from transformers.models.gpt2.tokenization_gpt2_fast import GPT2TokenizerFast\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from C1P1__mj_implementation import Config, DemoTransformer\n",
    "\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = t.device(\n",
    "    \"mps\"\n",
    "    if t.backends.mps.is_available()\n",
    "    else \"cuda\"\n",
    "    if t.cuda.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load GPT-2 Small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\matth\\Workspace\\arena3\\.venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1602: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\matth\\Workspace\\arena3\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1160: UserWarning: expandable_segments not supported on this platform (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\c10/cuda/CUDAAllocatorConfig.h:28.)\n",
      "  return t.to(\n"
     ]
    }
   ],
   "source": [
    "reference_gpt2 = HookedTransformer.from_pretrained(\n",
    "    \"gpt2-small\",\n",
    "    fold_ln=False,\n",
    "    center_unembed=False,\n",
    "    center_writing_weights=False,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling from a Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Objectives\n",
    "\n",
    "- Learn how to sample from a transformer.\n",
    "  - Includes basic methods like **greedy search** or **top-k**, and more advanced methods like **beam search**.\n",
    "\n",
    "- Learn how to cache the output from a transformer, so that it can be used to generate text more efficiently.\n",
    "  - Rewrite sampling functions to use caching."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obvious way to sample tokens: always take the token assigned the highest probability!\n",
    "\n",
    "- This can lead to boring and repetitive outcomes.\n",
    "- At worst, it can lock our transformer into a loop.\n",
    "\n",
    "##### Read the HuggingFace blog post: [\"How to generate text: using different decoding methods for language generation with Transformers\"](https://huggingface.co/blog/how-to-generate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TransformerSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cfg = Config()\n",
    "model = DemoTransformer(model_cfg).to(device)\n",
    "model.load_state_dict(reference_gpt2.state_dict(), strict=False)\n",
    "\n",
    "tokenizer = reference_gpt2.tokenizer\n",
    "\n",
    "\n",
    "class TransformerSampler:\n",
    "    def __init__(self, model: DemoTransformer, tokenizer: GPT2TokenizerFast):\n",
    "        self.model = model\n",
    "        self.cfg = model.cfg\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    @t.inference_mode()\n",
    "    def sample(self, prompt: str, max_tokens_generated=100, verbose=False, **kwargs):\n",
    "        \"\"\"\n",
    "        Returns a string of autoregressively generated text, starting from the prompt.\n",
    "\n",
    "        Sampling terminates at max_tokens_generated, or when the model generates an\n",
    "        end-of-sequence token.\n",
    "\n",
    "        kwargs are passed to sample_next_token, to give detailed instructions on how\n",
    "        new tokens are chosen.\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE!\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    @t.inference_mode()\n",
    "    def beam_search(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        num_return_sequences: int,\n",
    "        num_beams: int,\n",
    "        max_new_tokens: int,\n",
    "        no_repeat_ngram_size: int = 0,\n",
    "        verbose=False,\n",
    "    ) -> list[tuple[float, Tensor]]:\n",
    "        \"\"\"\n",
    "        Returns a string of autoregressively generated text, starting from the prompt.\n",
    "\n",
    "        Sampling terminates at max_tokens_generated, or when the model generates an\n",
    "        end-of-sequence token.\n",
    "\n",
    "        kwargs are passed to sample_next_token, to give detailed instructions on how\n",
    "        new tokens are chosen.\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE!\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    @staticmethod\n",
    "    def sample_next_token(\n",
    "        input_ids: Int[Tensor, \"seq_len\"],\n",
    "        logits: Float[Tensor, \"d_vocab\"],\n",
    "        temperature=1.0,\n",
    "        top_k=0,\n",
    "        top_p=0.0,\n",
    "        frequency_penalty=0.0,\n",
    "        seed=None,\n",
    "    ):\n",
    "        assert input_ids.ndim == 1, \"input_ids should be a 1D sequence of token ids\"\n",
    "        assert temperature >= 0, \"Temperature should be non-negative\"\n",
    "        assert 0 <= top_p <= 1.0, \"Top-p must be a probability\"\n",
    "        assert 0 <= top_k, \"Top-k must be non-negative\"\n",
    "        assert not (\n",
    "            top_p != 0 and top_k != 0\n",
    "        ), \"At most one of top-p and top-k supported\"\n",
    "\n",
    "        # Set random seeds for reproducibility\n",
    "        if seed is not None:\n",
    "            t.manual_seed(seed)\n",
    "            np.random.seed(seed)\n",
    "\n",
    "        # Apply all the specialized sampling methods\n",
    "        if temperature == 0:\n",
    "            return TransformerSampler.greedy_search(logits)\n",
    "        elif temperature != 1.0:\n",
    "            logits = TransformerSampler.apply_temperature(logits, temperature)\n",
    "        if frequency_penalty != 0.0:\n",
    "            logits = TransformerSampler.apply_frequency_penalty(\n",
    "                input_ids, logits, frequency_penalty\n",
    "            )\n",
    "        if top_k > 0:\n",
    "            return TransformerSampler.sample_top_k(logits, top_k)\n",
    "        if top_p > 0.0:\n",
    "            return TransformerSampler.sample_top_p(logits, top_p)\n",
    "        return TransformerSampler.sample_basic(logits)\n",
    "\n",
    "    @staticmethod\n",
    "    def greedy_search(logits: Float[Tensor, \"d_vocab\"]) -> int:\n",
    "        \"\"\"\n",
    "        Returns the most likely token (as an int).\n",
    "        \"\"\"\n",
    "        out = logits.argmax().item()\n",
    "        return out\n",
    "\n",
    "    @staticmethod\n",
    "    def apply_temperature(\n",
    "        logits: Float[Tensor, \"d_vocab\"], temperature: float\n",
    "    ) -> Float[Tensor, \"d_vocab\"]:\n",
    "        \"\"\"\n",
    "        Applies temperature scaling to the logits.\n",
    "        \"\"\"\n",
    "        return logits / temperature\n",
    "\n",
    "    @staticmethod\n",
    "    def apply_frequency_penalty(\n",
    "        input_ids: Int[Tensor, \"seq_len\"],\n",
    "        logits: Float[Tensor, \"d_vocab\"],\n",
    "        freq_penalty: float,\n",
    "    ) -> Float[Tensor, \"d_vocab\"]:\n",
    "        \"\"\"\n",
    "        Applies a frequency penalty to the logits.\n",
    "        \"\"\"\n",
    "        counts = t.bincount(input_ids, minlength=logits.shape[-1])\n",
    "\n",
    "        return logits - (freq_penalty * counts)\n",
    "\n",
    "    @staticmethod\n",
    "    def sample_basic(logits: Float[Tensor, \"d_vocab\"]) -> int:\n",
    "        \"\"\"\n",
    "        Samples from the distribution defined by the logits.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @staticmethod\n",
    "    def sample_top_k(logits: Float[Tensor, \"d_vocab\"], k: int) -> int:\n",
    "        \"\"\"\n",
    "        Samples from the top k most likely tokens.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @staticmethod\n",
    "    def sample_top_p(\n",
    "        logits: Float[Tensor, \"d_vocab\"], top_p: float, min_tokens_to_keep: int = 1\n",
    "    ) -> int:\n",
    "        \"\"\"\n",
    "        Samples from the most likely tokens which make up at least p cumulative probability.\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Sampling Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: implement `sample()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`sample()` takes in a prompt (string), encodes it as a sequence of token ids using `self.tokenizer.encode`, and then continually generates new tokens by repeating the following steps:\n",
    "\n",
    "1. Pass the tokenized prompt through the model to get logits\n",
    "\n",
    "2. Take the logit vector corresponding to the last token in the prompt (i.e., prediction for the *next* token)\n",
    "\n",
    "3. Sample from this distribution to get a new token, using `self.sample_next_token(input_ids, logits, **kwargs)`. `kwargs` contains all the sampling-specific args, e.g., **temperature**, **top-k**, etc.\n",
    "\n",
    "4. Append this new token to the input tokens, and repeat until we meet one of two termination criteria:\n",
    "   - We generate `max_tokens_generated` new tokens, or\n",
    "\n",
    "   - We generate the EOS token, accessed via `self.tokenizer.eos_token_id.\n",
    "\n",
    "Finally, we use `self.tokenizer.decode` to convert the generated token ids back into a string, and return the string.\n",
    "\n",
    "We also have a `verbose` arg - use to print output while it's being sampled.\n",
    "\n",
    "A few hints:\n",
    "- Don't forget about tensor shapes! The model's input should always have a `batch` dimension.\n",
    "\n",
    "- `sample_next_token()` will return an integer. Wrap this into a tensor before concatenating it to the end of the input IDs\n",
    "\n",
    "- Remember device!\n",
    "\n",
    "- Put the model in evaluation mode using `model.eval()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@t.inference_mode()\n",
    "def sample(self, prompt: str, max_tokens_generated=100, verbose=False, **kwargs):\n",
    "    \"\"\"\n",
    "    Returns a string of autoregressively generated text, starting from the prompt.\n",
    "\n",
    "    Sampling terminates at max_tokens_generated, or when the model generates an\n",
    "    end-of-sequence token.\n",
    "\n",
    "    kwargs are passed to sample_next_token, to give detailed instructions on how\n",
    "    new tokens are chosen.\n",
    "    \"\"\"\n",
    "    self.model.eval()\n",
    "\n",
    "    # Default return type is list. `return_tensors=\"pt\"`` returns a 2D PyTorch tensor.\n",
    "    tokens = self.tokenizer.encode(prompt, return_tensors=\"pt\").to(device)[0]\n",
    "\n",
    "    for _ in range(max_tokens_generated):\n",
    "        # Need batch dim, and only the last n_ctx tokens - don't want to exceed context length (better to warn?).\n",
    "        logits_last_token = self.model(tokens[None, -self.cfg.n_ctx :])[0, -1]\n",
    "\n",
    "        # Note that sample_next_token will return a scalar tensor - need to wrap in a list to give it a dim.\n",
    "        token_next = t.tensor(\n",
    "            # Note use of TransformerSampler rather than `self` - this is a static method.\n",
    "            [TransformerSampler.sample_next_token(tokens, logits_last_token, **kwargs)],\n",
    "            device=device,\n",
    "        )\n",
    "        tokens = t.cat((tokens, token_next), dim=-1)\n",
    "\n",
    "        if tokens[-1] == self.tokenizer.eos_token_id:\n",
    "            break\n",
    "\n",
    "    res = self.tokenizer.decode(tokens)\n",
    "\n",
    "    if verbose:\n",
    "        print(res)\n",
    "\n",
    "    return res\n",
    "\n",
    "\n",
    "TransformerSampler.sample = sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greedy decoding with prompt: 'Jingle bells, jingle bells, jingle all the way'\n",
      "\n",
      "Jingle bells, jingle bells, jingle all the way up to the top of the mountain.\n",
      "Your model said: 'Jingle bells, jingle bells, jingle all the way up to the top of the mountain.'\n",
      "\n",
      "Tests passed!\n"
     ]
    }
   ],
   "source": [
    "sampler = TransformerSampler(model, tokenizer)\n",
    "\n",
    "prompt = \"Jingle bells, jingle bells, jingle all the way\"\n",
    "print(f\"Greedy decoding with prompt: {prompt!r}\\n\")\n",
    "\n",
    "output = sampler.sample(prompt, max_tokens_generated=8, temperature=0.0, verbose=True)\n",
    "print(f\"Your model said: {output!r}\\n\")\n",
    "\n",
    "expected = (\n",
    "    \"Jingle bells, jingle bells, jingle all the way up to the top of the mountain.\"\n",
    ")\n",
    "assert output == expected\n",
    "\n",
    "print(\"Tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling with Categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch provides a `distributions` package containing convenient methods for sampling from various distributions\n",
    "\n",
    "For now, use `t.distributions.categorical.Categorical` to implement `sample_basic`. This just samples from the provided logits, which may have already been modified by the temperature and frequency penalties.\n",
    "\n",
    "Will be slow since we're not batching the samples (yet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Basic Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@staticmethod\n",
    "def sample_basic(logits: Float[Tensor, \"d_vocab\"]) -> int:\n",
    "    \"\"\"\n",
    "    Samples from the distribution defined by the logits.\n",
    "    \"\"\"\n",
    "    distn = Categorical(logits=logits)\n",
    "    return distn.sample().item()\n",
    "\n",
    "\n",
    "TransformerSampler.sample_basic = sample_basic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bde8943cef4a4151beaadaa07e708f95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: ' church'. Expected freq 0.0648, observed freq 0.0629\n",
      "Word: ' house' . Expected freq 0.0367, observed freq 0.0361\n",
      "Word: ' temple'. Expected freq 0.0145, observed freq 0.0137\n",
      "Word: ' same'  . Expected freq 0.0104, observed freq 0.0105\n",
      "Word: ' Church'. Expected freq 0.0097, observed freq 0.0106\n",
      "Tests passed!\n"
     ]
    }
   ],
   "source": [
    "prompt = \"John and Mary went to the\"\n",
    "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "logits = model(input_ids)[0, -1]\n",
    "\n",
    "expected_top_5 = {\n",
    "    \" church\": 0.0648,\n",
    "    \" house\": 0.0367,\n",
    "    \" temple\": 0.0145,\n",
    "    \" same\": 0.0104,\n",
    "    \" Church\": 0.0097,\n",
    "}\n",
    "frequency_of_top_5 = defaultdict(int)\n",
    "\n",
    "N = 10_000\n",
    "for _ in tqdm(range(N)):\n",
    "    token = TransformerSampler.sample_next_token(input_ids.squeeze(), logits)\n",
    "    frequency_of_top_5[tokenizer.decode(token)] += 1\n",
    "\n",
    "for word in expected_top_5:\n",
    "    expected_freq = expected_top_5[word]\n",
    "    observed_freq = frequency_of_top_5[word] / N\n",
    "    print(\n",
    "        f\"Word: {word!r:<9}. Expected freq {expected_freq:.4f}, observed freq {observed_freq:.4f}\"\n",
    "    )\n",
    "    assert (\n",
    "        abs(observed_freq - expected_freq) < 0.01\n",
    "    ), \"Try increasing N if this fails by a small amount.\"\n",
    "\n",
    "print(\"Tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Temperature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***** Implemented above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notes on temperature\n",
    "\n",
    "To apply a temperature to our sampling means to scale all logits by `1/temperature`. The basic intuition is:\n",
    "\n",
    "- A higher temperature means a smaller scale factor, which diminishes the differences between logits. This leads to a more uniform distribution and more random sampling.\n",
    "\n",
    "- A lower temperature means a larger scale factor, which amplifies the differences between logits. This makes the highest logit dominate the softmax distribution, resulting in greedy sampling.\n",
    "\n",
    "##### Derivation\n",
    "\n",
    "`sample_basic()` samples from a `Categorical` distribution of logits. These logits are unnormalised log probabilities, and are converted to probabilities via softmax. I.e., given logits $x_i$, the probabilities $P(i)$ are given by:\n",
    "\n",
    "$$ \n",
    "P(i) = \\frac{e^{x_i / T}}{\\sum_j{e^{x_j / T}}}\n",
    "$$\n",
    "\n",
    "where $T$ is the temperature parameter.\n",
    "\n",
    "Let $x_k = \\max\\limits_{i}(x_i)$ be the maximum logit among all tokens. For any other token $i \\ne k$, the ratio of probabilities is:\n",
    "\n",
    "$$\n",
    "\\frac{P(i)}{P(k)} = \\frac{e^{x_i / T}}{e^{x_k / T}} = e^{(x_i-x_k)/T}\n",
    "$$\n",
    "\n",
    "Now, $x_i - x_k \\le 0$ since $x_k$ is the maximum. As $T$ approaches $0$:\n",
    "\n",
    "- If $x_i - x_k \\lt 0$\n",
    "\n",
    "$$\n",
    "\\lim\\limits_{T \\rightarrow 0^+}{\\frac{x_i-x_k}{T} = -\\infty} \\\\[10pt]\n",
    "\\therefore\\lim\\limits_{T \\rightarrow 0^+}{\\frac{P(i)}{P(k)}} = \\lim\\limits_{T \\rightarrow 0^+}{e^{(x_i-x_k)/T} = 0}\n",
    "$$\n",
    "\n",
    "- If $x_i - x_k = 0$\n",
    "\n",
    "$$\n",
    "x_i = x_k\\\\[10pt]\n",
    "\\therefore \\frac{P(i)}{P(k)} = 1\n",
    "$$\n",
    "\n",
    "Thus, as temperature $T$ approaches zero:\n",
    "- $P(k)$ approaches 1.\n",
    "\n",
    "- $P(i)$ for all $i \\ne k$ approaches 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A low temperature \"sharpens\" or \"peaks\" the distribution:  tensor([  0.0000, 693.1472])\n",
      "A high temperature flattens the distribution:  tensor([0.0000, 0.0007])\n",
      "Tests passed!\n"
     ]
    }
   ],
   "source": [
    "logits = t.tensor([1, 2]).log()\n",
    "\n",
    "cold_logits = TransformerSampler.apply_temperature(logits, temperature=0.001)\n",
    "print('A low temperature \"sharpens\" or \"peaks\" the distribution: ', cold_logits)\n",
    "t.testing.assert_close(cold_logits, 1000.0 * logits)\n",
    "\n",
    "hot_logits = TransformerSampler.apply_temperature(logits, temperature=1000.0)\n",
    "print(\"A high temperature flattens the distribution: \", hot_logits)\n",
    "t.testing.assert_close(hot_logits, 0.001 * logits)\n",
    "\n",
    "print(\"Tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Frequency Penalty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***** Implemented above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests passed!\n"
     ]
    }
   ],
   "source": [
    "bieber_prompt = \"And I was like Baby, baby, baby, oh Like, Baby, baby, baby, no Like, Baby, baby, baby, oh I thought you'd always be mine, mine\"\n",
    "input_ids = tokenizer.encode(bieber_prompt, return_tensors=\"pt\")\n",
    "logits = t.ones(tokenizer.vocab_size)\n",
    "penalized_logits = TransformerSampler.apply_frequency_penalty(\n",
    "    input_ids.squeeze(), logits, 2.0\n",
    ")\n",
    "\n",
    "assert (\n",
    "    penalized_logits[5156].item() == -11\n",
    "), \"Expected 6 occurrences of ' baby' with leading space, 1-2*6=-11\"\n",
    "assert (\n",
    "    penalized_logits[14801].item() == -5\n",
    "), \"Expected 3 occurrences of ' Baby' with leading space, 1-2*3=-5\"\n",
    "\n",
    "print(\"Tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling - Manual Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                                             Sampling - Manual Testing                                             </span>\n",
       "┏━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Name                  </span>┃<span style=\"font-weight: bold\"> Kwargs                       </span>┃<span style=\"font-weight: bold\"> Output                                                   </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ High freq penalty     │ {'frequency_penalty': 100.0} │ \"Jingle bells, jingle bells, jingle all the way back to  │\n",
       "│                       │                              │ Akron.\\nHow many YouCaring women are as a gas station    │\n",
       "│                       │                              │ attendant? 3 of LBBC's 5\"                                │\n",
       "│                       │                              │                                                          │\n",
       "│ Negative freq penalty │ {'frequency_penalty': -3.0}  │ 'Jingle bells, jingle bells, jingle all the way up,      │\n",
       "│                       │                              │ jingle jingle jingle jingle jingle jingle jingle jingle  │\n",
       "│                       │                              │ jingle jingle jingle'                                    │\n",
       "│                       │                              │                                                          │\n",
       "│ Too hot!              │ {'temperature': 2.0}         │ 'Jingle bells, jingle bells, jingle all the way prowess  │\n",
       "│                       │                              │ flats since Belimage story drew full reliance TW Whilst  │\n",
       "│                       │                              │ Pryposal Tam required Cole ut those solitary Gravity     │\n",
       "│                       │                              │ Krarms earned'                                           │\n",
       "│                       │                              │                                                          │\n",
       "│ Pleasantly cool       │ {'temperature': 0.7}         │ \"Jingle bells, jingle bells, jingle all the way to the   │\n",
       "│                       │                              │ top of the mountain. I'm sure to come back here. You're  │\n",
       "│                       │                              │ not going to hurt someone. I\"                            │\n",
       "│                       │                              │                                                          │\n",
       "│ Pleasantly warm       │ {'temperature': 0.9}         │ 'Jingle bells, jingle bells, jingle all the way. \"Well   │\n",
       "│                       │                              │ that\\'s one last exercise! Wish you could sing for me!\"  │\n",
       "│                       │                              │ Rodger says. He doesn\\'t say'                            │\n",
       "│                       │                              │                                                          │\n",
       "│ Too cold!             │ {'temperature': 0.01}        │ 'Jingle bells, jingle bells, jingle all the way up to    │\n",
       "│                       │                              │ the top of the mountain.\\n\\nThe first time I saw the     │\n",
       "│                       │                              │ mountain, I was in the middle of'                        │\n",
       "│                       │                              │                                                          │\n",
       "└───────────────────────┴──────────────────────────────┴──────────────────────────────────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                                             Sampling - Manual Testing                                             \u001b[0m\n",
       "┏━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mName                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mKwargs                      \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput                                                  \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ High freq penalty     │ {'frequency_penalty': 100.0} │ \"Jingle bells, jingle bells, jingle all the way back to  │\n",
       "│                       │                              │ Akron.\\nHow many YouCaring women are as a gas station    │\n",
       "│                       │                              │ attendant? 3 of LBBC's 5\"                                │\n",
       "│                       │                              │                                                          │\n",
       "│ Negative freq penalty │ {'frequency_penalty': -3.0}  │ 'Jingle bells, jingle bells, jingle all the way up,      │\n",
       "│                       │                              │ jingle jingle jingle jingle jingle jingle jingle jingle  │\n",
       "│                       │                              │ jingle jingle jingle'                                    │\n",
       "│                       │                              │                                                          │\n",
       "│ Too hot!              │ {'temperature': 2.0}         │ 'Jingle bells, jingle bells, jingle all the way prowess  │\n",
       "│                       │                              │ flats since Belimage story drew full reliance TW Whilst  │\n",
       "│                       │                              │ Pryposal Tam required Cole ut those solitary Gravity     │\n",
       "│                       │                              │ Krarms earned'                                           │\n",
       "│                       │                              │                                                          │\n",
       "│ Pleasantly cool       │ {'temperature': 0.7}         │ \"Jingle bells, jingle bells, jingle all the way to the   │\n",
       "│                       │                              │ top of the mountain. I'm sure to come back here. You're  │\n",
       "│                       │                              │ not going to hurt someone. I\"                            │\n",
       "│                       │                              │                                                          │\n",
       "│ Pleasantly warm       │ {'temperature': 0.9}         │ 'Jingle bells, jingle bells, jingle all the way. \"Well   │\n",
       "│                       │                              │ that\\'s one last exercise! Wish you could sing for me!\"  │\n",
       "│                       │                              │ Rodger says. He doesn\\'t say'                            │\n",
       "│                       │                              │                                                          │\n",
       "│ Too cold!             │ {'temperature': 0.01}        │ 'Jingle bells, jingle bells, jingle all the way up to    │\n",
       "│                       │                              │ the top of the mountain.\\n\\nThe first time I saw the     │\n",
       "│                       │                              │ mountain, I was in the middle of'                        │\n",
       "│                       │                              │                                                          │\n",
       "└───────────────────────┴──────────────────────────────┴──────────────────────────────────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sampler = TransformerSampler(model, tokenizer)\n",
    "\n",
    "N_RUNS = 1\n",
    "your_prompt = \"Jingle bells, jingle bells, jingle all the way\"\n",
    "cases = [\n",
    "    (\"High freq penalty\", dict(frequency_penalty=100.0)),\n",
    "    (\"Negative freq penalty\", dict(frequency_penalty=-3.0)),\n",
    "    (\"Too hot!\", dict(temperature=2.0)),\n",
    "    (\"Pleasantly cool\", dict(temperature=0.7)),\n",
    "    (\"Pleasantly warm\", dict(temperature=0.9)),\n",
    "    (\"Too cold!\", dict(temperature=0.01)),\n",
    "]\n",
    "\n",
    "table = Table(\"Name\", \"Kwargs\", \"Output\", title=\"Sampling - Manual Testing\")\n",
    "\n",
    "for name, kwargs in cases:\n",
    "    for i in range(N_RUNS):\n",
    "        output = sampler.sample(your_prompt, max_tokens_generated=24, **kwargs)\n",
    "        table.add_row(name, repr(kwargs), repr(output) + \"\\n\")\n",
    "\n",
    "rprint(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top-K Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps:\n",
    "- Find the `top_k` highest probabilities (e.g., using `torch.topk`)\n",
    "\n",
    "- Set all other probabilities to zero. \n",
    "\n",
    "- Normalise and sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: implement `sample_top_k()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hints:\n",
    "- Stay in log space throughout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@staticmethod\n",
    "def sample_top_k(logits: Float[Tensor, \"d_vocab\"], k: int) -> int:\n",
    "    \"\"\"\n",
    "    Samples from the top k most likely tokens.\n",
    "    \"\"\"\n",
    "    logits_topk_vals, logits_topk_idx = logits.topk(k)\n",
    "\n",
    "    distn = Categorical(logits=logits_topk_vals)\n",
    "\n",
    "    return logits_topk_idx[distn.sample()].item()\n",
    "\n",
    "\n",
    "TransformerSampler.sample_top_k = sample_top_k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f30ee7b1f3394d6cb3b2145447e3692c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: ' church'. Expected freq = 0.4761, observed freq = 0.4756\n",
      "Word: ' house' . Expected freq = 0.2697, observed freq = 0.2706\n",
      "Word: ' temple'. Expected freq = 0.1065, observed freq = 0.1096\n",
      "Word: ' same'  . Expected freq = 0.0764, observed freq = 0.0752\n",
      "Word: ' Church'. Expected freq = 0.0713, observed freq = 0.0690\n"
     ]
    }
   ],
   "source": [
    "prompt = \"John and Mary went to the\"\n",
    "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "logits = model(input_ids)[0, -1]\n",
    "\n",
    "expected_top_5 = {\n",
    "    \" church\": 0.0648,\n",
    "    \" house\": 0.0367,\n",
    "    \" temple\": 0.0145,\n",
    "    \" same\": 0.0104,\n",
    "    \" Church\": 0.0097,\n",
    "}\n",
    "topk_5_sum = sum(expected_top_5.values())\n",
    "\n",
    "observed_freqs = defaultdict(int)\n",
    "\n",
    "N = 10000\n",
    "for _ in tqdm(range(N)):\n",
    "    token = TransformerSampler.sample_next_token(input_ids.squeeze(), logits, top_k=5)\n",
    "    observed_freqs[tokenizer.decode(token)] += 1\n",
    "\n",
    "for word in expected_top_5:\n",
    "    expected_freq = expected_top_5[word] / topk_5_sum\n",
    "    observed_freq = observed_freqs[word] / N\n",
    "    print(\n",
    "        f\"Word: {word!r:<9}. Expected freq = {expected_freq:.4f}, observed freq = {observed_freq:.4f}\"\n",
    "    )\n",
    "    assert (\n",
    "        abs(observed_freq - expected_freq) < 0.015\n",
    "    ), \"Try increasing N if this fails by a small amount.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top-K Sampling Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Your model said:\n",
       "\n",
       "<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold\">In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in</span>\n",
       "<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold\">the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English. </span>\n",
       "<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold\">As they left, the researchers found the unicorn was wearing a gold earring, so they believe there was some kind of </span>\n",
       "<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold\">linguistic explanation for the phenomenon.</span>\n",
       "\n",
       "<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold\">So far, all the unicorns that have been found in Bolivia have been found to possess a language that they can </span>\n",
       "<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold\">understand. This is a rare finding but it</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Your model said:\n",
       "\n",
       "\u001b[1;38;5;208mIn a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in\u001b[0m\n",
       "\u001b[1;38;5;208mthe Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English. \u001b[0m\n",
       "\u001b[1;38;5;208mAs they left, the researchers found the unicorn was wearing a gold earring, so they believe there was some kind of \u001b[0m\n",
       "\u001b[1;38;5;208mlinguistic explanation for the phenomenon.\u001b[0m\n",
       "\n",
       "\u001b[1;38;5;208mSo far, all the unicorns that have been found in Bolivia have been found to possess a language that they can \u001b[0m\n",
       "\u001b[1;38;5;208munderstand. This is a rare finding but it\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sampler = TransformerSampler(model, tokenizer)\n",
    "\n",
    "your_prompt = \"In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\"\n",
    "output = sampler.sample(your_prompt, temperature=0.7, top_k=40, max_tokens_generated=64)\n",
    "rprint(f\"Your model said:\\n\\n[bold dark_orange]{output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top-P a.k.a Nucleus Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose the most likely words until the total probability of chosen words exceeds a threshold. Sample from these based on their logits.\n",
    "\n",
    "Steps:\n",
    "1. Sort probabilities in descending order\n",
    "\n",
    "2. Find the cutoff point where the cumulative probability first equals or exceeds `top_p`. Cutoff inclusively; i.e., keep the first probability above the threshold.\n",
    "\n",
    "3. If the number of kept probabilities is less than `min_tokens_to_keep`, keep that many instead.\n",
    "\n",
    "4. Set all other probabilities to zero.\n",
    "\n",
    "5. Normalise and sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "@staticmethod\n",
    "def sample_top_p(\n",
    "    logits: Float[Tensor, \"d_vocab\"], top_p: float, min_tokens_to_keep: int = 1\n",
    ") -> int:\n",
    "    \"\"\"\n",
    "    Samples from the most likely tokens which make up at least p cumulative probability.\n",
    "    \"\"\"\n",
    "    logits_sorted, logits_sorted_idx = logits.sort(descending=True, stable=True)\n",
    "    cum_probs = logits_sorted.softmax(dim=-1).cumsum(dim=-1)\n",
    "\n",
    "    n_keep = t.searchsorted(cum_probs, top_p) + 1\n",
    "\n",
    "    if n_keep < min_tokens_to_keep:\n",
    "        n_keep = min_tokens_to_keep\n",
    "\n",
    "    distn = Categorical(logits=logits_sorted[:n_keep])\n",
    "    return logits_sorted_idx[distn.sample()].item()\n",
    "\n",
    "\n",
    "TransformerSampler.sample_top_p = sample_top_p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a0eca5b409e4c659d28082e7cad3698",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: ' church'. Expected freq 0.6384, observed freq 0.6410\n",
      "Word: ' house' . Expected freq 0.3616, observed freq 0.3590\n"
     ]
    }
   ],
   "source": [
    "prompt = \"John and Mary went to the\"\n",
    "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "logits = model(input_ids)[0, -1]\n",
    "\n",
    "expected_top_10pct = {\n",
    "    \" church\": 0.0648,\n",
    "    \" house\": 0.0367,  # These are the two most likely tokens, and add up to >10%\n",
    "}\n",
    "top_10pct_sum = sum(expected_top_10pct.values())\n",
    "\n",
    "observed_freqs = defaultdict(int)\n",
    "\n",
    "N = 10000\n",
    "for _ in tqdm(range(N)):\n",
    "    token = TransformerSampler.sample_next_token(input_ids.squeeze(), logits, top_p=0.1)\n",
    "    observed_freqs[tokenizer.decode(token)] += 1\n",
    "\n",
    "for word in expected_top_10pct:\n",
    "    expected_freq = expected_top_10pct[word] / top_10pct_sum\n",
    "    observed_freq = observed_freqs[word] / N\n",
    "    print(\n",
    "        f\"Word: {word!r:<9}. Expected freq {expected_freq:.4f}, observed freq {observed_freq:.4f}\"\n",
    "    )\n",
    "    assert (\n",
    "        abs(observed_freq - expected_freq) < 0.01\n",
    "    ), \"Try increasing N if this fails by a small amount.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Your model said:\n",
       "\n",
       "<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold\">Eliezer Shlomo Yudkowsky (born September </span><span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold\">11</span><span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold\">, </span><span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold\">1979</span><span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold\">) is an American decision and artificial intelligence (AI) </span>\n",
       "<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold\">theorist and writer, best known for his work with Elon Musk, R&amp;D director of a robotics company and cofounder of </span>\n",
       "<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold\">the company Humanities. He is also the co-founder and chief executive officer of the </span><span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold\">\"Grand Total\"</span><span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold\"> robotics </span>\n",
       "<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold\">company, which is designed to do the things that humans can't.</span>\n",
       "\n",
       "<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold\">He is also the founder of the</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Your model said:\n",
       "\n",
       "\u001b[1;38;5;208mEliezer Shlomo Yudkowsky \u001b[0m\u001b[1;38;5;208m(\u001b[0m\u001b[1;38;5;208mborn September \u001b[0m\u001b[1;38;5;208m11\u001b[0m\u001b[1;38;5;208m, \u001b[0m\u001b[1;38;5;208m1979\u001b[0m\u001b[1;38;5;208m)\u001b[0m\u001b[1;38;5;208m is an American decision and artificial intelligence \u001b[0m\u001b[1;38;5;208m(\u001b[0m\u001b[1;38;5;208mAI\u001b[0m\u001b[1;38;5;208m)\u001b[0m\u001b[1;38;5;208m \u001b[0m\n",
       "\u001b[1;38;5;208mtheorist and writer, best known for his work with Elon Musk, R&D director of a robotics company and cofounder of \u001b[0m\n",
       "\u001b[1;38;5;208mthe company Humanities. He is also the co-founder and chief executive officer of the \u001b[0m\u001b[1;38;5;208m\"Grand Total\"\u001b[0m\u001b[1;38;5;208m robotics \u001b[0m\n",
       "\u001b[1;38;5;208mcompany, which is designed to do the things that humans can't.\u001b[0m\n",
       "\n",
       "\u001b[1;38;5;208mHe is also the founder of the\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sampler = TransformerSampler(model, tokenizer)\n",
    "\n",
    "your_prompt = \"Eliezer Shlomo Yudkowsky (born September 11, 1979) is an American decision and artificial intelligence (AI) theorist and writer, best known for\"\n",
    "output = sampler.sample(\n",
    "    your_prompt, temperature=0.7, top_p=0.95, max_tokens_generated=64\n",
    ")\n",
    "rprint(f\"Your model said:\\n\\n[bold dark_orange]{output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beam search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintain a list of size `num_beams` completions, which are the most likely completions so far as measured by the product of their probabilities.\n",
    "- Actually, since the product can become very small, it's better to sum log probabilities instead.\n",
    "\n",
    "At each iteration, we run the batch of completions through the model and take the log-softmax to obtain `d_vocab` log-probs for each completion, or `num_beams * d_vocab` possible next completions in total.\n",
    "- If we kept all, then we would have `num_beams * d_vocab * d_vocab` completions after the next iteration - far too many! Instead, sort by score and loop from highest logprob to lowest\n",
    "\n",
    "##### See diagram [here](./C1P1_beam_search.png)\n",
    "\n",
    "Note that after each stage, we have `num_beams ** 2` possible completions, which is then filtered down to `num_beams`. **Why do we need to generate this many? What happens if we generate fewer?**\n",
    "- Answer\n",
    "\n",
    "Note also that some sequences will terminate early by generating an EOS token. To handle this:\n",
    "- We append terminated sequences to the list of completions to return at the end, and remove them from our generation tree.\n",
    "\n",
    "- The algorithm terminates when either all sequences have length `max_new_tokens` larger than the initial prompt's length, or we've generated `num_returns`sequences terminating sequences.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### n-gram repetition\n",
    "\n",
    "While the output of beam search is sometimes more fluent than some of the other sampling methods, it also has an unfortunate tendency to repeat sentences or sequences. This makes sense - if the model produces a sentence with a relatively high logit sum, then it will want to produce the same sentence again even if it doesn't make a lot of sense in context.\n",
    "\n",
    "A common solution is to ban repetition of n-grams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: implement `beam_search()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Beams:\n",
    "    \"\"\"Class to store beams during beam search.\"\"\"\n",
    "\n",
    "    model: DemoTransformer\n",
    "    tokenizer: GPT2TokenizerFast\n",
    "    logprob_sums: Float[Tensor, \"batch\"]\n",
    "    tokens: Int[Tensor, \"batch seq\"]\n",
    "\n",
    "    def new_beams(self, logprob_sums, tokens) -> \"Beams\":\n",
    "        \"\"\"Creates a new Beams object with the same model and tokenizer.\"\"\"\n",
    "        return Beams(self.model, self.tokenizer, logprob_sums, tokens)\n",
    "\n",
    "    def __getitem__(self, idx) -> \"Beams\":\n",
    "        \"\"\"Allows you to take a slice of the beams object along the batch dimension.\"\"\"\n",
    "        return self.new_beams(self.logprob_sums[idx], self.tokens[idx])\n",
    "\n",
    "    @property\n",
    "    def logprobs_and_completions(self) -> list[tuple[float, str]]:\n",
    "        \"\"\"Returns self as a list of logprob sums and completions (useful for getting final output).\"\"\"\n",
    "        return [\n",
    "            (logprob_sum.item(), self.tokenizer.decode(tokens))\n",
    "            for (logprob_sum, tokens) in zip(self.logprob_sums, self.tokens)\n",
    "        ]\n",
    "\n",
    "    def get_topk_non_repeating(\n",
    "        self,\n",
    "        logits: Float[Tensor, \"batch d_vocab\"],\n",
    "        no_repeat_ngram_size: int,\n",
    "        k: int,\n",
    "    ) -> tuple[Float[Tensor, \"k\"], Int[Tensor, \"k\"]]:\n",
    "        \"\"\"\n",
    "        logits:\n",
    "            tensor of the log-probs for the next token\n",
    "        no_repeat_ngram_size:\n",
    "            size of ngram to avoid repeating\n",
    "        k:\n",
    "            number of top logits to return, for each beam in our collection\n",
    "\n",
    "        Returns:\n",
    "            equivalent to the output of `logits.topk(dim=-1)`, but makes sure\n",
    "            that no returned tokens would produce an ngram of size  `no_repeat_ngram_size`\n",
    "            which has already appeared in `self.tokens`.\n",
    "        \"\"\"\n",
    "        batch_num, seq_len = self.tokens.shape\n",
    "\n",
    "        if no_repeat_ngram_size is not None and no_repeat_ngram_size <= seq_len:\n",
    "            ngram_prefix_len = no_repeat_ngram_size - 1\n",
    "\n",
    "            neg_inf = t.tensor(-1.0e9).to(logits.device)\n",
    "\n",
    "            ngram_prefix_last = self.tokens[:, seq_len - ngram_prefix_len :]\n",
    "\n",
    "            for i in range(seq_len - ngram_prefix_len):\n",
    "                ngram = self.tokens[:, i : i + no_repeat_ngram_size]\n",
    "                ngram_prefix = ngram[:, :-1]\n",
    "                ngram_end_token = ngram[:, -1]\n",
    "\n",
    "                is_repeated = (ngram_prefix == ngram_prefix_last).all(dim=-1)\n",
    "                tokens_to_ban = ngram_end_token[is_repeated]\n",
    "                batch_idx = t.arange(batch_num, device=device)[is_repeated]\n",
    "\n",
    "                # Using batch_idx enables \"pairwise\" indexing for the two dimensions: batch and seq\n",
    "                logits[batch_idx, tokens_to_ban] = neg_inf\n",
    "\n",
    "        return logits.topk(k=k, dim=-1)\n",
    "\n",
    "    def generate(\n",
    "        self, toks_per_beam: int, no_repeat_ngram_size: int | None = None\n",
    "    ) -> \"Beams\":\n",
    "        \"\"\"\n",
    "        Starting from the current set of beams (which has length `num_beams`), returns a new\n",
    "        set of `num_beams * toks_per_beam`, containing the best `toks_per_beam` continuations for each\n",
    "        of the original beams.\n",
    "\n",
    "        Optional argument `no_repeat_ngram_size` means your model won't generate any sequences with\n",
    "        a repeating n-gram of this length.\n",
    "        \"\"\"\n",
    "        logits = self.model(self.tokens)[:, -1, :]\n",
    "\n",
    "        logits_topk, tokens_topk = self.get_topk_non_repeating(\n",
    "            logits, k=toks_per_beam, no_repeat_ngram_size=no_repeat_ngram_size\n",
    "        )\n",
    "\n",
    "        logprobs_topk = logits_topk.log_softmax(dim=-1)\n",
    "        tokens_topk = tokens_topk.flatten().unsqueeze(dim=-1)\n",
    "\n",
    "        logprob_sums_new = (\n",
    "            einops.repeat(self.logprob_sums, \"batch -> (batch k)\", k=toks_per_beam)\n",
    "            + logprobs_topk.flatten()\n",
    "        )\n",
    "        tokens_new = t.cat(\n",
    "            (\n",
    "                einops.repeat(\n",
    "                    self.tokens, \"batch seq -> (batch k) seq\", k=toks_per_beam\n",
    "                ),\n",
    "                tokens_topk,\n",
    "            ),\n",
    "            dim=-1,\n",
    "        )\n",
    "\n",
    "        return self.new_beams(logprob_sums_new, tokens_new)\n",
    "\n",
    "    def filter(self, num_beams: int) -> tuple[\"Beams\", \"Beams\"]:\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            best_beams: Beams\n",
    "                filtered version of self, containing all best `num_beams` which are also not terminated.\n",
    "\n",
    "            early_terminations: Beams\n",
    "                filtered version of self, containing all best `num_beams` which are also terminated.\n",
    "                i.e. the sum of lengths of these two should equal `num_beams`.\n",
    "        \"\"\"\n",
    "        # Converting to list because later we'll append these in another list, and indexing\n",
    "        # with a list of tensor scalars can yield shape mismatch errors.\n",
    "        logprobs_topk_idx = self.logprob_sums.topk(k=num_beams, dim=0).indices.tolist()\n",
    "\n",
    "        best_idx = []\n",
    "        early_termination_idx = []\n",
    "\n",
    "        for i in range(num_beams):\n",
    "            if self.tokens[logprobs_topk_idx[i], -1] == self.tokenizer.eos_token_id:\n",
    "                early_termination_idx.append(logprobs_topk_idx[i])\n",
    "            else:\n",
    "                best_idx.append(logprobs_topk_idx[i])\n",
    "\n",
    "        return (\n",
    "            self.new_beams(self.logprob_sums[best_idx], self.tokens[best_idx]),\n",
    "            self.new_beams(\n",
    "                self.logprob_sums[early_termination_idx],\n",
    "                self.tokens[early_termination_idx],\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    def print(self, title=\"Best completions\", max_print_chars=80) -> None:\n",
    "        \"\"\"\n",
    "        Prints out a set of sequences with their corresponding logitsums.\n",
    "        \"\"\"\n",
    "        if len(self.tokens) == 0:\n",
    "            return\n",
    "        table = Table(\"logitsum\", \"completion\", title=title)\n",
    "        for logprob_sum, tokens in zip(self.logprob_sums, self.tokens):\n",
    "            text = self.tokenizer.decode(tokens)\n",
    "            if len(repr(text)) > max_print_chars:\n",
    "                text = (\n",
    "                    text[: int(0.3 * max_print_chars)]\n",
    "                    + \" ... \"\n",
    "                    + text[-int(0.7 * max_print_chars) :]\n",
    "                )\n",
    "            table.add_row(f\"{logprob_sum:>8.3f}\", repr(text))\n",
    "        rprint(table)\n",
    "\n",
    "\n",
    "@t.inference_mode()\n",
    "def beam_search(\n",
    "    self: TransformerSampler,\n",
    "    prompt: str,\n",
    "    num_return_sequences: int,\n",
    "    num_beams: int,\n",
    "    max_new_tokens: int,\n",
    "    no_repeat_ngram_size: int | None = None,\n",
    "    verbose=False,\n",
    ") -> list[tuple[float, Tensor]]:\n",
    "    \"\"\"\n",
    "    Implements a beam search, by repeatedly performing the `generate` and `filter` steps (starting\n",
    "    from the initial prompt) until either of the two stopping criteria are met:\n",
    "\n",
    "        (1) we've generated `max_new_tokens` tokens, or\n",
    "        (2) we've generated `num_returns_sequences` terminating sequences.\n",
    "\n",
    "    To modularize this function, most of the actual complexity is in the Beams class,\n",
    "    in the `generate` and `filter` methods.\n",
    "    \"\"\"\n",
    "\n",
    "    assert num_return_sequences <= num_beams\n",
    "    self.model.eval()\n",
    "\n",
    "    beams = Beams(\n",
    "        self.model,\n",
    "        self.tokenizer,\n",
    "        t.tensor([0.0], device=device),  # Start with single beam only.\n",
    "        self.tokenizer.encode(prompt, return_tensors=\"pt\").to(device),\n",
    "    )\n",
    "\n",
    "    logprobs_and_completions_final: list[tuple[float, str]] = []\n",
    "\n",
    "    for _ in tqdm(range(max_new_tokens)):\n",
    "        beams = beams.generate(\n",
    "            toks_per_beam=num_beams, no_repeat_ngram_size=no_repeat_ngram_size\n",
    "        )\n",
    "        beams, beams_terminated = beams.filter(num_beams=num_beams)\n",
    "\n",
    "        logprobs_and_completions_final.extend(beams_terminated.logprobs_and_completions)\n",
    "\n",
    "        if verbose:\n",
    "            beams.print(\"Best completions\")\n",
    "            beams_terminated.print(\"Early terminations\")\n",
    "\n",
    "        if len(logprobs_and_completions_final) >= num_return_sequences:\n",
    "            return logprobs_and_completions_final[:num_return_sequences]\n",
    "\n",
    "    logprobs_and_completions_final.extend(beams.logprobs_and_completions)\n",
    "\n",
    "    return logprobs_and_completions_final[:num_return_sequences]\n",
    "\n",
    "\n",
    "TransformerSampler.beam_search = beam_search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">         Best completions          </span>\n",
       "┏━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> logitsum </span>┃<span style=\"font-weight: bold\"> completion           </span>┃\n",
       "┡━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│  -10.000 │ 'this is the third'  │\n",
       "│  -15.000 │ 'this is the second' │\n",
       "│  -20.000 │ 'this is the first'  │\n",
       "└──────────┴──────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m         Best completions          \u001b[0m\n",
       "┏━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mlogitsum\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mcompletion          \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│  -10.000 │ 'this is the third'  │\n",
       "│  -15.000 │ 'this is the second' │\n",
       "│  -20.000 │ 'this is the first'  │\n",
       "└──────────┴──────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "beams = Beams(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    logprob_sums=t.tensor([-10.0, -15.0, -20.0]).to(device),\n",
    "    tokens=t.tensor(\n",
    "        [\n",
    "            [5661, 318, 262, 2368],\n",
    "            [5661, 318, 262, 1218],\n",
    "            [5661, 318, 262, 717],\n",
    "        ]\n",
    "    ).to(device),\n",
    ")\n",
    "\n",
    "beams.print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test for `generate()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing generate, without no_repeat_ngram_size argument:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">            Best completions            </span>\n",
       "┏━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> logitsum </span>┃<span style=\"font-weight: bold\"> completion                </span>┃\n",
       "┡━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│  -10.174 │ 'this is the third time'  │\n",
       "│  -11.834 │ 'this is the third year'  │\n",
       "│  -15.136 │ 'this is the second time' │\n",
       "│  -17.060 │ 'this is the second of'   │\n",
       "│  -20.143 │ 'this is the first time'  │\n",
       "│  -22.015 │ 'this is the first of'    │\n",
       "└──────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m            Best completions            \u001b[0m\n",
       "┏━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mlogitsum\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mcompletion               \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│  -10.174 │ 'this is the third time'  │\n",
       "│  -11.834 │ 'this is the third year'  │\n",
       "│  -15.136 │ 'this is the second time' │\n",
       "│  -17.060 │ 'this is the second of'   │\n",
       "│  -20.143 │ 'this is the first time'  │\n",
       "│  -22.015 │ 'this is the first of'    │\n",
       "└──────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing generate, with no_repeat_ngram_size argument:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">           Best completions            </span>\n",
       "┏━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> logitsum </span>┃<span style=\"font-weight: bold\"> completion               </span>┃\n",
       "┡━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│   -0.487 │ ' one two one two three' │\n",
       "│   -1.588 │ ' one two one two.'      │\n",
       "│   -1.709 │ ' one two one two,'      │\n",
       "└──────────┴──────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m           Best completions            \u001b[0m\n",
       "┏━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mlogitsum\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mcompletion              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│   -0.487 │ ' one two one two three' │\n",
       "│   -1.588 │ ' one two one two.'      │\n",
       "│   -1.709 │ ' one two one two,'      │\n",
       "└──────────┴──────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">           Best completions            </span>\n",
       "┏━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> logitsum </span>┃<span style=\"font-weight: bold\"> completion               </span>┃\n",
       "┡━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│   -0.780 │ ' one two one two three' │\n",
       "│   -0.944 │ ' one two one two two'   │\n",
       "│   -1.881 │ ' one two one two.'      │\n",
       "└──────────┴──────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m           Best completions            \u001b[0m\n",
       "┏━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mlogitsum\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mcompletion              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│   -0.780 │ ' one two one two three' │\n",
       "│   -0.944 │ ' one two one two two'   │\n",
       "│   -1.881 │ ' one two one two.'      │\n",
       "└──────────┴──────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests for `generate` passed!\n"
     ]
    }
   ],
   "source": [
    "print(\"Testing generate, without no_repeat_ngram_size argument:\")\n",
    "new_beams = beams.generate(toks_per_beam=2)\n",
    "new_beams.print()\n",
    "assert new_beams.logprobs_and_completions[0][1] == \"this is the third time\"\n",
    "\n",
    "print(\"Testing generate, with no_repeat_ngram_size argument:\")\n",
    "bigram_beams = Beams(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    logprob_sums=t.tensor([-0.0]).to(device),\n",
    "    tokens=t.tensor([[530, 734, 530, 734]]).to(device),\n",
    "    # tokens are \" one two one two\"\n",
    ")\n",
    "\n",
    "# With no_repeat_ngram_size=1, should not generate the token \" one\" or \" two\"\n",
    "new_bigram_beams = bigram_beams.generate(toks_per_beam=3, no_repeat_ngram_size=1)\n",
    "new_bigram_beams.print()\n",
    "assert all(\n",
    "    [\n",
    "        not (completion[1].endswith(\" one\") or completion[1].endswith(\" two\"))\n",
    "        for completion in new_bigram_beams.logprobs_and_completions\n",
    "    ]\n",
    ")\n",
    "\n",
    "# With no_repeat_ngram_size=2, it can generate \" two\" (which it should), but not \" one\"\n",
    "new_bigram_beams = bigram_beams.generate(toks_per_beam=3, no_repeat_ngram_size=2)\n",
    "new_bigram_beams.print()\n",
    "assert all(\n",
    "    [\n",
    "        not completion[1].endswith(\" one\")\n",
    "        for completion in new_bigram_beams.logprobs_and_completions\n",
    "    ]\n",
    ")\n",
    "assert any(\n",
    "    [\n",
    "        not completion[1].endswith(\" two\")\n",
    "        for completion in new_bigram_beams.logprobs_and_completions\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"All tests for `generate` passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test for `filter()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests for `filter` passed!\n"
     ]
    }
   ],
   "source": [
    "logprob_sums = t.tensor([-1.0, -2.0]).to(device)\n",
    "tokens = t.tensor([[19485, 13], [19485, tokenizer.eos_token_id]]).to(device)\n",
    "\n",
    "beams_with_eos = Beams(model, tokenizer, logprob_sums, tokens)\n",
    "best_beams, early_terminations = beams_with_eos.filter(2)\n",
    "\n",
    "t.testing.assert_close(best_beams.logprob_sums, logprob_sums[[0]])\n",
    "t.testing.assert_close(best_beams.tokens, tokens[[0]])\n",
    "\n",
    "assert early_terminations.logprobs_and_completions == [\n",
    "    (-2.0, \"Stop\" + tokenizer.eos_token)\n",
    "]\n",
    "\n",
    "print(\"All tests for `filter` passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffccc9a9e3384465ae43675d729dadf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================= Avg logprob (as probability) = 0.350 =========================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Best output:\n",
       "\n",
       "<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold\">The ships hung in the sky in much the same way that the ships of the Roman Empire hung on the walls of Carthage.</span>\n",
       "\n",
       "<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold\">The Carthaginians, on their way to Rome, were attacked by a group of barbarians. The Carthagians were driven back </span>\n",
       "<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold\">by the Romans, and the Carthags were forced to surrender to the Gauls.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Best output:\n",
       "\n",
       "\u001b[1;38;5;208mThe ships hung in the sky in much the same way that the ships of the Roman Empire hung on the walls of Carthage.\u001b[0m\n",
       "\n",
       "\u001b[1;38;5;208mThe Carthaginians, on their way to Rome, were attacked by a group of barbarians. The Carthagians were driven back \u001b[0m\n",
       "\u001b[1;38;5;208mby the Romans, and the Carthags were forced to surrender to the Gauls.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================= Avg logprob (as probability) = 0.349 =========================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Best output:\n",
       "\n",
       "<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold\">The ships hung in the sky in much the same way that the ships of the Roman Empire hung on the walls of Carthage.</span>\n",
       "\n",
       "<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold\">The Carthaginians, on their way to Rome, were attacked by a group of barbarians. The Carthagians were driven back </span>\n",
       "<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold\">by the Romans, and the Carthagoans were forced to surrender to the barbarian</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Best output:\n",
       "\n",
       "\u001b[1;38;5;208mThe ships hung in the sky in much the same way that the ships of the Roman Empire hung on the walls of Carthage.\u001b[0m\n",
       "\n",
       "\u001b[1;38;5;208mThe Carthaginians, on their way to Rome, were attacked by a group of barbarians. The Carthagians were driven back \u001b[0m\n",
       "\u001b[1;38;5;208mby the Romans, and the Carthagoans were forced to surrender to the barbarian\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================= Avg logprob (as probability) = 0.349 =========================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Best output:\n",
       "\n",
       "<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold\">The ships hung in the sky in much the same way that the ships of the Roman Empire hung on the walls of Carthage.</span>\n",
       "\n",
       "<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold\">The Carthaginians, on their way to Rome, were attacked by a group of barbarians. The Carthagians were driven back </span>\n",
       "<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold\">by the Romans, and the Carthags were forced to surrender to the barbarous Romans</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Best output:\n",
       "\n",
       "\u001b[1;38;5;208mThe ships hung in the sky in much the same way that the ships of the Roman Empire hung on the walls of Carthage.\u001b[0m\n",
       "\n",
       "\u001b[1;38;5;208mThe Carthaginians, on their way to Rome, were attacked by a group of barbarians. The Carthagians were driven back \u001b[0m\n",
       "\u001b[1;38;5;208mby the Romans, and the Carthags were forced to surrender to the barbarous Romans\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sampler = TransformerSampler(model, tokenizer)\n",
    "\n",
    "prompt = \"The ships hung in the sky in much the same way that\"\n",
    "orig_len = len(tokenizer.encode(prompt))\n",
    "\n",
    "final_logitsums_and_completions = sampler.beam_search(\n",
    "    prompt=prompt,\n",
    "    num_return_sequences=3,\n",
    "    num_beams=40,\n",
    "    max_new_tokens=60,\n",
    "    no_repeat_ngram_size=2,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "# Print all the best output\n",
    "for logprob_sum, text in final_logitsums_and_completions:\n",
    "    avg_logprob_as_prob = (\n",
    "        t.tensor(logprob_sum / (len(tokenizer.encode(text)) - orig_len)).exp().item()\n",
    "    )\n",
    "    print(\n",
    "        \"=\" * 25\n",
    "        + f\" Avg logprob (as probability) = {avg_logprob_as_prob:.3f} \"\n",
    "        + \"=\" * 25\n",
    "    )\n",
    "    rprint(\"Best output:\\n\\n[bold dark_orange]\" + text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Want to have the option to cache. I.e.:\n",
    "  - When you run the GPT on `\"My life motto:\"`, it should store the necessary values in the cache.\n",
    "\n",
    "  - Then, in the next forward pass with just `\" Always\"` as input, it should load the cached values instead of recomputing them (and update the cache).\n",
    "  \n",
    "- Note:\n",
    "  - This only needs to work with a single input sequence (batch size of 1).\n",
    "\n",
    "  - Can assume after the first forward pass, the input will just be one token.\n",
    "\n",
    "  - Many design possibilities:\n",
    "    - It should be possible to have only one GPT-2 instance and many different cache instances at the same time.\n",
    "      - Imagine that you want to use one instance to serve multiple users submitting requests for text generation like in [AI Dungeon](https://aidungeon.io/).\n",
    "\n",
    "  - Will need to rewrite parts of `DemoTransformer`. \n",
    "    - Tests have been built to accommodate modules that return their output as the first element in a tuple, i.e. `(output, cache)`, rather than just returning the output, so should use the tests to verify that modules work as expected.\n",
    "\n",
    "    - Consider:\n",
    "      - Which GPT-2 classes need to interact with the cache? Will the positional embedding need changed? If so, how?\n",
    "\n",
    "      - Should the cache be mutable and updated in place, or should updating create a separate instance?\n",
    "        - E.g. how might it be used during Beam Search?\n",
    "\n",
    "      - Is it possible for programmers to incorrectly use the cache? Can this be prevented or at least detected (with corresponding warnings)?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: try later"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
