{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import os\n",
    "\n",
    "from jaxtyping import Float, Int\n",
    "import torch as t\n",
    "from torch import Tensor\n",
    "from torch.distributions.categorical import Categorical\n",
    "from transformer_lens import HookedTransformer\n",
    "from transformers.models.gpt2.tokenization_gpt2_fast import GPT2TokenizerFast\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from C1P1__mj_implementation import Config, DemoTransformer\n",
    "\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = t.device(\n",
    "    \"mps\"\n",
    "    if t.backends.mps.is_available()\n",
    "    else \"cuda\"\n",
    "    if t.cuda.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load GPT-2 Small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\matth\\Workspace\\arena3\\.venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1602: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "reference_gpt2 = HookedTransformer.from_pretrained(\n",
    "    \"gpt2-small\",\n",
    "    fold_ln=False,\n",
    "    center_unembed=False,\n",
    "    center_writing_weights=False,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling from a Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Objectives\n",
    "\n",
    "- Learn how to sample from a transformer.\n",
    "  - Includes basic methods like **greedy search** or **top-k**, and more advanced methods like **beam search**.\n",
    "\n",
    "- Learn how to cache the output from a transformer, so that it can be used to generate text more efficiently.\n",
    "  - Rewrite sampling functions to use caching."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obvious way to sample tokens: always take the token assigned the highest probability!\n",
    "\n",
    "- This can lead to boring and repetitive outcomes.\n",
    "- At worst, it can lock our transformer into a loop.\n",
    "\n",
    "##### Read the HuggingFace blog post: [\"How to generate text: using different decoding methods for language generation with Transformers\"](https://huggingface.co/blog/how-to-generate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TransformerSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cfg = Config()\n",
    "model = DemoTransformer(model_cfg).to(device)\n",
    "model.load_state_dict(reference_gpt2.state_dict(), strict=False)\n",
    "\n",
    "tokenizer = reference_gpt2.tokenizer\n",
    "\n",
    "\n",
    "class TransformerSampler:\n",
    "    def __init__(self, model: DemoTransformer, tokenizer: GPT2TokenizerFast):\n",
    "        self.model = model\n",
    "        self.cfg = model.cfg\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    @t.inference_mode()\n",
    "    def sample(self, prompt: str, max_tokens_generated=100, verbose=False, **kwargs):\n",
    "        \"\"\"\n",
    "        Returns a string of autoregressively generated text, starting from the prompt.\n",
    "\n",
    "        Sampling terminates at max_tokens_generated, or when the model generates an\n",
    "        end-of-sequence token.\n",
    "\n",
    "        kwargs are passed to sample_next_token, to give detailed instructions on how\n",
    "        new tokens are chosen.\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE!\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    @t.inference_mode()\n",
    "    def beam_search(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        num_return_sequences: int,\n",
    "        num_beams: int,\n",
    "        max_new_tokens: int,\n",
    "        no_repeat_ngram_size: int = 0,\n",
    "        verbose=False,\n",
    "    ) -> list[tuple[float, Tensor]]:\n",
    "        \"\"\"\n",
    "        Returns a string of autoregressively generated text, starting from the prompt.\n",
    "\n",
    "        Sampling terminates at max_tokens_generated, or when the model generates an\n",
    "        end-of-sequence token.\n",
    "\n",
    "        kwargs are passed to sample_next_token, to give detailed instructions on how\n",
    "        new tokens are chosen.\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE!\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    @staticmethod\n",
    "    def sample_next_token(\n",
    "        input_ids: Int[Tensor, \"seq_len\"],\n",
    "        logits: Float[Tensor, \"d_vocab\"],\n",
    "        temperature=1.0,\n",
    "        top_k=0,\n",
    "        top_p=0.0,\n",
    "        frequency_penalty=0.0,\n",
    "        seed=None,\n",
    "    ):\n",
    "        assert input_ids.ndim == 1, \"input_ids should be a 1D sequence of token ids\"\n",
    "        assert temperature >= 0, \"Temperature should be non-negative\"\n",
    "        assert 0 <= top_p <= 1.0, \"Top-p must be a probability\"\n",
    "        assert 0 <= top_k, \"Top-k must be non-negative\"\n",
    "        assert not (\n",
    "            top_p != 0 and top_k != 0\n",
    "        ), \"At most one of top-p and top-k supported\"\n",
    "\n",
    "        # Set random seeds for reproducibility\n",
    "        if seed is not None:\n",
    "            t.manual_seed(seed)\n",
    "            np.random.seed(seed)\n",
    "\n",
    "        # Apply all the specialized sampling methods\n",
    "        if temperature == 0:\n",
    "            return TransformerSampler.greedy_search(logits)\n",
    "        elif temperature != 1.0:\n",
    "            logits = TransformerSampler.apply_temperature(logits, temperature)\n",
    "        if frequency_penalty != 0.0:\n",
    "            logits = TransformerSampler.apply_frequency_penalty(\n",
    "                input_ids, logits, frequency_penalty\n",
    "            )\n",
    "        if top_k > 0:\n",
    "            return TransformerSampler.sample_top_k(logits, top_k)\n",
    "        if top_p > 0.0:\n",
    "            return TransformerSampler.sample_top_p(logits, top_p)\n",
    "        return TransformerSampler.sample_basic(logits)\n",
    "\n",
    "    @staticmethod\n",
    "    def greedy_search(logits: Float[Tensor, \"d_vocab\"]) -> int:\n",
    "        \"\"\"\n",
    "        Returns the most likely token (as an int).\n",
    "        \"\"\"\n",
    "        out = logits.argmax().item()\n",
    "        return out\n",
    "\n",
    "    @staticmethod\n",
    "    def apply_temperature(\n",
    "        logits: Float[Tensor, \"d_vocab\"], temperature: float\n",
    "    ) -> Float[Tensor, \"d_vocab\"]:\n",
    "        \"\"\"\n",
    "        Applies temperature scaling to the logits.\n",
    "        \"\"\"\n",
    "        return logits / temperature\n",
    "\n",
    "    @staticmethod\n",
    "    def apply_frequency_penalty(\n",
    "        input_ids: Int[Tensor, \"seq_len\"],\n",
    "        logits: Float[Tensor, \"d_vocab\"],\n",
    "        freq_penalty: float,\n",
    "    ) -> Float[Tensor, \"d_vocab\"]:\n",
    "        \"\"\"\n",
    "        Applies a frequency penalty to the logits.\n",
    "        \"\"\"\n",
    "        counts = t.bincount(input_ids, minlength=logits.shape[-1])\n",
    "\n",
    "        return logits - (freq_penalty * counts)\n",
    "\n",
    "    @staticmethod\n",
    "    def sample_basic(logits: Float[Tensor, \"d_vocab\"]) -> int:\n",
    "        \"\"\"\n",
    "        Samples from the distribution defined by the logits.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @staticmethod\n",
    "    def sample_top_k(logits: Float[Tensor, \"d_vocab\"], k: int) -> int:\n",
    "        \"\"\"\n",
    "        Samples from the top k most likely tokens.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @staticmethod\n",
    "    def sample_top_p(\n",
    "        logits: Float[Tensor, \"d_vocab\"], top_p: float, min_tokens_to_keep: int = 1\n",
    "    ) -> int:\n",
    "        \"\"\"\n",
    "        Samples from the most likely tokens which make up at least p cumulative probability.\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Sampling Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: implement `sample()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`sample()` takes in a prompt (string), encodes it as a sequence of token ids using `self.tokenizer.encode`, and then continually generates new tokens by repeating the following steps:\n",
    "\n",
    "1. Pass the tokenized prompt through the model to get logits\n",
    "\n",
    "2. Take the logit vector corresponding to the last token in the prompt (i.e., prediction for the *next* token)\n",
    "\n",
    "3. Sample from this distribution to get a new token, using `self.sample_next_token(input_ids, logits, **kwargs)`. `kwargs` contains all the sampling-specific args, e.g., **temperature**, **top-k**, etc.\n",
    "\n",
    "4. Append this new token to the input tokens, and repeat until we meet one of two termination criteria:\n",
    "   - We generate `max_tokens_generated` new tokens, or\n",
    "\n",
    "   - We generate the EOS token, accessed via `self.tokenizer.eos_token_id.\n",
    "\n",
    "Finally, we use `self.tokenizer.decode` to convert the generated token ids back into a string, and return the string.\n",
    "\n",
    "We also have a `verbose` arg - use to print output while it's being sampled.\n",
    "\n",
    "A few hints:\n",
    "- Don't forget about tensor shapes! The model's input should always have a `batch` dimension.\n",
    "\n",
    "- `sample_next_token()` will return an integer. Wrap this into a tensor before concatenating it to the end of the input IDs\n",
    "\n",
    "- Remember device!\n",
    "\n",
    "- Put the model in evaluation mode using `model.eval()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "@t.inference_mode()\n",
    "def sample(self, prompt: str, max_tokens_generated=100, verbose=False, **kwargs):\n",
    "    \"\"\"\n",
    "    Returns a string of autoregressively generated text, starting from the prompt.\n",
    "\n",
    "    Sampling terminates at max_tokens_generated, or when the model generates an\n",
    "    end-of-sequence token.\n",
    "\n",
    "    kwargs are passed to sample_next_token, to give detailed instructions on how\n",
    "    new tokens are chosen.\n",
    "    \"\"\"\n",
    "    self.model.eval()\n",
    "\n",
    "    # Default return type is list. `return_tensors=\"pt\"`` returns a 2D PyTorch tensor.\n",
    "    tokens = self.tokenizer.encode(prompt, return_tensors=\"pt\").to(device)[0]\n",
    "\n",
    "    for _ in range(max_tokens_generated):\n",
    "        # Need batch dim, and only the last n_ctx tokens - don't want to exceed context length (better to warn?).\n",
    "        logits_last_token = self.model(tokens[None, -self.cfg.n_ctx :])[0, -1]\n",
    "\n",
    "        # Note that sample_next_token will return a scalar tensor - need to wrap in a list to give it a dim.\n",
    "        token_next = t.tensor(\n",
    "            [self.sample_next_token(tokens, logits_last_token, **kwargs)], device=device\n",
    "        )\n",
    "        tokens = t.cat((tokens, token_next), dim=-1)\n",
    "\n",
    "        if tokens[-1] == self.tokenizer.eos_token_id:\n",
    "            break\n",
    "\n",
    "    res = self.tokenizer.decode(tokens)\n",
    "\n",
    "    if verbose:\n",
    "        print(res)\n",
    "\n",
    "    return res\n",
    "\n",
    "\n",
    "TransformerSampler.sample = sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greedy decoding with prompt: 'Jingle bells, jingle bells, jingle all the way'\n",
      "\n",
      "Jingle bells, jingle bells, jingle all the way up to the top of the mountain.\n",
      "Your model said: 'Jingle bells, jingle bells, jingle all the way up to the top of the mountain.'\n",
      "\n",
      "Tests passed!\n"
     ]
    }
   ],
   "source": [
    "sampler = TransformerSampler(model, tokenizer)\n",
    "\n",
    "prompt = \"Jingle bells, jingle bells, jingle all the way\"\n",
    "print(f\"Greedy decoding with prompt: {prompt!r}\\n\")\n",
    "\n",
    "output = sampler.sample(prompt, max_tokens_generated=8, temperature=0.0, verbose=True)\n",
    "print(f\"Your model said: {output!r}\\n\")\n",
    "\n",
    "expected = (\n",
    "    \"Jingle bells, jingle bells, jingle all the way up to the top of the mountain.\"\n",
    ")\n",
    "assert output == expected\n",
    "\n",
    "print(\"Tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling with Categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch provides a `distributions` package containing convenient methods for sampling from various distributions\n",
    "\n",
    "For now, use `t.distributions.categorical.Categorical` to implement `sample_basic`. This just samples from the provided logits, which may have already been modified by the temperature and frequency penalties.\n",
    "\n",
    "Will be slow since we're not batching the samples (yet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Basic Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "@staticmethod\n",
    "def sample_basic(logits: Float[Tensor, \"d_vocab\"]) -> int:\n",
    "    \"\"\"\n",
    "    Samples from the distribution defined by the logits.\n",
    "    \"\"\"\n",
    "    distn = Categorical(logits=logits)\n",
    "    return distn.sample().item()\n",
    "\n",
    "\n",
    "TransformerSampler.sample_basic = sample_basic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b962eeacde964bd3891b4b0561136752",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: ' church'. Expected freq 0.0648, observed freq 0.0622\n",
      "Word: ' house' . Expected freq 0.0367, observed freq 0.0380\n",
      "Word: ' temple'. Expected freq 0.0145, observed freq 0.0140\n",
      "Word: ' same'  . Expected freq 0.0104, observed freq 0.0110\n",
      "Word: ' Church'. Expected freq 0.0097, observed freq 0.0117\n",
      "Tests passed!\n"
     ]
    }
   ],
   "source": [
    "prompt = \"John and Mary went to the\"\n",
    "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "logits = model(input_ids)[0, -1]\n",
    "\n",
    "expected_top_5 = {\n",
    "    \" church\": 0.0648,\n",
    "    \" house\": 0.0367,\n",
    "    \" temple\": 0.0145,\n",
    "    \" same\": 0.0104,\n",
    "    \" Church\": 0.0097,\n",
    "}\n",
    "frequency_of_top_5 = defaultdict(int)\n",
    "\n",
    "N = 10_000\n",
    "for _ in tqdm(range(N)):\n",
    "    token = TransformerSampler.sample_next_token(input_ids.squeeze(), logits)\n",
    "    frequency_of_top_5[tokenizer.decode(token)] += 1\n",
    "\n",
    "for word in expected_top_5:\n",
    "    expected_freq = expected_top_5[word]\n",
    "    observed_freq = frequency_of_top_5[word] / N\n",
    "    print(\n",
    "        f\"Word: {word!r:<9}. Expected freq {expected_freq:.4f}, observed freq {observed_freq:.4f}\"\n",
    "    )\n",
    "    assert (\n",
    "        abs(observed_freq - expected_freq) < 0.01\n",
    "    ), \"Try increasing N if this fails by a small amount.\"\n",
    "\n",
    "print(\"Tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Temperature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***** Implemented above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A low temperature \"sharpens\" or \"peaks\" the distribution:  tensor([  0.0000, 693.1472])\n",
      "A high temperature flattens the distribution:  tensor([0.0000, 0.0007])\n",
      "Tests passed!\n"
     ]
    }
   ],
   "source": [
    "logits = t.tensor([1, 2]).log()\n",
    "\n",
    "cold_logits = TransformerSampler.apply_temperature(logits, temperature=0.001)\n",
    "print('A low temperature \"sharpens\" or \"peaks\" the distribution: ', cold_logits)\n",
    "t.testing.assert_close(cold_logits, 1000.0 * logits)\n",
    "\n",
    "hot_logits = TransformerSampler.apply_temperature(logits, temperature=1000.0)\n",
    "print(\"A high temperature flattens the distribution: \", hot_logits)\n",
    "t.testing.assert_close(hot_logits, 0.001 * logits)\n",
    "\n",
    "print(\"Tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Frequency Penalty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***** Implemented above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests passed!\n"
     ]
    }
   ],
   "source": [
    "bieber_prompt = \"And I was like Baby, baby, baby, oh Like, Baby, baby, baby, no Like, Baby, baby, baby, oh I thought you'd always be mine, mine\"\n",
    "input_ids = tokenizer.encode(bieber_prompt, return_tensors=\"pt\")\n",
    "logits = t.ones(tokenizer.vocab_size)\n",
    "penalized_logits = TransformerSampler.apply_frequency_penalty(\n",
    "    input_ids.squeeze(), logits, 2.0\n",
    ")\n",
    "\n",
    "assert (\n",
    "    penalized_logits[5156].item() == -11\n",
    "), \"Expected 6 occurrences of ' baby' with leading space, 1-2*6=-11\"\n",
    "assert (\n",
    "    penalized_logits[14801].item() == -5\n",
    "), \"Expected 3 occurrences of ' Baby' with leading space, 1-2*3=-5\"\n",
    "\n",
    "print(\"Tests passed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
