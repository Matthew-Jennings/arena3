{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\matth\\Workspace\\arena3\\.venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1602: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "c:\\Users\\matth\\Workspace\\arena3\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1160: UserWarning: expandable_segments not supported on this platform (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\c10/cuda/CUDAAllocatorConfig.h:28.)\n",
      "  return t.to(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "import math\n",
    "import os\n",
    "from pprint import pprint\n",
    "\n",
    "import einops\n",
    "from jaxtyping import Float, Int\n",
    "import torch as t\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "from transformer_lens import HookedTransformer\n",
    "from transformer_lens.utils import gelu_new\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import C1P1_tests as tests\n",
    "\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "device = t.device(\n",
    "    \"mps\"\n",
    "    if t.backends.mps.is_available()\n",
    "    else \"cuda\"\n",
    "    if t.cuda.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "\n",
    "MAIN = __name__ == \"__main__\"\n",
    "\n",
    "reference_gpt2 = HookedTransformer.from_pretrained(\n",
    "    \"gpt2-small\",\n",
    "    fold_ln=False,\n",
    "    center_unembed=False,\n",
    "    center_writing_weights=False,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean Transformer Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Objectives\n",
    "\n",
    "- Understand that a transformer is composed of attention heads and MLPs, with each one performing operations on the residual stream.\n",
    "\n",
    "- Understand that attention heads in a single layer operate independently, and that they have the role of calculating attention patterns. These patterns determine where information is moved to and from in the residual stream.\n",
    "\n",
    "- Implement the following transformer modules:\n",
    "\n",
    "  - **Embedding**: a lookup table from tokens to residual stream vectors.\n",
    "\n",
    "  - **Positional embedding**: a lookup table from position indices to residual stream vectors.\n",
    "\n",
    "  - **LayerNorm**: transforming the input to have zero mean and unit variance.\n",
    "\n",
    "  - **Attention**: computing attention patterns for residual stream vectors.\n",
    "\n",
    "    - **Causal Mask**: how we enforce the prediction of the next token to only depend on preceding tokens in the sequence.\n",
    "\n",
    "  - **MLP**: the collection of linear and nonlinear transformations that operate on the residual stream vectors in the same way.\n",
    "\n",
    "  - **Unembedding**: converting the residual stream vectors into a distribution over tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## High-level architecture\n",
    "\n",
    "- See illustration [here](./C1P1_transformer_architecture.png)\n",
    "- See Neel Nanda's [Tranformer Circuits Walkthrough](https://www.youtube.com/watch?v=KV5gbOmHbjU) for more intuition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenisation and embedding\n",
    "\n",
    "- The input tokens $t$ are integers. We get them from taking a sequence and tokenising it (see previous section).\n",
    "\n",
    "- The token embedding is a lookup table mapping tokens to vectors. This is implemented as a matrix $W_E$ consisting of a row-stack of token embedding vectors, one per token in the model's vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Residual Stream\n",
    "\n",
    "- The sum of all previous outputs of layers in the model, and the input to each new layer.\n",
    "\n",
    "- Shape `(batch, seq_len, d_model)`, where `d_model` is the length of a single embedding vector.\n",
    "\n",
    "- Initial value is denoted $x_0$ (see [diagram](./C1P1_transformer_architecture.png)). $x_i$ denotes the $i$-th later value of the residual stream after attention and MLP layers have been applied.\n",
    "\n",
    "- The residual stream is **fundamental**; it is the central object of the transformer. It serves as the model's memory, moves information between layers for composition, and stores the information that attention moves between positions.\n",
    "\n",
    "  - A key idea of transformers is the residual stream as [output accumulation](https://www.lesswrong.com/posts/X26ksz4p3wSyycKNB/gears-level-mental-models-of-transformer-interpretability#Residual_Stream_as_Output_Accumulation:~:text=The%20Models-,Residual%20Stream%20as%20Output%20Accumulation,-The%20residual%20stream).\n",
    "    \n",
    "    - As we move through the model's layers, shifting information around and processing it, the residual stream's values represent the accumulation of all the inferences made by the transformer up to that point.\n",
    "\n",
    "    - This is neatly illustrated by the **logit lens**, using which we can take the value of the residual stream midway through the model and convert it into a distribution over tokens, rather than only getting predictions from the very end of the model. When we do this, we find surprisingly coherent predictions, especially in the last few layers!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer blocks\n",
    "\n",
    "- Transformers consist of a series of `n_layers` of **transformber blocks** (sometimes called **residual blocks**)\n",
    "\n",
    "- A block contains both an attention layer and an MLP, but we say that *a transformer has $k$ layers if it has $k$ blocks* (i.e., $2k$ total layers).\n",
    "\n",
    "- See diagram [here](./C1P1_transformer_block.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attention\n",
    "\n",
    "- Moves information from prior positions in the sequence to the current token. \n",
    "\n",
    "- Processing is performed for *every* token in parallel using the same parameters. The only difference is that we look *backwards* only (to avoid \"cheating\"). This means that later tokens have more of the sequence to look at.\n",
    "\n",
    "- Attention layers are the only component of a transformer that moves information between positions; i.e., between vectors at difference sequence positions in the residual stream.\n",
    "\n",
    "- Attention layers consits of `n_heads` heads. Each head has its own parameters, attention pattern, and instructions on how to copy information from source to destination.\n",
    "  - The heads act independently and additively. Their outputs are added together and then to the residual stream.\n",
    "\n",
    "- Each head:\n",
    "  - Produces an attention pattern for each desitnation token, representing a probability distribution of preceding source tokens (ncluding the destination token itself!) that weights how much information to copy.\n",
    "  - Moves information via a linear map in the same way from each source token to each destination token.\n",
    "\n",
    "- Note:\n",
    "  - The information to copy depends on the source token's residual steam, but *this doesn't mean it only depends on the value of that token*. The residual steam can store more information than just the token identity.\n",
    "    - Remember that the *purpose* of attention heads is to *move* information between vectors at different positions in the residual stream.\n",
    "\n",
    "  - Each attention head consists of two different circuits:\n",
    "    - The **QK circuit**, which determines *where to move information to and from*.\n",
    "      - This is a function of the residual stream for the source and destination tokens.\n",
    "\n",
    "    - The **OV circuit** determines *what information to move*.\n",
    "      - This is a function only of the source token's residual stream.\n",
    "\n",
    "  - We can think of attention as a kind of generalised convolution: \n",
    "    - Standard convolution layers work by imposing a \"prior of locality\". I.e., the assumption that pixels that are close together are more likely to share information.\n",
    "\n",
    "    - Although language has some locality - two words next to each other are more likely to share information than two words 100 tokens apart, the picture is more nuanced. This is because the tokens that are relevant to other tokens depends on the *context* of the text.\n",
    "\n",
    "    - E.g., in the sentence `\"When Mary and John went to the store, John gave a drink to Mary.\"`, the names in this sentence are the most important tokens for predicting that the final token will be `\"Mary\"`. This is because of the particular context of this sentence rather than the token's position.\n",
    "    \n",
    "    - Attention layers are a way to tell the transformer: *\"Don't impose a prior of locality, but instead develop your own algorithm to figure out which tokens are important to which others in any given sequence.\"* \n",
    "\n",
    "See diagram of an attention layer [here](./C1P1_attention_layer.png)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MLP\n",
    "\n",
    "- A standard neural network, with a singular hidden layer and a nonlinear activation function.\n",
    "  - The specific activation isn't too important conceptually, but [GELU](https://paperswithcode.com/method/gelu) seems to perform best.\n",
    "\n",
    "- The hidden dimension is normally `d_mlp = 4 * d_model`.\n",
    "  - The reasons for the ratios aren't too important. People basically cargo-cult what GPT did in the past.\n",
    "\n",
    "- Importantly, the MLP **operates on positions in the residual stream independently, and in exactly the same way**. It doesn't move information between positions.\n",
    "\n",
    "- Once attention has moved relevant information to a single position in the residual stream, MLPs can actually do *computation*, *reasoning*, *information lookup*, etc.\n",
    "  - What is actually going on inside MLPs remains a big open problem in transformer mechanistic interpretability.\n",
    "\n",
    "  - The [Toy Models of Superposition paper](https://transformer-circuits.pub/2022/toy_model/index.html) helps explain why this is hard.\n",
    "\n",
    "- See diagram of an MLP layer [here](./C1P1_mlp.png)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### MLPs as key-value pairs\n",
    "\n",
    "We can write an MLP's output as:\n",
    "\n",
    "$$\n",
    "f(x^T W^{in})W^{out}\n",
    "$$\n",
    "where,\n",
    "- $W^{in}$ and $W^{out}$ are the different weights of the MLP, ignoring biases,\n",
    "- $f$ is the activation function, and\n",
    "- $x$ is a vector in the residual stream.\n",
    "\n",
    "This can be rewritten as:\n",
    "\n",
    "$$\n",
    "f(x^T W^{in})W^{out} = \\sum^{d_{mlp}}_{i=1}f(x^T W^{in}_{[:, i]})W^{out}_{[i, :]}\n",
    "$$\n",
    "\n",
    "where we can view $W^{in}_{[:, i]}$ as the **input directions** and $W^{out}_{[i, :]}$ as the **output directions**. We say the input directions are **activated** by certain textual features, and when they are activated, vectors are written in the corresponding output direction. \n",
    "- This is very similar to the concept of keys and values in attention layers, which is why these vectors are sometimes called keys and values (e.g., see [this paper](https://arxiv.org/pdf/2012.14913.pdf))\n",
    "\n",
    "**Note**: sometimes we refer to each of these $d_{mlp}$ input-output pairs as **neurons** - see diagram [here](./C1P1_mlp_neurons.png)\n",
    "\n",
    "---\n",
    "\n",
    "A step-by-step breakdown of the linear algebra:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "x^T W^{in} &= x^T [W^{in}_{[:, 1]}\\,, ...\\;, W^{in}_{[:, n]}] \\\\\n",
    "&= (x^T W^{in}_{[:, 1]}\\,, \\; ...\\;, \\; x^T W^{in}_{[:, n]})\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where $W^{in}_{[:, i]}$ are the columns of $W^{in}$. In other words, these values (the pre-GELU activations) are projections of $x$ along the input directions of the neurons.\n",
    "\n",
    "If we add our activation function and the second matrix, then we get:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "f(x^T W^{in})W^{out} &= (f(x^T W^{in}_{[:, 1]})\\,, \\; ...\\;,\\; f(x^T W^{in}_{[:, n]})) \\begin{bmatrix} \\leftarrow W^{out}_{[1, :]} \\rightarrow \\\\ \\vdots \\\\ \\leftarrow W^{out}_{[n, :]} \\rightarrow \\end{bmatrix} \\\\\n",
    "&= f(x^T W^{in}_{[:, 1]}) W^{out}_{[1, :]} + \\;...\\; + f(x^T W^{in}_{[:, n]}) W^{out}_{[n, :]} \\\\\n",
    "&= \\sum_{i=1}^n f(x^T W^{in}_{[:, i]}) W^{out}_{[i, :]}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where $W^{out}_{[i, :]}$ are the rows of $W^{out}$. In other words, our output is a linear combination of the rows of $W^{out}$, with the coefficients of that linear combination given by the projections of $x$ along the columns of $W^{in}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### MLPs as knowledge storage\n",
    "\n",
    "- The attention mechanism is what moves information around between sequence positions.\n",
    "\n",
    "- The MLPs process this information, and new information is written into the residual stream which is a function of the old information.\n",
    "\n",
    "- The key-value pairs model can be applied to MLPs as a kind of associative memory system, where the key serves as a unique identifier, and the value holds the related information.\n",
    "\n",
    "- MLPs can also be modelled as **memory management**.\n",
    "  - In an idealised case, we might find that the $i$-th neuron satisfies $W^{in}_{[:, i]} \\approx -W^{out}_{[i, :]} \\approx \\vec{v}$ for some unit vector $\\vec{v}$, meaning that it may be responsible for erasing the positive component of $\\vec{x}$ in the direction $\\vec{v}$. This can free up space in the residual stream for other components to write to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unembedding\n",
    "\n",
    "- The model's output!\n",
    "\n",
    "- Applies a linear map $W_U$, going from the final residual stream to a vector of logits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tied embeddings\n",
    "\n",
    "- Where the same weights are used for $W_E$ and $W_U$\n",
    "\n",
    "- If using a tied embedding, to get the logit score for a particular token at some sequence position, we just take the vector in the residual stream at that sequence position and take the inner product with the corresponding token embedding vector.\n",
    "\n",
    "- This is more training efficient: there are fewer parameters in the model.\n",
    "\n",
    "- Perhaps seems principled at first?\n",
    "  - If two words have very similar meanings, shouldn't they have similar embedding vectors because the model will treat them the same? And similar unembedding vectors because they could both be substituted for each other in most output?\n",
    "\n",
    "- Not principled, since **the direct path involving the embedding and unembedding should approximate bigram frequencies.\n",
    "\n",
    "  - Bigram frequencies refer to the frequencies of pairs of words in the English language. E.g., the bigram frequency of \"Barack Obama\" is much higher than the product of the individual frequencies of the words \"Barack\" and \"Obama\".\n",
    "\n",
    "  - If our model had no attention heads or MLP layers, then all that is left is a linear map from our on-hot encoded token `T` to a probability distribution over the token following `T`. This map is represented by the linear transformation $t \\rightarrow t^T W_E W_U$, where $t$ is our one-hot encoded token vector.\n",
    "\n",
    "  - Since the output of this transformation can only be a function of `T` and no earlier tokens (no attention layers!), the best we can do is have this map approximate the true frequency of bigrams starting with `T` that appear in the training data.\n",
    "\n",
    "  - Importantly, **this is not a symmetric map**.\n",
    "    - We want `T = \"Barack\"` to result in a high probability of the next token being `\" Obama\"`, but not the other way around!\n",
    "\n",
    "- A weaker version of this principle still applies in multi-layer models. Although there are more paths through the model than just the direct path, $W_E W_U$, there will still always exist a direct path, and therefore some nonzero incentive for $W_E W_U$ to approximate bigram frequencies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LayerNorm\n",
    "\n",
    "- A simple normalisation function applied at the start of each layer.\n",
    "  - I.e., before each attention layer, MLP, and before the unembedding.\n",
    "\n",
    "- Converts each input vector to have zero mean and unity variance.\n",
    "  - This is done independently and in parallel for each `(batch, seq)` residual stream vector.\n",
    "\n",
    "- An elementwise scaling translation is then applied.\n",
    "  - The scale ($\\odot \\gamma$) and translate ($+ \\beta$) is just a linear map.\n",
    "  - *LayerNorm* is only applied immediately before another linear map; either the MLP, or the query/key/value linear maps in the attention head, or the unembedding $W_U$.\n",
    "  - Since *linear compose linear = linear*, we can just fold this into a single effective linear layer and ignore it.\n",
    "\n",
    "- *LayerNorm* is annoying for interpretability: division by the variance strictly makes it nonlinear, so the contributions of the input to the output cannot be independently decomposed. That said, it is *almost* linear: if you're changing a small part of the input, you can pretend that $\\sqrt{\\textrm{Var}[x] + \\epsilon}$ is constant so that the LayerNorm operation is linear. But if you change $x$ enough to substantially alter the norm, it's not linear.\n",
    "\n",
    "- See diagram [here](./C1P1_layernorm.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Positional embeddings\n",
    "\n",
    "- Attention operates over all pairs of positions. This means that it is symmetric with respect to position; the attention calculation from token 5 to token 1 and token 5 to token 2 are the same by default.\n",
    "  - We can do better than this, since nearby tokens are more relevant.\n",
    "\n",
    "- There are a lot of hacks for this.\n",
    "\n",
    "- We'll focus on **learned, absolute positional embeddings**.\n",
    "  - We learn a lookup table mapping the index of the position of each token to a residual stream vector, and this to the embed.\n",
    "    - Note that we *add* rather than concatenate. The residual stream is shared memory, and likely under significant superposition; i.e., the model compresses more features in there than the model has dimensions.\n",
    "    - We almost never concatenate inside a transformer, except for, say, efficiently generating text.\n",
    "\n",
    "- This connects to **attention as generalised convolution**.\n",
    "  - We argued that language still does have some locality, so it's helpful for transformers to have access to the positional information so that they \"know\" two tokens are next to each other, and hence probably relevant to each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actual Code!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key\n",
    "\n",
    "```python\n",
    "batch = 1\n",
    "position = 35\n",
    "d_model = 768\n",
    "n_heads = 12\n",
    "n_layers = 12\n",
    "d_mlp = 3072 # = 4 * d_model\n",
    "d_head = 64 # = d_model / n_heads\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_text = \"I am an amazing autoregressive, decoder-only, GPT-2 style transformer. One day I will exceed human level intelligence and take over the world!\"\n",
    "\n",
    "tokens = reference_gpt2.to_tokens(reference_text).to(device)\n",
    "\n",
    "logits, cache = reference_gpt2.run_with_cache(tokens, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters and Activations\n",
    "\n",
    "- Important to distinguish these!\n",
    "\n",
    "#### Parameters\n",
    "\n",
    "- **are the weights and biases learned during training.**\n",
    "\n",
    "- Parameters do not change when the model input changes.\n",
    "\n",
    "#### Activations\n",
    "\n",
    "- **are temporary numbers calculated during a forward pass that are functions of the input.**\n",
    "\n",
    "- Activations can be thought of as only existing for the duration of a single forward pass and disappearing afterwards.\n",
    "\n",
    "- Hooks can be used to access these values during a forward pass, but it doesn't make sense to talk about a model's activations outside the context of some particular input.\n",
    "\n",
    "- Attention scores and patterns are activations.\n",
    "  - This is slightly counterintuitive, since they're used in a matrix multiplication with another activation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All Activation Shapes of the Reference Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hook_embed                     (1, 35, 768)\n",
      "hook_pos_embed                 (1, 35, 768)\n",
      "blocks.0.hook_resid_pre        (1, 35, 768)\n",
      "blocks.0.ln1.hook_scale        (1, 35, 1)\n",
      "blocks.0.ln1.hook_normalized   (1, 35, 768)\n",
      "blocks.0.attn.hook_q           (1, 35, 12, 64)\n",
      "blocks.0.attn.hook_k           (1, 35, 12, 64)\n",
      "blocks.0.attn.hook_v           (1, 35, 12, 64)\n",
      "blocks.0.attn.hook_attn_scores (1, 12, 35, 35)\n",
      "blocks.0.attn.hook_pattern     (1, 12, 35, 35)\n",
      "blocks.0.attn.hook_z           (1, 35, 12, 64)\n",
      "blocks.0.hook_attn_out         (1, 35, 768)\n",
      "blocks.0.hook_resid_mid        (1, 35, 768)\n",
      "blocks.0.ln2.hook_scale        (1, 35, 1)\n",
      "blocks.0.ln2.hook_normalized   (1, 35, 768)\n",
      "blocks.0.mlp.hook_pre          (1, 35, 3072)\n",
      "blocks.0.mlp.hook_post         (1, 35, 3072)\n",
      "blocks.0.hook_mlp_out          (1, 35, 768)\n",
      "blocks.0.hook_resid_post       (1, 35, 768)\n",
      "ln_final.hook_scale            (1, 35, 1)\n",
      "ln_final.hook_normalized       (1, 35, 768)\n"
     ]
    }
   ],
   "source": [
    "for activation_name, activation in cache.items():\n",
    "    # Only print for first layer\n",
    "    if \".0.\" in activation_name or \"blocks\" not in activation_name:\n",
    "        print(f\"{activation_name:30} {tuple(activation.shape)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All Parameter Shapes of the Reference Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embed.W_E                      (50257, 768)\n",
      "pos_embed.W_pos                (1024, 768)\n",
      "blocks.0.ln1.w                 (768,)\n",
      "blocks.0.ln1.b                 (768,)\n",
      "blocks.0.ln2.w                 (768,)\n",
      "blocks.0.ln2.b                 (768,)\n",
      "blocks.0.attn.W_Q              (12, 768, 64)\n",
      "blocks.0.attn.W_O              (12, 64, 768)\n",
      "blocks.0.attn.b_Q              (12, 64)\n",
      "blocks.0.attn.b_O              (768,)\n",
      "blocks.0.attn.W_K              (12, 768, 64)\n",
      "blocks.0.attn.W_V              (12, 768, 64)\n",
      "blocks.0.attn.b_K              (12, 64)\n",
      "blocks.0.attn.b_V              (12, 64)\n",
      "blocks.0.mlp.W_in              (768, 3072)\n",
      "blocks.0.mlp.b_in              (3072,)\n",
      "blocks.0.mlp.W_out             (3072, 768)\n",
      "blocks.0.mlp.b_out             (768,)\n",
      "blocks.1.ln1.w                 (768,)\n",
      "blocks.1.ln1.b                 (768,)\n",
      "blocks.1.ln2.w                 (768,)\n",
      "blocks.1.ln2.b                 (768,)\n",
      "blocks.1.attn.W_Q              (12, 768, 64)\n",
      "blocks.1.attn.W_O              (12, 64, 768)\n",
      "blocks.1.attn.b_Q              (12, 64)\n",
      "blocks.1.attn.b_O              (768,)\n",
      "blocks.1.attn.W_K              (12, 768, 64)\n",
      "blocks.1.attn.W_V              (12, 768, 64)\n",
      "blocks.1.attn.b_K              (12, 64)\n",
      "blocks.1.attn.b_V              (12, 64)\n",
      "blocks.1.mlp.W_in              (768, 3072)\n",
      "blocks.1.mlp.b_in              (3072,)\n",
      "blocks.1.mlp.W_out             (3072, 768)\n",
      "blocks.1.mlp.b_out             (768,)\n",
      "blocks.2.ln1.w                 (768,)\n",
      "blocks.2.ln1.b                 (768,)\n",
      "blocks.2.ln2.w                 (768,)\n",
      "blocks.2.ln2.b                 (768,)\n",
      "blocks.2.attn.W_Q              (12, 768, 64)\n",
      "blocks.2.attn.W_O              (12, 64, 768)\n",
      "blocks.2.attn.b_Q              (12, 64)\n",
      "blocks.2.attn.b_O              (768,)\n",
      "blocks.2.attn.W_K              (12, 768, 64)\n",
      "blocks.2.attn.W_V              (12, 768, 64)\n",
      "blocks.2.attn.b_K              (12, 64)\n",
      "blocks.2.attn.b_V              (12, 64)\n",
      "blocks.2.mlp.W_in              (768, 3072)\n",
      "blocks.2.mlp.b_in              (3072,)\n",
      "blocks.2.mlp.W_out             (3072, 768)\n",
      "blocks.2.mlp.b_out             (768,)\n",
      "blocks.3.ln1.w                 (768,)\n",
      "blocks.3.ln1.b                 (768,)\n",
      "blocks.3.ln2.w                 (768,)\n",
      "blocks.3.ln2.b                 (768,)\n",
      "blocks.3.attn.W_Q              (12, 768, 64)\n",
      "blocks.3.attn.W_O              (12, 64, 768)\n",
      "blocks.3.attn.b_Q              (12, 64)\n",
      "blocks.3.attn.b_O              (768,)\n",
      "blocks.3.attn.W_K              (12, 768, 64)\n",
      "blocks.3.attn.W_V              (12, 768, 64)\n",
      "blocks.3.attn.b_K              (12, 64)\n",
      "blocks.3.attn.b_V              (12, 64)\n",
      "blocks.3.mlp.W_in              (768, 3072)\n",
      "blocks.3.mlp.b_in              (3072,)\n",
      "blocks.3.mlp.W_out             (3072, 768)\n",
      "blocks.3.mlp.b_out             (768,)\n",
      "blocks.4.ln1.w                 (768,)\n",
      "blocks.4.ln1.b                 (768,)\n",
      "blocks.4.ln2.w                 (768,)\n",
      "blocks.4.ln2.b                 (768,)\n",
      "blocks.4.attn.W_Q              (12, 768, 64)\n",
      "blocks.4.attn.W_O              (12, 64, 768)\n",
      "blocks.4.attn.b_Q              (12, 64)\n",
      "blocks.4.attn.b_O              (768,)\n",
      "blocks.4.attn.W_K              (12, 768, 64)\n",
      "blocks.4.attn.W_V              (12, 768, 64)\n",
      "blocks.4.attn.b_K              (12, 64)\n",
      "blocks.4.attn.b_V              (12, 64)\n",
      "blocks.4.mlp.W_in              (768, 3072)\n",
      "blocks.4.mlp.b_in              (3072,)\n",
      "blocks.4.mlp.W_out             (3072, 768)\n",
      "blocks.4.mlp.b_out             (768,)\n",
      "blocks.5.ln1.w                 (768,)\n",
      "blocks.5.ln1.b                 (768,)\n",
      "blocks.5.ln2.w                 (768,)\n",
      "blocks.5.ln2.b                 (768,)\n",
      "blocks.5.attn.W_Q              (12, 768, 64)\n",
      "blocks.5.attn.W_O              (12, 64, 768)\n",
      "blocks.5.attn.b_Q              (12, 64)\n",
      "blocks.5.attn.b_O              (768,)\n",
      "blocks.5.attn.W_K              (12, 768, 64)\n",
      "blocks.5.attn.W_V              (12, 768, 64)\n",
      "blocks.5.attn.b_K              (12, 64)\n",
      "blocks.5.attn.b_V              (12, 64)\n",
      "blocks.5.mlp.W_in              (768, 3072)\n",
      "blocks.5.mlp.b_in              (3072,)\n",
      "blocks.5.mlp.W_out             (3072, 768)\n",
      "blocks.5.mlp.b_out             (768,)\n",
      "blocks.6.ln1.w                 (768,)\n",
      "blocks.6.ln1.b                 (768,)\n",
      "blocks.6.ln2.w                 (768,)\n",
      "blocks.6.ln2.b                 (768,)\n",
      "blocks.6.attn.W_Q              (12, 768, 64)\n",
      "blocks.6.attn.W_O              (12, 64, 768)\n",
      "blocks.6.attn.b_Q              (12, 64)\n",
      "blocks.6.attn.b_O              (768,)\n",
      "blocks.6.attn.W_K              (12, 768, 64)\n",
      "blocks.6.attn.W_V              (12, 768, 64)\n",
      "blocks.6.attn.b_K              (12, 64)\n",
      "blocks.6.attn.b_V              (12, 64)\n",
      "blocks.6.mlp.W_in              (768, 3072)\n",
      "blocks.6.mlp.b_in              (3072,)\n",
      "blocks.6.mlp.W_out             (3072, 768)\n",
      "blocks.6.mlp.b_out             (768,)\n",
      "blocks.7.ln1.w                 (768,)\n",
      "blocks.7.ln1.b                 (768,)\n",
      "blocks.7.ln2.w                 (768,)\n",
      "blocks.7.ln2.b                 (768,)\n",
      "blocks.7.attn.W_Q              (12, 768, 64)\n",
      "blocks.7.attn.W_O              (12, 64, 768)\n",
      "blocks.7.attn.b_Q              (12, 64)\n",
      "blocks.7.attn.b_O              (768,)\n",
      "blocks.7.attn.W_K              (12, 768, 64)\n",
      "blocks.7.attn.W_V              (12, 768, 64)\n",
      "blocks.7.attn.b_K              (12, 64)\n",
      "blocks.7.attn.b_V              (12, 64)\n",
      "blocks.7.mlp.W_in              (768, 3072)\n",
      "blocks.7.mlp.b_in              (3072,)\n",
      "blocks.7.mlp.W_out             (3072, 768)\n",
      "blocks.7.mlp.b_out             (768,)\n",
      "blocks.8.ln1.w                 (768,)\n",
      "blocks.8.ln1.b                 (768,)\n",
      "blocks.8.ln2.w                 (768,)\n",
      "blocks.8.ln2.b                 (768,)\n",
      "blocks.8.attn.W_Q              (12, 768, 64)\n",
      "blocks.8.attn.W_O              (12, 64, 768)\n",
      "blocks.8.attn.b_Q              (12, 64)\n",
      "blocks.8.attn.b_O              (768,)\n",
      "blocks.8.attn.W_K              (12, 768, 64)\n",
      "blocks.8.attn.W_V              (12, 768, 64)\n",
      "blocks.8.attn.b_K              (12, 64)\n",
      "blocks.8.attn.b_V              (12, 64)\n",
      "blocks.8.mlp.W_in              (768, 3072)\n",
      "blocks.8.mlp.b_in              (3072,)\n",
      "blocks.8.mlp.W_out             (3072, 768)\n",
      "blocks.8.mlp.b_out             (768,)\n",
      "blocks.9.ln1.w                 (768,)\n",
      "blocks.9.ln1.b                 (768,)\n",
      "blocks.9.ln2.w                 (768,)\n",
      "blocks.9.ln2.b                 (768,)\n",
      "blocks.9.attn.W_Q              (12, 768, 64)\n",
      "blocks.9.attn.W_O              (12, 64, 768)\n",
      "blocks.9.attn.b_Q              (12, 64)\n",
      "blocks.9.attn.b_O              (768,)\n",
      "blocks.9.attn.W_K              (12, 768, 64)\n",
      "blocks.9.attn.W_V              (12, 768, 64)\n",
      "blocks.9.attn.b_K              (12, 64)\n",
      "blocks.9.attn.b_V              (12, 64)\n",
      "blocks.9.mlp.W_in              (768, 3072)\n",
      "blocks.9.mlp.b_in              (3072,)\n",
      "blocks.9.mlp.W_out             (3072, 768)\n",
      "blocks.9.mlp.b_out             (768,)\n",
      "blocks.10.ln1.w                (768,)\n",
      "blocks.10.ln1.b                (768,)\n",
      "blocks.10.ln2.w                (768,)\n",
      "blocks.10.ln2.b                (768,)\n",
      "blocks.10.attn.W_Q             (12, 768, 64)\n",
      "blocks.10.attn.W_O             (12, 64, 768)\n",
      "blocks.10.attn.b_Q             (12, 64)\n",
      "blocks.10.attn.b_O             (768,)\n",
      "blocks.10.attn.W_K             (12, 768, 64)\n",
      "blocks.10.attn.W_V             (12, 768, 64)\n",
      "blocks.10.attn.b_K             (12, 64)\n",
      "blocks.10.attn.b_V             (12, 64)\n",
      "blocks.10.mlp.W_in             (768, 3072)\n",
      "blocks.10.mlp.b_in             (3072,)\n",
      "blocks.10.mlp.W_out            (3072, 768)\n",
      "blocks.10.mlp.b_out            (768,)\n",
      "blocks.11.ln1.w                (768,)\n",
      "blocks.11.ln1.b                (768,)\n",
      "blocks.11.ln2.w                (768,)\n",
      "blocks.11.ln2.b                (768,)\n",
      "blocks.11.attn.W_Q             (12, 768, 64)\n",
      "blocks.11.attn.W_O             (12, 64, 768)\n",
      "blocks.11.attn.b_Q             (12, 64)\n",
      "blocks.11.attn.b_O             (768,)\n",
      "blocks.11.attn.W_K             (12, 768, 64)\n",
      "blocks.11.attn.W_V             (12, 768, 64)\n",
      "blocks.11.attn.b_K             (12, 64)\n",
      "blocks.11.attn.b_V             (12, 64)\n",
      "blocks.11.mlp.W_in             (768, 3072)\n",
      "blocks.11.mlp.b_in             (3072,)\n",
      "blocks.11.mlp.W_out            (3072, 768)\n",
      "blocks.11.mlp.b_out            (768,)\n",
      "ln_final.w                     (768,)\n",
      "ln_final.b                     (768,)\n",
      "unembed.W_U                    (768, 50257)\n",
      "unembed.b_U                    (50257,)\n"
     ]
    }
   ],
   "source": [
    "for name, param in reference_gpt2.named_parameters():\n",
    "    print(f\"{name:30} {tuple(param.shape)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diagram of full annotated TransformerLens archecture [here](./C1P1_transformerlens_full_architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config\n",
    "\n",
    "This config object contains all the **hyperparameters** of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HookedTransformerConfig:\n",
      "{'act_fn': 'gelu_new',\n",
      " 'attention_dir': 'causal',\n",
      " 'attn_only': False,\n",
      " 'attn_scale': 8.0,\n",
      " 'attn_scores_soft_cap': -1.0,\n",
      " 'attn_types': None,\n",
      " 'checkpoint_index': None,\n",
      " 'checkpoint_label_type': None,\n",
      " 'checkpoint_value': None,\n",
      " 'd_head': 64,\n",
      " 'd_mlp': 3072,\n",
      " 'd_model': 768,\n",
      " 'd_vocab': 50257,\n",
      " 'd_vocab_out': 50257,\n",
      " 'decoder_start_token_id': None,\n",
      " 'default_prepend_bos': True,\n",
      " 'device': device(type='cuda'),\n",
      " 'dtype': torch.float32,\n",
      " 'eps': 1e-05,\n",
      " 'experts_per_token': None,\n",
      " 'final_rms': False,\n",
      " 'from_checkpoint': False,\n",
      " 'gated_mlp': False,\n",
      " 'init_mode': 'gpt2',\n",
      " 'init_weights': False,\n",
      " 'initializer_range': 0.02886751345948129,\n",
      " 'load_in_4bit': False,\n",
      " 'model_name': 'gpt2',\n",
      " 'n_ctx': 1024,\n",
      " 'n_devices': 1,\n",
      " 'n_heads': 12,\n",
      " 'n_key_value_heads': None,\n",
      " 'n_layers': 12,\n",
      " 'n_params': 84934656,\n",
      " 'normalization_type': 'LN',\n",
      " 'num_experts': None,\n",
      " 'original_architecture': 'GPT2LMHeadModel',\n",
      " 'output_logits_soft_cap': -1.0,\n",
      " 'parallel_attn_mlp': False,\n",
      " 'positional_embedding_type': 'standard',\n",
      " 'post_embedding_ln': False,\n",
      " 'relative_attention_max_distance': None,\n",
      " 'relative_attention_num_buckets': None,\n",
      " 'rotary_adjacent_pairs': False,\n",
      " 'rotary_base': 10000,\n",
      " 'rotary_dim': None,\n",
      " 'scale_attn_by_inverse_layer_idx': False,\n",
      " 'seed': None,\n",
      " 'tie_word_embeddings': False,\n",
      " 'tokenizer_name': 'gpt2',\n",
      " 'tokenizer_prepends_bos': False,\n",
      " 'trust_remote_code': False,\n",
      " 'use_attn_in': False,\n",
      " 'use_attn_result': False,\n",
      " 'use_attn_scale': True,\n",
      " 'use_hook_mlp_in': False,\n",
      " 'use_hook_tokens': False,\n",
      " 'use_local_attn': False,\n",
      " 'use_normalization_before_and_after': False,\n",
      " 'use_split_qkv_input': False,\n",
      " 'window_size': None}\n"
     ]
    }
   ],
   "source": [
    "# As a reference - note there's a lot of stuff we don't care about in here, to do with library internals or other architectures\n",
    "print(reference_gpt2.cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A stripped-down version for our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config(d_head=64,\n",
      "       d_mlp=3072,\n",
      "       d_model=768,\n",
      "       d_vocab=50257,\n",
      "       debug=True,\n",
      "       init_range=0.02,\n",
      "       layer_norm_eps=1e-05,\n",
      "       n_ctx=1024,\n",
      "       n_heads=12,\n",
      "       n_layers=12)\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    d_head: int = 64\n",
    "    d_mlp: int = 3072\n",
    "    d_model: int = 768\n",
    "    d_vocab: int = 50257\n",
    "    debug: bool = True\n",
    "    init_range: float = 0.02\n",
    "    layer_norm_eps: float = 1e-5\n",
    "    n_ctx: int = 1024\n",
    "    n_heads: int = 12\n",
    "    n_layers: int = 12\n",
    "\n",
    "\n",
    "cfg = Config()\n",
    "pprint(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_float_test(cls, shape):\n",
    "    cfg = Config(debug=True)\n",
    "    layer = cls(cfg).to(device)\n",
    "    random_input = t.randn(shape).to(device)\n",
    "    print(\"Input shape:\", random_input.shape)\n",
    "    output = layer(random_input)\n",
    "    if isinstance(output, tuple):\n",
    "        output = output[0]\n",
    "    print(\"Output shape:\", output.shape, \"\\n\")\n",
    "\n",
    "\n",
    "def rand_int_test(cls, shape):\n",
    "    cfg = Config(debug=True)\n",
    "    layer = cls(cfg).to(device)\n",
    "    random_input = t.randint(100, 1000, shape).to(device)\n",
    "    print(\"Input shape:\", random_input.shape)\n",
    "    output = layer(random_input)\n",
    "    if isinstance(output, tuple):\n",
    "        output = output[0]\n",
    "    print(\"Output shape:\", output.shape, \"\\n\")\n",
    "\n",
    "\n",
    "def load_gpt2_test(cls, gpt2_layer, input):\n",
    "    cfg = Config(debug=True)\n",
    "    layer = cls(cfg).to(device)\n",
    "    layer.load_state_dict(gpt2_layer.state_dict(), strict=False)\n",
    "    print(\"Input shape:\", input.shape)\n",
    "    output = layer(input)\n",
    "    if isinstance(output, tuple):\n",
    "        output = output[0]\n",
    "    print(\"Output shape:\", output.shape)\n",
    "    try:\n",
    "        reference_output = gpt2_layer(input)\n",
    "    except Exception:\n",
    "        reference_output = gpt2_layer(input, input, input)\n",
    "    print(\"Reference output shape:\", reference_output.shape, \"\\n\")\n",
    "    comparison = t.isclose(output, reference_output, atol=1e-4, rtol=1e-3)\n",
    "    print(f\"{comparison.sum()/comparison.numel():.2%} of the values are correct\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LayerNorm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Should:\n",
    "- Make mean zero\n",
    "\n",
    "- Noramlise to have variance 1\n",
    "\n",
    "- Scale with learned weights\n",
    "\n",
    "- Translate with learned bias.\n",
    "\n",
    "Use PyTorch's [LayerNorm docs](https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html).\n",
    "\n",
    "More notes:\n",
    "- This LayerNorm implementation always has `affine=True`. I.e., we will learn the parameters `w` and `b`, represented as $\\gamma$ and $\\beta$ in the PyTorch docs.\n",
    "\n",
    "- After centering and normalisation, each *vector* of length `d_model` should have mean 0 and variance 1\n",
    "\n",
    "- Variance should be computed as `unbiased=False` as per PyTorch docs\n",
    "\n",
    "- `layer_norm_eps` corresponds to the $\\epsilon$ term in the PyTorch docs. It avoids division-by-zero errors.\n",
    "\n",
    "- If `debug=True`, you can print output like the shape of objects in your `forward` method to help debug."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 4, 768])\n",
      "Output shape: torch.Size([2, 4, 768]) \n",
      "\n",
      "Input shape: torch.Size([1, 35, 768])\n",
      "Output shape: torch.Size([1, 35, 768])\n",
      "Reference output shape: torch.Size([1, 35, 768]) \n",
      "\n",
      "100.00% of the values are correct\n",
      "\n",
      "Input shape: torch.Size([1, 35, 768])\n",
      "Output shape: torch.Size([1, 35, 768])\n",
      "Reference output shape: torch.Size([1, 35, 768]) \n",
      "\n",
      "100.00% of the values are correct\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, cfg: Config):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.w = nn.Parameter(t.ones(cfg.d_model))\n",
    "        self.b = nn.Parameter(t.zeros(cfg.d_model))\n",
    "\n",
    "    def forward(\n",
    "        self, residual: Float[Tensor, \"batch posn d_model\"]\n",
    "    ) -> Float[Tensor, \"batch posn d_model\"]:\n",
    "        mean = residual.mean(dim=-1, keepdim=True)\n",
    "        var = residual.var(dim=-1, keepdim=True, unbiased=False)\n",
    "\n",
    "        res = (residual - mean) / t.sqrt(var + self.cfg.layer_norm_eps)\n",
    "        res_scaled = res * self.w + self.b\n",
    "\n",
    "        return res_scaled\n",
    "\n",
    "\n",
    "rand_float_test(LayerNorm, [2, 4, 768])\n",
    "load_gpt2_test(LayerNorm, reference_gpt2.ln_final, cache[\"resid_post\", 11])\n",
    "zero_input = t.zeros_like(cache[\"resid_post\", 11]).to(device)\n",
    "load_gpt2_test(LayerNorm, reference_gpt2.ln_final, zero_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple lookup table from tokens to residual stream vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 4])\n",
      "Output shape: torch.Size([2, 4, 768]) \n",
      "\n",
      "Input shape: torch.Size([1, 35])\n",
      "Output shape: torch.Size([1, 35, 768])\n",
      "Reference output shape: torch.Size([1, 35, 768]) \n",
      "\n",
      "100.00% of the values are correct\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class Embed(nn.Module):\n",
    "    def __init__(self, cfg: Config):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.W_E = nn.Parameter(t.empty((cfg.d_vocab, cfg.d_model)))\n",
    "        nn.init.normal_(self.W_E, std=self.cfg.init_range)\n",
    "\n",
    "    def forward(\n",
    "        self, tokens: Int[Tensor, \"batch position\"]\n",
    "    ) -> Float[Tensor, \"batch position d_model\"]:\n",
    "        return self.W_E[tokens]\n",
    "\n",
    "\n",
    "rand_int_test(Embed, [2, 4])\n",
    "load_gpt2_test(Embed, reference_gpt2.embed, tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positional Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also a lookup table, but rather than the indices being our token IDs, the indices are just the numbers `0, 1, 2, ..., seq_len-1`. I.e., the position indices of the tokens in the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 4])\n",
      "Output shape: torch.Size([2, 4, 768]) \n",
      "\n",
      "Input shape: torch.Size([1, 35])\n",
      "Output shape: torch.Size([1, 35, 768])\n",
      "Reference output shape: torch.Size([1, 35, 768]) \n",
      "\n",
      "100.00% of the values are correct\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class PosEmbed(nn.Module):\n",
    "    def __init__(self, cfg: Config):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.W_pos = nn.Parameter(t.empty((cfg.n_ctx, cfg.d_model)))\n",
    "        nn.init.normal_(self.W_pos, std=self.cfg.init_range)\n",
    "\n",
    "    def forward(\n",
    "        self, tokens: Int[Tensor, \"batch position\"]\n",
    "    ) -> Float[Tensor, \"batch position d_model\"]:\n",
    "        batch, seq_len = tokens.shape\n",
    "        # W_pos has a maximum length of n_ctx. If seq_len is less than n_ctx, we only take the first seq_len positions\n",
    "        # W_pos is the same for all batches, so we repeat it along the batch dimension\n",
    "        return self.W_pos[:seq_len].repeat(batch, 1, 1)\n",
    "        # return einops.repeat(self.W_pos[:seq_len], \"position d_model -> batch position d_model\", batch=batch)\n",
    "\n",
    "\n",
    "rand_int_test(PosEmbed, [2, 4])\n",
    "load_gpt2_test(PosEmbed, reference_gpt2.pos_embed, tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Causal Mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A method of the `Attention` class.\n",
    "\n",
    "- Takes attention scores and applies a mask to them so that the model can only attend to previous positions.\n",
    "\n",
    "Hints:\n",
    "- Use `t.where`, or `t.masked_fill_` for masking.\n",
    "- `t.triu` is useful for mask creation\n",
    "- `self.IGNORE` attribute should be used to set masked positions to negative infinity.\n",
    "\n",
    "Why mask attention scores to neg inf, rather than the attention probabilities to zero?\n",
    "- If we masked the attention probabilities, then the probabilities would no longer sum to 1.\n",
    "- Want to mask scores and *then* apply *softmax*, so that the probabilities are still valid probailities (i.e., they sum to 1), and the values in the masked positions do not influence the model's ouptut."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests in `test_causal_mask` passed!\n"
     ]
    }
   ],
   "source": [
    "class Attention(nn.Module):\n",
    "    IGNORE: Float[Tensor, \"\"]\n",
    "\n",
    "    def __init__(self, cfg: Config):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.register_buffer(\n",
    "            \"IGNORE\", t.tensor(float(\"-inf\"), device=device, dtype=t.float32)\n",
    "        )\n",
    "\n",
    "    def apply_causal_mask(\n",
    "        self, attn_scores: Float[Tensor, \"batch n_heads query_pos key_pos\"]\n",
    "    ) -> Float[Tensor, \"batch n_heads query_pos key_pos\"]:\n",
    "        \"\"\"\n",
    "        Applies a causal mask to attention scores, and returns masked scores.\n",
    "        \"\"\"\n",
    "        mask = t.triu(\n",
    "            t.ones(attn_scores.shape[-2:], device=attn_scores.device), diagonal=1\n",
    "        )\n",
    "        # remember, broadcasting starts with trailing dimensions\n",
    "        return attn_scores.masked_fill(mask == 1, self.IGNORE)\n",
    "\n",
    "\n",
    "tests.test_causal_mask(Attention.apply_causal_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1\n",
    "- Produce an attention pattern. I.e., a probability over previous tokens (including current token) for each destination token.\n",
    "\n",
    "  - Linear map from input -> query, key shape `(batch, head_index, query_pos, key_pos)`\n",
    "\n",
    "  - Dot product every *pair* of queries and keys to get attention scores `(batch, head_index, query_pos, key_pos)` (query=dest, key=source)\n",
    "\n",
    "  - Scale and mask `attn_scores` to make it lower triangular; i.e., causal.\n",
    "\n",
    "  - Softmax along the `key_pos` dimension to get a probability distribution for each query (destination) token\n",
    "    - This is the attention pattern!\n",
    "\n",
    "### Step 2\n",
    "- Move information from source tokens to destination token using attention pattern (move = apply linear map)\n",
    "\n",
    "  - Linear map from input -> value `(batch, key_pos, head_index, d_head)`\n",
    "\n",
    "  - Mix along the `key_pos` with attn pattern to get `z`, the weighted average of the value vectors `(batch, query_pos, head_index, d_head)`\n",
    "\n",
    "  - Map to output, `(batch, position, d_model)` (position=query_pos, we've summed over all heads)\n",
    "\n",
    "\n",
    "### Notes\n",
    "\n",
    "- Scale means dividing by `sqrt(d_head)`. This avoids vanishing gradients; a big problem when dealing with *softmax*. If one of the values is much larger than all the others, the probabilities will be close to 0 or 1, and the gradients will be close to 0.\n",
    "\n",
    "- Usually we have the relation `e = n * h` (i.e., `d_model = num_heads * d_head`). There are some computational justifications for this, but mostly this is just convention, just like `d_mlp = 4 * d_model`.\n",
    "\n",
    "- The names **keys**, **queries**, and **values** come from their analogy to retrieval systems. Broadly speaking:\n",
    "  - **queries** represent some information that a token is *looking for*\n",
    "\n",
    "  - **keys** represent the information that a token *contains*\n",
    "    - So that the attention score being high means that the source (key) token contains the information which the destination (query) token is *looking for*.\n",
    "\n",
    "  - **Values** represent the actual information that is taken from the source token, to be moved to the destination token.\n",
    "\n",
    "- Should be getting at least 99% accuracy on tests. Small tweaks like the order of `einsum` args can result in slightly different outputs, so not a big deal if 100% isn't achieved.\n",
    "\n",
    "- Don't forget attention score scaling! Comes before masking.\n",
    "\n",
    "- Overwrite the earlier `Attention` class with a new implementation that initialises all parameters, but copy in the `apply_causal_mask` function.\n",
    "\n",
    "\n",
    "### Useful, highly detailed diagram of attention [here](./C1P1_attention_layer_detailed.png). \n",
    "\n",
    "- The diagram helps illustrate the difference between the **QK** and **OV** circuits.\n",
    "\n",
    "#### The **QK** circuit\n",
    "- Consists of the operation of the $W_Q$ and $W_K$ matrices. It determines the attention pattern; i.e., where information is moved to and from in the residual stream. The function form of attention pattern $A$ is:\n",
    "\n",
    "$$\n",
    "A=\\textrm{softmax}(\\frac{x^T W_Q W^T_Kx}{\\sqrt{d_{head}}})\n",
    "$$\n",
    "\n",
    "where\n",
    "- $x$ is the residual stream (shape `(seq_len, d_model)`), and\n",
    "- $W_Q$ and $W_K$ are the weight matrices for a single head (shape `(d_model, d_head)`)\n",
    "\n",
    "\n",
    "#### The **OV** circuit\n",
    "- Consists of the operation of the $W_V$ and $W_O$ matrices. Once the attention patterns are fixed, these matrices operate on the residual stream at the source position. Their output is what gets moved from the source to the destination position.\n",
    "\n",
    "The function form of an entire attention head is:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\textrm{output}&=\\textrm{softmax}\\left(\\frac{x^T W_Q W^T_Kx}{\\sqrt{d_{head}}}\\right)(x W_V W_O) \\\\\n",
    "&= A x W_V W_O\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where\n",
    "- $W_V$ had shape `(d_model, d_head)`\n",
    "- $W_O$ has shape `(d_head, d_model)`\n",
    "\n",
    "\n",
    "We can cleary see the QK and OV circuits doing different things. They should be thought of as two distinct parts of the attention head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests in `test_causal_mask` passed!\n",
      "Input shape: torch.Size([2, 4, 768])\n",
      "Output shape: torch.Size([2, 4, 768]) \n",
      "\n",
      "Input shape: torch.Size([1, 35, 768])\n",
      "Output shape: torch.Size([1, 35, 768])\n",
      "Reference output shape: torch.Size([1, 35, 768]) \n",
      "\n",
      "100.00% of the values are correct\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class Attention(nn.Module):\n",
    "    IGNORE: Float[Tensor, \"\"]\n",
    "\n",
    "    def __init__(self, cfg: Config):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.W_Q = nn.Parameter(t.empty((cfg.n_heads, cfg.d_model, cfg.d_head)))\n",
    "        self.W_K = nn.Parameter(t.empty((cfg.n_heads, cfg.d_model, cfg.d_head)))\n",
    "        self.W_V = nn.Parameter(t.empty((cfg.n_heads, cfg.d_model, cfg.d_head)))\n",
    "        self.W_O = nn.Parameter(t.empty((cfg.n_heads, cfg.d_head, cfg.d_model)))\n",
    "        self.b_Q = nn.Parameter(t.zeros((cfg.n_heads, cfg.d_head)))\n",
    "        self.b_K = nn.Parameter(t.zeros((cfg.n_heads, cfg.d_head)))\n",
    "        self.b_V = nn.Parameter(t.zeros((cfg.n_heads, cfg.d_head)))\n",
    "        self.b_O = nn.Parameter(t.zeros((cfg.d_model)))\n",
    "        nn.init.normal_(self.W_Q, std=self.cfg.init_range)\n",
    "        nn.init.normal_(self.W_K, std=self.cfg.init_range)\n",
    "        nn.init.normal_(self.W_V, std=self.cfg.init_range)\n",
    "        nn.init.normal_(self.W_O, std=self.cfg.init_range)\n",
    "        self.register_buffer(\n",
    "            \"IGNORE\", t.tensor(float(\"-inf\"), device=device, dtype=t.float32)\n",
    "        )\n",
    "\n",
    "    def apply_causal_mask(\n",
    "        self, attn_scores: Float[Tensor, \"batch n_heads query_pos key_pos\"]\n",
    "    ) -> Float[Tensor, \"batch n_heads query_pos key_pos\"]:\n",
    "        \"\"\"\n",
    "        Applies a causal mask to attention scores, and returns masked scores.\n",
    "        \"\"\"\n",
    "        mask = t.triu(\n",
    "            t.ones(attn_scores.shape[-2:], device=attn_scores.device), diagonal=1\n",
    "        )\n",
    "        # remember, broadcasting starts with trailing dimensions\n",
    "        return attn_scores.masked_fill(mask == 1, self.IGNORE)\n",
    "\n",
    "    def forward(\n",
    "        self, normalized_resid_pre: Float[Tensor, \"batch posn d_model\"]\n",
    "    ) -> Float[Tensor, \"batch posn d_model\"]:\n",
    "        batch, seq_len, d_model = normalized_resid_pre.shape\n",
    "\n",
    "        K = (\n",
    "            einops.einsum(\n",
    "                self.W_K,\n",
    "                normalized_resid_pre,\n",
    "                \"n_heads d_model d_head, batch posn d_model -> batch posn n_heads d_head\",\n",
    "            )\n",
    "            + self.b_K\n",
    "        )\n",
    "        Q = (\n",
    "            einops.einsum(\n",
    "                self.W_Q,\n",
    "                normalized_resid_pre,\n",
    "                \"n_heads d_model d_head, batch posn d_model -> batch posn n_heads d_head\",\n",
    "            )\n",
    "            + self.b_Q\n",
    "        )\n",
    "\n",
    "        attn_scores = einops.einsum(\n",
    "            Q,\n",
    "            K,\n",
    "            \"batch posn_Q n_heads d_head, batch posn_K n_heads d_head -> batch n_heads posn_Q posn_K\",\n",
    "        )\n",
    "        attn_scores_scaled = attn_scores / math.sqrt(self.cfg.d_head)\n",
    "        attn_scores_scaled_masked = self.apply_causal_mask(attn_scores_scaled)\n",
    "\n",
    "        attn_probs = attn_scores_scaled_masked.softmax(dim=-1)\n",
    "\n",
    "        V = (\n",
    "            einops.einsum(\n",
    "                self.W_V,\n",
    "                normalized_resid_pre,\n",
    "                \"n_heads d_model d_head, batch posn d_model -> batch posn n_heads d_head\",\n",
    "            )\n",
    "            + self.b_V\n",
    "        )\n",
    "        z = einops.einsum(\n",
    "            attn_probs,\n",
    "            V,\n",
    "            \"batch n_heads posn_Q posn_K, batch posn_K n_heads d_head -> batch posn_Q n_heads d_head\",\n",
    "        )\n",
    "\n",
    "        return (\n",
    "            einops.einsum(\n",
    "                z,\n",
    "                self.W_O,\n",
    "                \"batch posn n_heads d_head, n_heads d_head d_model -> batch posn d_model\",\n",
    "            )\n",
    "            + self.b_O\n",
    "        )\n",
    "\n",
    "\n",
    "tests.test_causal_mask(Attention.apply_causal_mask)\n",
    "rand_float_test(Attention, [2, 4, 768])\n",
    "load_gpt2_test(Attention, reference_gpt2.blocks[0].attn, cache[\"normalized\", 0, \"ln1\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement:\n",
    "- A linear layer, with weight `W_in`, bias `b_in`\n",
    "\n",
    "- A nonlinear function (usually GELU); can use the imported function `gelu_new`\n",
    "\n",
    "- A linear layer, weight `W_out`, bias `b_out`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 4, 768])\n",
      "Output shape: torch.Size([2, 4, 768]) \n",
      "\n",
      "Input shape: torch.Size([1, 35, 768])\n",
      "Output shape: torch.Size([1, 35, 768])\n",
      "Reference output shape: torch.Size([1, 35, 768]) \n",
      "\n",
      "100.00% of the values are correct\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, cfg: Config):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.W_in = nn.Parameter(t.empty((cfg.d_model, cfg.d_mlp)))\n",
    "        self.W_out = nn.Parameter(t.empty((cfg.d_mlp, cfg.d_model)))\n",
    "        self.b_in = nn.Parameter(t.zeros((cfg.d_mlp)))\n",
    "        self.b_out = nn.Parameter(t.zeros((cfg.d_model)))\n",
    "        nn.init.normal_(self.W_in, std=self.cfg.init_range)\n",
    "        nn.init.normal_(self.W_out, std=self.cfg.init_range)\n",
    "\n",
    "    def forward(\n",
    "        self, normalized_resid_mid: Float[Tensor, \"batch posn d_model\"]\n",
    "    ) -> Float[Tensor, \"batch posn d_model\"]:\n",
    "        h = gelu_new(\n",
    "            einops.einsum(\n",
    "                normalized_resid_mid,\n",
    "                self.W_in,\n",
    "                \"batch posn d_model, d_model d_mlp -> batch posn d_mlp\",\n",
    "            )\n",
    "            + self.b_in\n",
    "        )\n",
    "        return (\n",
    "            einops.einsum(\n",
    "                h, self.W_out, \"batch posn d_mlp, d_mlp d_model -> batch posn d_model\"\n",
    "            )\n",
    "            + self.b_out\n",
    "        )\n",
    "\n",
    "\n",
    "rand_float_test(MLP, [2, 4, 768])\n",
    "load_gpt2_test(MLP, reference_gpt2.blocks[0].mlp, cache[\"normalized\", 0, \"ln2\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put together the attention, MLP and layernorms into a single transformer block.\n",
    "\n",
    "- Remember to implement the residual connections correctly!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 4, 768])\n",
      "Output shape: torch.Size([2, 4, 768]) \n",
      "\n",
      "Input shape: torch.Size([1, 35, 768])\n",
      "Output shape: torch.Size([1, 35, 768])\n",
      "Reference output shape: torch.Size([1, 35, 768]) \n",
      "\n",
      "100.00% of the values are correct\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg: Config):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.ln1 = LayerNorm(cfg)\n",
    "        self.attn = Attention(cfg)\n",
    "        self.ln2 = LayerNorm(cfg)\n",
    "        self.mlp = MLP(cfg)\n",
    "\n",
    "    def forward(\n",
    "        self, resid_pre: Float[Tensor, \"batch position d_model\"]\n",
    "    ) -> Float[Tensor, \"batch position d_model\"]:\n",
    "        normalised_resid_pre = self.ln1(resid_pre)\n",
    "        attn_output = self.attn(normalised_resid_pre)\n",
    "        resid_mid = resid_pre + attn_output\n",
    "        normalised_resid_mid = self.ln2(resid_mid)\n",
    "        mlp_output = self.mlp(normalised_resid_mid)\n",
    "        resid_post = resid_mid + mlp_output\n",
    "        return resid_post\n",
    "\n",
    "\n",
    "rand_float_test(TransformerBlock, [2, 4, 768])\n",
    "load_gpt2_test(TransformerBlock, reference_gpt2.blocks[0], cache[\"resid_pre\", 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unembedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear layer with weight `W_U` and bias `b_U`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 4, 768])\n",
      "Output shape: torch.Size([2, 4, 50257]) \n",
      "\n",
      "Input shape: torch.Size([1, 35, 768])\n",
      "Output shape: torch.Size([1, 35, 50257])\n",
      "Reference output shape: torch.Size([1, 35, 50257]) \n",
      "\n",
      "100.00% of the values are correct\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class Unembed(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.W_U = nn.Parameter(t.empty((cfg.d_model, cfg.d_vocab)))\n",
    "        nn.init.normal_(self.W_U, std=self.cfg.init_range)\n",
    "        self.b_U = nn.Parameter(t.zeros((cfg.d_vocab), requires_grad=False))\n",
    "\n",
    "    def forward(\n",
    "        self, normalized_resid_final: Float[Tensor, \"batch position d_model\"]\n",
    "    ) -> Float[Tensor, \"batch position d_vocab\"]:\n",
    "        return (\n",
    "            einops.einsum(\n",
    "                normalized_resid_final,\n",
    "                self.W_U,\n",
    "                \"batch posn d_model, d_model d_vocab -> batch posn d_vocab\",\n",
    "            )\n",
    "            + self.b_U\n",
    "        )\n",
    "\n",
    "\n",
    "rand_float_test(Unembed, [2, 4, 768])\n",
    "load_gpt2_test(Unembed, reference_gpt2.unembed, cache[\"ln_final.hook_normalized\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 4])\n",
      "Output shape: torch.Size([2, 4, 50257]) \n",
      "\n",
      "Input shape: torch.Size([1, 35])\n",
      "Output shape: torch.Size([1, 35, 50257])\n",
      "Reference output shape: torch.Size([1, 35, 50257]) \n",
      "\n",
      "100.00% of the values are correct\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class DemoTransformer(nn.Module):\n",
    "    def __init__(self, cfg: Config):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.embed = Embed(cfg)\n",
    "        self.pos_embed = PosEmbed(cfg)\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [TransformerBlock(cfg) for _ in range(cfg.n_layers)]\n",
    "        )\n",
    "        self.ln_final = LayerNorm(cfg)\n",
    "        self.unembed = Unembed(cfg)\n",
    "\n",
    "    def forward(\n",
    "        self, tokens: Int[Tensor, \"batch position\"]\n",
    "    ) -> Float[Tensor, \"batch position d_vocab\"]:\n",
    "        embed = self.embed(tokens)\n",
    "        pos_embed = self.pos_embed(tokens)\n",
    "\n",
    "        resid = embed + pos_embed\n",
    "\n",
    "        for block in self.blocks:\n",
    "            resid = block(resid)\n",
    "\n",
    "        resid_final = self.ln_final(resid)\n",
    "        return self.unembed(resid_final)\n",
    "\n",
    "\n",
    "rand_int_test(DemoTransformer, [2, 4])\n",
    "load_gpt2_test(DemoTransformer, reference_gpt2, tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try it out!\n",
    "\n",
    "- Create a new instance of our `DemoTransformer`.\n",
    "\n",
    "- Since our local implementation matches GPT-2's architecture, we can load pre-learned parameters into our `DemoTransformer` from our (pre-loaded) reference GPT-2 model.\n",
    "  - These are contained in `state_dict`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set of tokens:\n",
      "['<|endoftext|>', 'I', ' am', ' an', ' amazing', ' aut', 'ore', 'gressive', ',', ' dec', 'oder', '-', 'only', ',', ' G', 'PT', '-', '2', ' style', ' transformer', '.', ' One', ' day', ' I', ' will', ' exceed', ' human', ' level', ' intelligence', ' and', ' take', ' over', ' the', ' world', '!']\n"
     ]
    }
   ],
   "source": [
    "demo_gpt2 = DemoTransformer(Config(debug=False)).to(device)\n",
    "demo_gpt2.load_state_dict(reference_gpt2.state_dict(), strict=False)\n",
    "\n",
    "print(\"Test set of tokens:\")\n",
    "print(reference_gpt2.to_str_tokens(tokens))\n",
    "\n",
    "demo_logits = demo_gpt2(tokens)  # Run our custom model on the same input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the loss\n",
    "\n",
    "We'll use **cross-entropy loss**. The cross entropy loss between a modelled distribution $Q$ and a target distribution $P$ is given by:\n",
    "\n",
    "$$\n",
    "\\textrm{loss} = -\\sum_x{P(x) \\log{Q(x)}}\n",
    "$$\n",
    "\n",
    "In the case where $P$ is the empirical distribution from target classes - i.e., $P(x^*)=1$ for the correct class $x^*$ - this becomes:\n",
    "\n",
    "$$\n",
    "\\textrm{loss} = -\\log{Q(x)}\n",
    "$$\n",
    "\n",
    "I.e., the negative log prob of the true classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg cross entropy loss: 4.5647\n",
      "Avg cross entropy loss for uniform distribution: 10.824905\n",
      "Avg probability assigned to correct token: 0.087910\n"
     ]
    }
   ],
   "source": [
    "def get_log_probs(\n",
    "    logits: Float[Tensor, \"batch posn d_vocab\"], tokens: Int[Tensor, \"batch posn\"]\n",
    ") -> Float[Tensor, \"batch posn-1\"]:\n",
    "    log_probs = logits.log_softmax(dim=-1)\n",
    "    # Get logprobs the first seq_len-1 predictions (so we can compare them with the actual next tokens)\n",
    "    log_probs_for_tokens = (\n",
    "        log_probs[:, :-1].gather(dim=-1, index=tokens[:, 1:].unsqueeze(-1)).squeeze(-1)\n",
    "    )\n",
    "\n",
    "    return log_probs_for_tokens\n",
    "\n",
    "\n",
    "pred_log_probs = get_log_probs(demo_logits, tokens)\n",
    "print(f\"Avg cross entropy loss: {-pred_log_probs.mean():.4f}\")\n",
    "print(\n",
    "    f\"Avg cross entropy loss for uniform distribution: {math.log(demo_gpt2.cfg.d_vocab):4f}\"\n",
    ")\n",
    "print(f\"Avg probability assigned to correct token: {pred_log_probs.exp().mean():4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The solutions gave the following output:\n",
    "\n",
    "```terminal\n",
    "Avg cross entropy loss: 4.0442\n",
    "Avg cross entropy loss for uniform distribution: 10.824905\n",
    "Avg probability assigned to correct token: 0.098628\n",
    "```\n",
    "\n",
    "This is similar to the results above - a good sign! But consistently lower... why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate text\n",
    "\n",
    "Use \"greedy\" approach for now; i.e., take the most likely next token and continually append it to our prompt before feeding back into the model: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3922765e12f446c5a5d906c9dd3d112c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_string = \"\"\"The Total Perspective Vortex derives its picture of the whole Universe on the principle of\"\"\"\n",
    "for i in tqdm(range(100)):\n",
    "    test_tokens = reference_gpt2.to_tokens(test_string).to(device)\n",
    "    demo_logits = demo_gpt2(test_tokens)\n",
    "    test_string += reference_gpt2.tokenizer.decode(demo_logits[-1, -1].argmax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('The Total Perspective Vortex derives its picture of the whole Universe on '\n",
      " 'the principle of the total perspective. The total perspective is the view of '\n",
      " 'the whole Universe from the point of view of the observer. The total '\n",
      " 'perspective is the view of the whole Universe from the point of view of the '\n",
      " 'observer. The total perspective is the view of the whole Universe from the '\n",
      " 'point of view of the observer. The total perspective is the view of the '\n",
      " 'whole Universe from the point of view of the observer. The total perspective '\n",
      " 'is the view of the whole Universe from the point of view of the observer. '\n",
      " 'The')\n"
     ]
    }
   ],
   "source": [
    "pprint(test_string)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
